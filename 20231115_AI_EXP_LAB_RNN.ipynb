{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent neural networks"
      ],
      "metadata": {
        "id": "-AR5nqcGqAcm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqble9UZkY_i"
      },
      "outputs": [],
      "source": [
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "        h_next = np.tanh(t)\n",
        "\n",
        "        self.cache = (x, h_prev, h_next)\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        # 'ppp' exercise\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, h_next = self.cache\n",
        "\n",
        "        # tanh\n",
        "        dt = dh_next * (1- h_next**2)\n",
        "        db = np.sum(dt, axis=0)\n",
        "        dWh = np.dot(h_prev.T ,dt)\n",
        "        dh_prev = np.dot(x.T ,dt)\n",
        "        dWx = np.dot(x.T ,dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "\n",
        "        self.grad[0][...] = dWx\n",
        "        self.grad[1][...] = dWh\n",
        "        self.grad[2][...] = db\n",
        "        return dx, dh_prev\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeRNN:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            # 'ppp' exercise\n",
        "            layer = RNN(*self.params)\n",
        "            self.h = layer.forward(xs(:, t, :), self.h)\n",
        "            hs[:, t, :] = self.h\n",
        "            self.layer.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        # 'ppp' exercise\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H =dhs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        for i in reversed(range(T)):\n",
        "          layer = self.layers(t)\n",
        "          dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "          dxs[:, t, :] = dx\n",
        "\n",
        "          for i, grad in enumerate(layer, grads):\n",
        "            grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "          self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h):\n",
        "        self.h = h\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h = None"
      ],
      "metadata": {
        "id": "X7ZHbDo0p3HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language models"
      ],
      "metadata": {
        "id": "snAaqyaVqL3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PPP exercise\n",
        "class TimeEmbedding:\n",
        "    def __init__(self, W):\n",
        "      self.params = [W]\n",
        "      self.grads = [np.zeros_like(W)]\n",
        "      self.layers = None\n",
        "      self.W = W\n",
        "\n",
        "    def forward(self, xs):\n",
        "      N, T = xs.shape\n",
        "      V, D = self.W.shape\n",
        "\n",
        "      out = np.empty((N, T, D), dtype='f')\n",
        "      self.layers = []\n",
        "\n",
        "    for i in range(T):\n",
        "      layer = Embedding(self.W)\n",
        "      out[:, t, :] = layer.forward(xs[:, t])\n",
        "      self.layers.append(layer)\n",
        "    return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, D = dout.shape\n",
        "        grad = 0\n",
        "        for t in range(T):\n",
        "          layer = self.layers[t]\n",
        "          layer.backward(dout[:, t, :])\n",
        "          grad += layer.grads[0]\n",
        "        self.grads[0][...] = grad\n",
        "        return None\n",
        "\n",
        "\n",
        "class TimeAffine:\n",
        "    def __init__(self, W, b):\n",
        "      self.params = [W, b]\n",
        "      self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "      self.xs = None\n",
        "\n",
        "    def forward(self, xs):\n",
        "      N ,T, D = xs.shape\n",
        "      W, b = self.params\n",
        "      rx = rs.reshape(N*T, -1)\n",
        "      out = np.dot(rx, W) + b\n",
        "      self.xs = xs\n",
        "      return out.reshape(N, T, -1)\n",
        "\n",
        "    def backward(self, dout):\n",
        "      xs = self.xs\n",
        "      N ,T, D = xs.shape\n",
        "      W, b = self.params\n",
        "\n",
        "      dout = dout.reshape(N * T, -1)\n",
        "      rx = xs.reshape(*xs.shape)\n",
        "\n",
        "      self.grads[0][...] = dW\n",
        "      self.grads[1][...] = db\n",
        "\n",
        "\n",
        "class TimeSoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "      self.params, self.grads = [], []\n",
        "      self.cache = None\n",
        "      self.ignore_label = -1\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "      N, T, V = xs.shape\n",
        "\n",
        "      if ts.dim == 3:\n",
        "        ts = ts.argmax(axis=2)\n",
        "\n",
        "      mask = (ts != self.ignore_label)\n",
        "      xs = xs.reshape(N*T, V)\n",
        "      ts = ts.reshape(N*T)\n",
        "      mask = mask.reshape(N*T)\n",
        "\n",
        "      ys = softmax(xs)\n",
        "      ls = np.log(ys[np.arrange(N*T), ts])\n",
        "      ls *= mask\n",
        "      loss = -np.sum(ls)\n",
        "      loss /= mask.sum()\n",
        "\n",
        "      self.cache = (ts, ys, mask, (N, T, V))\n",
        "      return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "      ts, ys, mask, (N, T, V) = self.cache\n",
        "\n",
        "      dx = ys\n",
        "      dx[np.arrange(N*T), ts] -= 1\n",
        "      dx *= dout\n",
        "      dx /= mask.sum()\n",
        "      dx *= mask[:, np.newaxis]\n",
        "\n",
        "      dx = dx.reshape((N, T, V))\n",
        "      return dx\n",
        "\n"
      ],
      "metadata": {
        "id": "9mC6HhKcqaYi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}