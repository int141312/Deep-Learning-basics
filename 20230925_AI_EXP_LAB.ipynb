{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from builtins import range\n",
        "from random import shuffle"
      ],
      "metadata": {
        "id": "NItFX_91nDzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modular layers"
      ],
      "metadata": {
        "id": "9KqvlQ53-J3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MulLayer:\n",
        "  def __init__(self):\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x * y\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.y  # exchange x & y\n",
        "    dy = dout * self.x\n",
        "\n",
        "    return dx, dy\n",
        "\n",
        "\n",
        "# 'ppp' exercise\n",
        "class AddLayer:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    out = x + y\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * 1\n",
        "    dy = dout * 1\n",
        "\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "JKPHZOvm-Mit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Two-layer net"
      ],
      "metadata": {
        "id": "_eMTA_2B72b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet:\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    # initialize the weights\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  def predict(self, x):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "\n",
        "    return y\n",
        "\n",
        "  # x : input data, t : ground-truth\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    t = np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "  # x : input data, t : ground-truth\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "    grads = {}\n",
        "\n",
        "    batch_num = x.shape[0]\n",
        "\n",
        "    # forward\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "\n",
        "    # backward\n",
        "    dy = (y - t) / batch_num\n",
        "\n",
        "    # 'ppp' exercise\n",
        "    grads['W2'] = np.dot(z1.T, dy)\n",
        "    grads['b2'] = np.sum(dy, axis=0)\n",
        "\n",
        "    da2 = np.dot(dy, W2.T)\n",
        "    dz1 = sigmoid_grad(a1) * da2\n",
        "    grads['W1'] = np.dot(x.T, dz1)\n",
        "    grads['b1'] = np.sum(dz1, axis=0)\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "w95E7OsT725k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}