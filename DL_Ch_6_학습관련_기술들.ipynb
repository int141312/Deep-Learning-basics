{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 6 . 학습 관련 기술들"
      ],
      "metadata": {
        "id": "lWRzef3SaLtW"
      },
      "id": "lWRzef3SaLtW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 수업 환경 구성을 위한 코드 (수업전 실행)"
      ],
      "metadata": {
        "id": "78zpFIX4-iAF"
      },
      "id": "78zpFIX4-iAF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MNIST 데이터 적재\n",
        "* common 모듈 사용을 위한 경로 지정시 pickle 오류 발생 문제 회피\n",
        "* 사전에 load_mnist 라이브러리 import 및 MNIST 파일 데이터 load"
      ],
      "metadata": {
        "id": "H9Vq6jHQLfbS"
      },
      "id": "H9Vq6jHQLfbS"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.0.1 mnist.py 업로드\n",
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]"
      ],
      "metadata": {
        "id": "HGcHUEOkDvNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd8dc50-5dbb-427d-93b1-bd0b67657938"
      },
      "id": "HGcHUEOkDvNn",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d8a40f59-104a-4d46-855c-6a8c99b4ac07\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d8a40f59-104a-4d46-855c-6a8c99b4ac07\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d12f58acb113>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 6.0.1 mnist.py 업로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    165\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.0.2 MNIST 데이터 적재\n",
        "from mnist import load_mnist\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)"
      ],
      "metadata": {
        "id": "Qq09BazxD6qP"
      },
      "id": "Qq09BazxD6qP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### common 라이브러리\n",
        "* 수업 시간에 작성한 여러 함수 및 원활한 프로그램 수행을 위한 utility 등 저자 제공 파일들\n",
        "* common 라이브러리 내 모듈 사용을 위해 google drive mount 및 경로 지정\n",
        "* 사전에 G 드라이브 내 Colab Notebooks 디렉토리에 common 폴더 복제"
      ],
      "metadata": {
        "id": "0tDLBlQ4MQFl"
      },
      "id": "0tDLBlQ4MQFl"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.0.3 common 폴더 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ga9tJh_65_GG"
      },
      "id": "Ga9tJh_65_GG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.0.4 수행 경로 변경\n",
        "%cd /content/drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "id": "GyGQwF7W7ajJ"
      },
      "id": "GyGQwF7W7ajJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "demographic-fetish",
      "metadata": {
        "id": "demographic-fetish"
      },
      "source": [
        "## 6.1 매개변수 갱신\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.1 모험가 이야기"
      ],
      "metadata": {
        "id": "erqb8hULUi82"
      },
      "id": "erqb8hULUi82"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.2 확률적 경사하강법 (SGD)"
      ],
      "metadata": {
        "id": "97dbkcjGBRix"
      },
      "id": "97dbkcjGBRix"
    },
    {
      "cell_type": "markdown",
      "id": "attempted-triple",
      "metadata": {
        "id": "attempted-triple"
      },
      "source": [
        "$$ \\mathbf{W} \\gets \\mathbf{W} + \\eta \\frac {\\partial L}{\\partial \\mathbf{W}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "circular-applicant",
      "metadata": {
        "id": "circular-applicant"
      },
      "outputs": [],
      "source": [
        "# 6.1.2.1 확률적 경사하강법 동작 원리\n",
        "class SGD :\n",
        "    def __init__(self, lr = 0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "right-psychology",
      "metadata": {
        "id": "right-psychology"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 6.1.2.2 SGD 클래스 적용 예시  psuedo code\n",
        "\"\"\"\n",
        "network = TwoLayerNet\n",
        "optimizer = SGD()\n",
        "\n",
        "for i in range(10000):\n",
        "    ...\n",
        "    x_batch, t_batch = get_minio_batch()\n",
        "    grads = network.gradient (x_batch, t_batch)\n",
        "    params = network.params\n",
        "    optimizer.update(params, grads)\n",
        "\n",
        "\"\"\"\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "psychological-friendly",
      "metadata": {
        "id": "psychological-friendly"
      },
      "source": [
        "### 6.1.3 SGD의 단점\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.4 모멘텀"
      ],
      "metadata": {
        "id": "Esso2siEUPtg"
      },
      "id": "Esso2siEUPtg"
    },
    {
      "cell_type": "markdown",
      "id": "manufactured-rhythm",
      "metadata": {
        "id": "manufactured-rhythm"
      },
      "source": [
        "$$ \\mathbf{ v} \\gets   \\alpha \\mathbf{ v}- \\eta \\frac {\\partial L} {\\partial \\mathbf{W}}$$\n",
        "$$ \\mathbf{W} \\gets \\mathbf{W} + \\mathbf{v}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "opening-rachel",
      "metadata": {
        "id": "opening-rachel"
      },
      "outputs": [],
      "source": [
        "# 6.1.4.1 모멘텀 동작 원리\n",
        "class Momentum :\n",
        "    def __init__(self, lr= 0.01, momentum = 0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update (self, params, grads):\n",
        "        if self.v is None :\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum * self.v[key] -\\\n",
        "                                self.lr * grads[key]\n",
        "            params[key] += self.v[key]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rational-romance",
      "metadata": {
        "id": "rational-romance"
      },
      "source": [
        "### 6.1.5 AdaGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "enormous-nelson",
      "metadata": {
        "id": "enormous-nelson"
      },
      "source": [
        "$$ \\mathbf{h} \\gets \\mathbf{h}  + \\frac {\\partial L} {\\partial \\mathbf{W}} \\odot \\frac {\\partial L} {\\partial \\mathbf{W}}$$\n",
        "$$ \\mathbf{W} \\gets \\mathbf{W} + \\eta \\frac{1}{ \\sqrt { \\mathbf{h}}} \\frac {\\partial L}{\\partial \\mathbf{W}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sharp-permit",
      "metadata": {
        "id": "sharp-permit"
      },
      "outputs": [],
      "source": [
        "# 6.1.5.1 AdaGrad 동작 원리\n",
        "class AdaGrad :\n",
        "    def __init__(self, lr = 0.01) :\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads ):\n",
        "        if self.h == None :\n",
        "            self.h = {}\n",
        "            for key, val in params.items() :\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] += self.lr *grads[key] / \\\n",
        "                      (np.sqrt(self.h[key]) + 1e-7 )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "naughty-works",
      "metadata": {
        "id": "naughty-works"
      },
      "source": [
        "### 6.1.6 Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.7 어느 갱신 방법을 이용할 것인가"
      ],
      "metadata": {
        "id": "GtHLhuVgG0c5"
      },
      "id": "GtHLhuVgG0c5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### optimizer 별 최소점 접근 경로\n",
        "\n",
        "* 초기값 (-7, 3) 에서 시작하여 각 optimizer의 방법으로 30번 매개변수를 갱신하여 최소점을 찾는 경로 시각화\n",
        "$$ f(x,y) =\\frac {1} {20} x^2 + y^2$$\n",
        "$$ \\frac {\\partial f} {\\partial x} = \\frac {x}{10}$$\n",
        "$$\\frac {\\partial f} {\\partial y} = 2y $$"
      ],
      "metadata": {
        "id": "53DdihWGTmaP"
      },
      "id": "53DdihWGTmaP"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1.7.1 Optimizer별 최소점 접근 경로 시각화\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "# 사전 정의된 옵티마이저\n",
        "from common.optimizer import *\n",
        "\n",
        "# 대상이 되는 함수 정의\n",
        "def f(x, y):\n",
        "    return x**2 / 20.0 + y**2\n",
        "\n",
        "def df(x, y):\n",
        "    return x / 10.0, 2.0*y\n",
        "\n",
        "# 초기값 설정\n",
        "init_pos = (-7.0, 2.0)\n",
        "\n",
        "# 매개변수와 기울기 초기값\n",
        "params = {}\n",
        "params['x'], params['y'] = init_pos[0], init_pos[1]\n",
        "grads = {}\n",
        "grads['x'], grads['y'] = 0, 0\n",
        "\n",
        "# 옵티마이저 생성\n",
        "optimizers = OrderedDict()\n",
        "optimizers[\"SGD\"] = SGD(lr=0.95)\n",
        "optimizers[\"Momentum\"] = Momentum(lr=0.1)\n",
        "optimizers[\"AdaGrad\"] = AdaGrad(lr=1.5)\n",
        "optimizers[\"Adam\"] = Adam(lr=0.3)\n",
        "\n",
        "\n",
        "# 4개의 그래프 작성을 위한 figure\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "# 옵티마이저별 탐색\n",
        "for idx, key in enumerate(optimizers, 1 ) :\n",
        "    optimizer = optimizers[key] # 옵티마이저 지정\n",
        "    x_history = []\n",
        "    y_history = []    # 이동 경로\n",
        "    params['x'], params['y'] = init_pos[0], init_pos[1]  # 초기값 설정\n",
        "\n",
        "    # optimizer 별로 30 번의 매개변수 갱신을 시도함.\n",
        "    for i in range(30):\n",
        "        x_history.append(params['x'])\n",
        "        y_history.append(params['y'])\n",
        "\n",
        "        grads['x'], grads['y'] = df(params['x'], params['y']) # 미분값은 구해줌\n",
        "        optimizer.update(params, grads)  ### 매개 변수 갱신 ###\n",
        "\n",
        "\n",
        "    # 갱신 결과 plotting\n",
        "    # 데이터 생성\n",
        "    x = np.arange(-10, 10, 0.01)\n",
        "    y = np.arange(-5, 5, 0.01)\n",
        "\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = f(X, Y)\n",
        "\n",
        "    # for simple contour line\n",
        "    mask = Z > 7\n",
        "    Z[mask] = 0\n",
        "\n",
        "    # plot\n",
        "    plt.subplot(2, 2, idx ) # subplot을 생성하여 그래프 작성\n",
        "\n",
        "    plt.plot(x_history, y_history, 'o-', color=\"red\") # 매개변수 출력\n",
        "    plt.contour(X, Y, Z)\n",
        "    plt.ylim(-10, 10)\n",
        "    plt.xlim(-10, 10)\n",
        "    plt.plot(0, 0, '+')\n",
        "    plt.title(key)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")"
      ],
      "metadata": {
        "id": "KRA4HzNg5bPh"
      },
      "id": "KRA4HzNg5bPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "indonesian-billion",
      "metadata": {
        "id": "indonesian-billion"
      },
      "source": [
        "### 6.1.8 MNIST 데이터 셋으로 본 갱신 방법 비교"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### optimizer 별 손실 비교 on MNIST"
      ],
      "metadata": {
        "id": "qbv8jXeFT1QX"
      },
      "id": "qbv8jXeFT1QX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "antique-therapist",
      "metadata": {
        "scrolled": false,
        "id": "antique-therapist"
      },
      "outputs": [],
      "source": [
        "# 6.1.8.1 MNIST 분류 손실 비교\n",
        "# 은닉층 4개층 100개 노드\n",
        "# 은닉층 활성화 함수 : ReLU\n",
        "# 가중치 초기 설정 : He\n",
        "\n",
        "# 배치 사이즈 : 118\n",
        "# 최대 반복 회수 :  2000 iteration\n",
        "\n",
        "# 실험 최적화 optimizer 4종  : SGD, Momentum, AdaGrad, Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from dataset.mnist import load_mnist\n",
        "from common.util import smooth_curve\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import *\n",
        "\n",
        "# 0:MNIST 데이터 읽어들이기==========\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 128\n",
        "max_iterations = 2000\n",
        "\n",
        "# 1:실험 설정==========\n",
        "optimizers = {}\n",
        "optimizers['SGD'] = SGD()\n",
        "optimizers['Momentum'] = Momentum()\n",
        "optimizers['AdaGrad'] = AdaGrad()\n",
        "optimizers['Adam'] = Adam()\n",
        "#optimizers['RMSprop'] = RMSprop()\n",
        "\n",
        "networks = {}\n",
        "train_loss = {}\n",
        "\n",
        "# optimize 별 신경망 구성\n",
        "for key in optimizers.keys():\n",
        "    networks[key] = MultiLayerNet(\n",
        "        input_size=784,\n",
        "        hidden_size_list=[100, 100, 100, 100],\n",
        "        output_size=10)\n",
        "    train_loss[key] = []\n",
        "\n",
        "\n",
        "# 2: 학습 ==========\n",
        "for i in range(max_iterations):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # optimize 별 학습 수행\n",
        "    for key in optimizers.keys():\n",
        "        grads = networks[key].gradient(x_batch, t_batch)\n",
        "        optimizers[key].update(networks[key].params, grads)\n",
        "\n",
        "        loss = networks[key].loss(x_batch, t_batch)\n",
        "        train_loss[key].append(loss)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print( f\"=========== iteration: {i} ===========\")\n",
        "        for key in optimizers.keys():\n",
        "            loss = networks[key].loss(x_batch, t_batch)\n",
        "            print(f\"{key} : {loss}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1.8.2  갱신 방법 별 손실율 그래프\n",
        "#  3.그래프 작성 =========\n",
        "markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n",
        "x = np.arange(max_iterations)\n",
        "\n",
        "plt.figure (figsize = (6,5))\n",
        "\n",
        "for key in optimizers.keys():\n",
        "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key],\n",
        "             markevery=100, label=key)\n",
        "plt.title ('MNIST Loss by optimizer')\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.ylim(0, 1)\n",
        "plt.xlim(0,2000)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G7aMsOpRLDq8"
      },
      "id": "G7aMsOpRLDq8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "placed-original",
      "metadata": {
        "id": "placed-original"
      },
      "source": [
        "## 6.2 가중치의 초기값\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2.1 초기값을 0으로 하면"
      ],
      "metadata": {
        "id": "ZJSfhdXMUrdH"
      },
      "id": "ZJSfhdXMUrdH"
    },
    {
      "cell_type": "markdown",
      "id": "silver-england",
      "metadata": {
        "id": "silver-england"
      },
      "source": [
        "### 6.2.2 은닉층의 활성화값 분포 (sigmoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 기울기 소실  :   $ N (0, 1)$"
      ],
      "metadata": {
        "id": "97sHEIZqPLZo"
      },
      "id": "97sHEIZqPLZo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "paperback-mattress",
      "metadata": {
        "id": "paperback-mattress"
      },
      "outputs": [],
      "source": [
        "# 6.2.2.1  은닉층 활성화값 분포 시각화\n",
        "#  5개 은닉층, 각 층 노드 수 100개\n",
        "#  활성화 함수 : 시그모이드 함수\n",
        "#  입력데이터 개수 : 1000 개\n",
        "#  가중치의 초기값 : 표준 정규 분포 N(0,1)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.random.randn(1000,100) #1000 개의 입력 데이터\n",
        "\n",
        "node_num = 100          # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5   # 은닉층이 5개\n",
        "activations={}          # 활성화 값 저장소\n",
        "\n",
        "for i in range(hidden_layer_size) :\n",
        "    if i != 0 :\n",
        "        x = activations [i-1]   # 이전 출력값이 입력값\n",
        "\n",
        "    w = np.random.randn(node_num, node_num)*1   #가중치 초기값 : 표준정규분포\n",
        "\n",
        "    z = np.dot(x, w)    # 신호 총합\n",
        "    a = sigmoid(z)      # 활성화\n",
        "    activations[i] = a  # 활성화 값 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "relevant-drill",
      "metadata": {
        "id": "relevant-drill"
      },
      "outputs": [],
      "source": [
        "# 6.2.2.2 각 층의 활성화 값 분포 히스토그램 작성\n",
        "\n",
        "fig = plt.figure(figsize=(15,3))\n",
        "fig.suptitle('stddev = 1 Normal Distribution  on Sigmoid',\n",
        "             fontsize = 15 , y = 1.05)\n",
        "\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str (i+1) + '-layer')\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 시그모이드 함수는 0과 1 부근에서 미분이 0에 가까워진다.  기울기가 0에 가까워지면 학습이 이어지지 않느다.\n",
        "* 활성화 값이 양 끝으로 어진다는 것은 신호 총합의 절대값이 크다는 의미\n",
        "* 가중치 값을 줄여주어야 한다."
      ],
      "metadata": {
        "id": "txggQKDCVngm"
      },
      "id": "txggQKDCVngm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 표현력의 제약 :  $ N (0, 0.01), \\sigma = 0.01$"
      ],
      "metadata": {
        "id": "sU14fxXmOM0Q"
      },
      "id": "sU14fxXmOM0Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "headed-andorra",
      "metadata": {
        "id": "headed-andorra"
      },
      "outputs": [],
      "source": [
        "# 6.2.2.3 위 # 6.2.2.1 과 동일한 조건에서 가중치 초기값을 표준편차 0.01인 정규분포로 변경하여 진행\n",
        "\n",
        "x = np.random.randn(1000,100) #1000 개의 입력 데이터\n",
        "\n",
        "node_num = 100          # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5   # 은닉층이 5개\n",
        "activations={}          # 활성화 값 저장소\n",
        "\n",
        "for i in range(hidden_layer_size) :\n",
        "    if i != 0 :\n",
        "        x = activations [i-1]   # 이전 출력값이 입력값\n",
        "\n",
        "    w = np.random.randn(node_num, node_num)* 0.01   ### 가중치 초기값 변경 : 표준편차 0.01 ###\n",
        "\n",
        "    z = np.dot(x, w)    # 신호 총합\n",
        "    a = sigmoid(z)      # 활성화\n",
        "    activations[i] = a  # 활성화 값 저장\n",
        "\n",
        "# 5개 층의 분포 시각화 @ stddev = 0.01  on Sigmoid\n",
        "fig = plt.figure(figsize=(15,3))\n",
        "fig.suptitle('stddev = 0.01 Normal Distribution  on Sigmoid',\n",
        "             fontsize = 15 , y = 1.05)\n",
        "\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str (i+1) + '-layer')\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 표준편차를 0.01인 정규 분포에 따르는 가중치 초기값은, 매우 0에 근접한 값 -0.03~0.03\n",
        "* 그 결과 시그모이드 활성화 값은 0.5 부근에 집중되어 있다.\n",
        "* 기울기 소실 문제는 해결 되지만, 유사한 가중치 값을 가지면 뉴런을 많이 만든 것이 의미 없게 된다.\n",
        "* **표현력을  제한**하는 문제가 발생한다."
      ],
      "metadata": {
        "id": "rixGT9JqbsYa"
      },
      "id": "rixGT9JqbsYa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Xavier 초기값 : $N (0, \\sigma), \\sigma = \\sqrt{1\\over n}$"
      ],
      "metadata": {
        "id": "zxx21e4BOcFG"
      },
      "id": "zxx21e4BOcFG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regional-burke",
      "metadata": {
        "id": "regional-burke"
      },
      "outputs": [],
      "source": [
        "# 6.2.2.4 위 # 6.2.2.1 과 동일한 조건에서 가중치 초기값을 Xavier 초기값으로 변경하여 진행\n",
        "\n",
        "x = np.random.randn(1000,100) #1000 개의 입력 데이터\n",
        "\n",
        "node_num = 100          # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5   # 은닉층이 5개\n",
        "activations={}          # 활성화 값 저장소\n",
        "\n",
        "for i in range(hidden_layer_size) :\n",
        "    if i != 0 :\n",
        "        x = activations [i-1]   # 이전 출력값이 입력값\n",
        "\n",
        "    ### 가중치 초기값 변경 : Xavier ###\n",
        "    w = np.random.randn(node_num, node_num)/np.sqrt (node_num)\n",
        "\n",
        "    z = np.dot(x, w)    # 신호 총합\n",
        "    a = sigmoid(z)      # 활성화\n",
        "    activations[i] = a  # 활성화 값 저장\n",
        "\n",
        "# 5개 층의 분포 시각화 @ Xavier on Sigmoid\n",
        "fig = plt.figure(figsize=(15,3))\n",
        "fig.suptitle('Xavier on Sigmoid', fontsize = 15 , y = 1.05)\n",
        "\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str (i+1) + '-layer')\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Xavier 초기값  표준편차가 앞 계층 노드의  1/√𝑛인 정규 분포에 따르는 가중치.\n",
        "* 1층에서 적절히 넓게 분포 되고, 이후 층이 깊어져도 일정 이상의 분포를 유지한다."
      ],
      "metadata": {
        "id": "6BxUY5t5cCBb"
      },
      "id": "6BxUY5t5cCBb"
    },
    {
      "cell_type": "markdown",
      "id": "absolute-duration",
      "metadata": {
        "id": "absolute-duration"
      },
      "source": [
        "### 6.2.3 ReLU를 사용할 때의 가중치 초기값"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### $ N (0, \\sigma), \\sigma = 0.01$ on ReLU"
      ],
      "metadata": {
        "id": "fVVYzN8SRiRk"
      },
      "id": "fVVYzN8SRiRk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incoming-engine",
      "metadata": {
        "id": "incoming-engine"
      },
      "outputs": [],
      "source": [
        "# 6.2.3.1 활성화 함수가 ReLU인 경우로 진행한다.  stddev 0.01 의 정규분포\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum (0, x)\n",
        "\n",
        "x = np.random.randn(1000,100) #1000 개의 입력 데이터\n",
        "\n",
        "node_num = 100          # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5   # 은닉층이 5개\n",
        "activations={}          # 활성화 값 저장소\n",
        "\n",
        "for i in range(hidden_layer_size) :\n",
        "    if i != 0 :\n",
        "        x = activations [i-1]   # 이전 출력값이 입력값\n",
        "\n",
        "    ### 가중치 초기값 변경 : stddev = 0.01 정규분포 ###\n",
        "    w = np.random.randn(node_num, node_num) * 0.01\n",
        "\n",
        "    z = np.dot(x, w)\n",
        "    a= relu(z)      ### a = sigmoid(z)를 치환 ###\n",
        "    activations[i] = a\n",
        "\n",
        "\n",
        "# 5개 층의 분포 시각화 @ stddev = 0.01 정규분포 on ReLU\n",
        "fig = plt.figure(figsize=(15,3))\n",
        "fig.suptitle('stddev = 0.01 Normal Distribution on ReLU', fontsize = 15 , y = 1.05)\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str (i+1) + '-layer')\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "    plt.ylim(0, 7000)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 𝑁(0, 0.01) 은 값이 0 으로 쏠린다. 학습이 이루어지지 않게 된다. 기울기 소실"
      ],
      "metadata": {
        "id": "wSouf5yrcVxT"
      },
      "id": "wSouf5yrcVxT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Xavier on ReLU : $N (0, \\sigma), \\sigma = \\sqrt{1\\over n}$"
      ],
      "metadata": {
        "id": "gJKCHxsbRnzO"
      },
      "id": "gJKCHxsbRnzO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "agricultural-belize",
      "metadata": {
        "id": "agricultural-belize"
      },
      "outputs": [],
      "source": [
        "# 6.2.3.2 Xavier on ReLU\n",
        "\n",
        "x = np.random.randn(1000,100) #1000 개의 입력 데이터\n",
        "\n",
        "node_num = 100          # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5   # 은닉층이 5개\n",
        "activations={}          # 활성화 값 저장소\n",
        "\n",
        "for i in range(hidden_layer_size) :\n",
        "    if i != 0 :\n",
        "        x = activations [i-1]   # 이전 출력값이 입력값\n",
        "\n",
        "    ### 가중치 초기값 변경 : Xavier ###\n",
        "    w = np.random.randn(node_num, node_num) /np.sqrt (node_num)\n",
        "\n",
        "    z = np.dot(x, w)\n",
        "    a= relu(z)      ### a = sigmoid(z)를 치환 ###\n",
        "    activations[i] = a\n",
        "\n",
        "# 5개 층의 분포 시각화 @ Xavier  on ReLU\n",
        "fig = plt.figure(figsize=(15,3))\n",
        "fig.suptitle('Xavier on ReLU', fontsize = 15, y = 1.05)\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str (i+1) + '-layer')\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "    plt.ylim(0,7000)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Xavier 초기값에서도 층이 깊어지면서 치우침이 커지고 기울기 소실 문제가 발생한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "FeywuFNFeHQJ"
      },
      "id": "FeywuFNFeHQJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### He on ReLU : $N (0, \\sigma), \\sigma = \\sqrt{2\\over n}$"
      ],
      "metadata": {
        "id": "pgvxQAwDRtf_"
      },
      "id": "pgvxQAwDRtf_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arctic-manual",
      "metadata": {
        "scrolled": true,
        "id": "arctic-manual"
      },
      "outputs": [],
      "source": [
        "# 6.2.3.3 He 초기값 on ReLU\n",
        "\n",
        "x = np.random.randn(1000,100) #1000 개의 입력 데이터\n",
        "\n",
        "node_num = 100          # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5   # 은닉층이 5개\n",
        "activations={}          # 활성화 값 저장소\n",
        "\n",
        "for i in range(hidden_layer_size) :\n",
        "    if i != 0 :\n",
        "        x = activations [i-1]   # 이전 출력값이 입력값\n",
        "\n",
        "    ### 가중치 초기값 변경 : He ###\n",
        "    w = np.random.randn(node_num, node_num)* np.sqrt (2/node_num)   ## test He\n",
        "\n",
        "    z = np.dot(x, w)\n",
        "    a= relu(z)     ### a = sigmoid(z)를 치환 ###\n",
        "    activations[i] = a\n",
        "\n",
        "# 5개 층의 분포 시각화 @ He on ReLU\n",
        "fig = plt.figure(figsize=(15,3))\n",
        "fig.suptitle('He on ReLU', fontsize = 15, y = 1.05)\n",
        "\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str (i+1) + '-layer')\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "    plt.ylim(0,7000)\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* He 초기값에서는 모든 층에서 균일하게 분포 되었다.\n",
        "* ReLU 함수는 음수에서 0이 되므로,  나머지를 넓게 사용 하기 위해 Xavier 입력값의 2배격인 He 입력값을 채택\n"
      ],
      "metadata": {
        "id": "rFPvQGkLeNHG"
      },
      "id": "rFPvQGkLeNHG"
    },
    {
      "cell_type": "markdown",
      "id": "radio-treasure",
      "metadata": {
        "id": "radio-treasure"
      },
      "source": [
        "### 6.2.4 MNIST 데이터셋으로 본 가중치 초기값 비교"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 가중치 초기값별 손실비교 on MNIST\n",
        "* weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
        "* optimizer = SGD(lr=0.01)"
      ],
      "metadata": {
        "id": "fThUOjjwOTR_"
      },
      "id": "fThUOjjwOTR_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "functional-newcastle",
      "metadata": {
        "id": "functional-newcastle"
      },
      "outputs": [],
      "source": [
        "# 6.2.4.1 MNIST 데이터셋을 서로 다른 가중치 초기값으로 학습 결과 비교\n",
        "# 은닉층 4개층 100개 노드\n",
        "# 은닉층 활성화 함수 : ReLU\n",
        "\n",
        "# 배치 사이즈 : 128\n",
        "# 최대 반복 회수 : 2000 iteration\n",
        "# 훈련방식 : SGD   학습률 0.01\n",
        "\n",
        "# 실험 초기값 3종  : std= 0.01,  Xavier, He\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mnist import load_mnist\n",
        "from common.util import smooth_curve\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "# 0:MNIST 데이터 읽기==========\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 128\n",
        "max_iterations = 2000\n",
        "\n",
        "\n",
        "# 1:실험의 설정==========\n",
        "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
        "#weight_init_types = {'std=0.01': 0.01, 'Xavier': 0.1, 'He': 0.14} # 은닉층 기준\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "networks = {}\n",
        "train_loss = {}\n",
        "# 3개의 신경망을 생성\n",
        "for key, weight_type in weight_init_types.items():\n",
        "    networks[key] = MultiLayerNet(input_size=784,\n",
        "                                  hidden_size_list=[100, 100, 100, 100],\n",
        "                                  output_size=10,\n",
        "                                  weight_init_std=weight_type)\n",
        "    train_loss[key] = []\n",
        "\n",
        "\n",
        "# 2:학습==========\n",
        "for i in range(max_iterations):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    for key in weight_init_types.keys():\n",
        "        grads = networks[key].gradient(x_batch, t_batch)\n",
        "        optimizer.update(networks[key].params, grads)\n",
        "\n",
        "        loss = networks[key].loss(x_batch, t_batch)\n",
        "        train_loss[key].append(loss)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"=========== iteration:{i} ===========\")\n",
        "        for key in weight_init_types.keys():\n",
        "            loss = networks[key].loss(x_batch, t_batch)\n",
        "            print(f'{key} : {loss}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2.4.4 Xavier, He 초기값\n",
        "# 입력층\n",
        "xavier_mnist = np.sqrt(1/784)\n",
        "he_mnist = np.sqrt (2/ 784)\n",
        "# 은닉층\n",
        "xavier_hidden = np.sqrt(1/100)\n",
        "he_hidden = np.sqrt (2/ 100)\n",
        "xavier_mnist, he_mnist, xavier_hidden, he_hidden"
      ],
      "metadata": {
        "id": "rs2eYN7ZgzYc"
      },
      "id": "rs2eYN7ZgzYc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2.4.3 가중치 초기값에 따른 손실감소 그래프\n",
        "# 3.그래프 ==========\n",
        "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
        "x = np.arange(max_iterations)\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "\n",
        "\n",
        "for key in weight_init_types.keys():\n",
        "    plt.plot(x, smooth_curve(train_loss[key]),\n",
        "             marker=markers[key],\n",
        "             markevery=100,\n",
        "             label=key)\n",
        "\n",
        "plt.title ('MNIST Loss by initial value ')\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlim(0,2000)\n",
        "plt.ylim(0, 2.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qIS6EReuOG3d"
      },
      "id": "qIS6EReuOG3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* std= 0.01의 경우는 거의 학습이 이루어지지 않으나, Xavier, He에서는 순조롭게 학습이 이루어진다. He가 약간 학습속도가 빠르다."
      ],
      "metadata": {
        "id": "r0kjpZbxkHzR"
      },
      "id": "r0kjpZbxkHzR"
    },
    {
      "cell_type": "markdown",
      "id": "ultimate-arbitration",
      "metadata": {
        "id": "ultimate-arbitration"
      },
      "source": [
        "## 6.3 뱃치 정규화\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3.1 뱃치 정규화 알고리즘"
      ],
      "metadata": {
        "id": "PRNHgLZBT1m2"
      },
      "id": "PRNHgLZBT1m2"
    },
    {
      "cell_type": "markdown",
      "id": "sonic-access",
      "metadata": {
        "id": "sonic-access"
      },
      "source": [
        "$$ \\mu_B \\gets \\frac {1} {m} \\sum^m_{i=1} x_i$$\n",
        "$$\\sigma^2_B \\gets \\frac {1} {m} \\sum^m_{i=1} (x_i-\\mu_B )^2  $$\n",
        "$$ \\hat x_i \\gets \\frac{x_i-\\mu_b}  {\\sqrt {\\sigma^2_B +  \\varepsilon}} $$\n",
        "$$ y_i \\gets \\gamma \\hat x_i + \\beta $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pleasant-uruguay",
      "metadata": {
        "id": "pleasant-uruguay"
      },
      "source": [
        "### 6.3.2 뱃치 정규화의 효과"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 다양한 가중치 초기값에서의 배치정규화 적용여부 on MNIST\n",
        "* use_batchnorm=True vs. Faslse"
      ],
      "metadata": {
        "id": "3kf5E2XMM5jP"
      },
      "id": "3kf5E2XMM5jP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sunrise-scott",
      "metadata": {
        "scrolled": false,
        "id": "sunrise-scott"
      },
      "outputs": [],
      "source": [
        "# 6.3.2.1 다양한 가중치 초기값에서 배치 정규화 계층 사용여부 비교 > 훈련 환경 준비\n",
        "\n",
        "# 배치 정규화 계층의 사용 비교 실험을 위해 두 훈련을 진행하는 함수 정의\n",
        "\n",
        "# 은닉층 5개층 100개 노드\n",
        "# 은닉층 활성화 함수 : ReLU\n",
        "\n",
        "# 배치 사이즈 : 100\n",
        "# 최대 반복 회수 : 20 epoch\n",
        "# 훈련방식 : SGD   학습률 0.01\n",
        "\n",
        "# 훈련데이터 크기 : 1000\n",
        "# 실험 다양한 가중치 초기 설정에서 배치 정규화 적용여부에 따른 차이 비교\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.optimizer import SGD, Adam\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 학습 데이터의 삭제\n",
        "x_train = x_train[:1000]\n",
        "t_train = t_train[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "train_size = x_train.shape[0]   # (1000,784)\n",
        "batch_size = 100\n",
        "learning_rate = 0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.3.2.2 다양한 가중치 초기값에서 배치 정규화 계층 사용여부 비교 > 훈련 함수 정의\n",
        "\n",
        "# 배치 정규화 계층의 사용 비교 실험을 위해 두 훈련을 진행하는 함수 정의\n",
        "# 훈련 함수\n",
        "def __train(weight_init_std):\n",
        "    # 배치 정규화층이 있는 신경망\n",
        "    bn_network = MultiLayerNetExtend(input_size=784,\n",
        "                          hidden_size_list=[100, 100, 100, 100, 100],\n",
        "                          output_size=10,\n",
        "                          weight_init_std=weight_init_std,\n",
        "                          use_batchnorm=True)\n",
        "\n",
        "    # 대조를 위해 배치 정규화 층이 없는 신경망\n",
        "    network = MultiLayerNetExtend(input_size=784,\n",
        "                          hidden_size_list=[100, 100, 100, 100, 100],\n",
        "                          output_size=10,\n",
        "                          weight_init_std=weight_init_std)\n",
        "    # 최적화 방식\n",
        "    optimizer = SGD(lr=learning_rate)\n",
        "\n",
        "    train_acc_list = []\n",
        "    bn_train_acc_list = []\n",
        "\n",
        "    iter_per_epoch = max(train_size / batch_size, 1)\n",
        "    epoch_cnt = 0\n",
        "\n",
        "    for i in range(1_000_000_000):\n",
        "        batch_mask = np.random.choice(train_size, batch_size)\n",
        "        x_batch = x_train[batch_mask]\n",
        "        t_batch = t_train[batch_mask]\n",
        "\n",
        "        for _network in (bn_network, network):\n",
        "            grads = _network.gradient(x_batch, t_batch)\n",
        "            optimizer.update(_network.params, grads)\n",
        "\n",
        "        # epoch 마다 정확도 측정\n",
        "        if i % iter_per_epoch == 0:\n",
        "            train_acc = network.accuracy(x_train, t_train)\n",
        "            bn_train_acc = bn_network.accuracy(x_train, t_train)\n",
        "            train_acc_list.append(train_acc)\n",
        "            bn_train_acc_list.append(bn_train_acc)\n",
        "\n",
        "            print(f'epoch: {epoch_cnt:2} |BN N/A {train_acc:4} - Applied :{bn_train_acc}')\n",
        "\n",
        "            epoch_cnt += 1\n",
        "            if epoch_cnt >= max_epochs:   # 20 epoch\n",
        "                break\n",
        "\n",
        "    return train_acc_list, bn_train_acc_list\n"
      ],
      "metadata": {
        "id": "79gNzzh7zyKG"
      },
      "id": "79gNzzh7zyKG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "victorian-horizon",
      "metadata": {
        "id": "victorian-horizon"
      },
      "outputs": [],
      "source": [
        "# 6.3.2.3 다양한 가중치 초기값에서 배치 정규화 계층 사용여부 비교 > 학습 수행\n",
        "\n",
        "## 1 ~ 0.0001 까지 16개 logscale 가중치 초기값 생성\n",
        "weight_scale_list = np.logspace(0, -4, num=16)\n",
        "\n",
        "\n",
        "#  가중치 초기 설정별 저장 변수 초기화\n",
        "results_bn = {}\n",
        "results_no_bn = {}\n",
        "\n",
        "# 16개 가중치 초기값 별로 학습 수행\n",
        "for i, w in enumerate(weight_scale_list):\n",
        "    print(f\"======== {i+1} /16 @ weight_init_std = {w:.7f} ==============\")\n",
        "\n",
        "    # 학습 수행\n",
        "    train_acc_list, bn_train_acc_list = __train(w)\n",
        "\n",
        "    # 그래프 작성을 위해 학습 수행 결과 저장\n",
        "    results_bn[w] = bn_train_acc_list\n",
        "    results_no_bn[w] = train_acc_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.3.2.4 다양한 가중치 초기값에서 배치 정규화 계층 사용여부 비교 > 그래프\n",
        "\n",
        "# ===== 그래프 작성 =========\n",
        "x = np.arange(max_epochs)\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.suptitle('Comparison batch normalization on various weight_init ')\n",
        "\n",
        "for i, w in enumerate(weight_scale_list):\n",
        "    bn_train_acc_list = results_bn[w]\n",
        "    train_acc_list = results_no_bn[w]\n",
        "\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.title (f'W: {w:.7f}')\n",
        "    if i == 15:\n",
        "        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n",
        "        plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
        "    else:\n",
        "        plt.plot(x, bn_train_acc_list, markevery=2)\n",
        "        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
        "\n",
        "    plt.ylim(0, 1.0)\n",
        "    if i % 4:\n",
        "        plt.yticks([])\n",
        "    else:\n",
        "        plt.ylabel(\"accuracy\")\n",
        "    if i < 12:\n",
        "        plt.xticks([])\n",
        "    else:\n",
        "        plt.xlabel(\"epochs\")\n",
        "    if i==15 :\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XFw3jNpJyFAM"
      },
      "id": "XFw3jNpJyFAM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "casual-player",
      "metadata": {
        "id": "casual-player"
      },
      "source": [
        "## 6.4 바른 학습을 위하여\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4.1 오버피팅"
      ],
      "metadata": {
        "id": "SWBUitE4TwuX"
      },
      "id": "SWBUitE4TwuX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### weight_decay_lambda = 0"
      ],
      "metadata": {
        "id": "gEAoK5hcMaTB"
      },
      "id": "gEAoK5hcMaTB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "focal-twist",
      "metadata": {
        "scrolled": true,
        "id": "focal-twist"
      },
      "outputs": [],
      "source": [
        "# 6.4.1.1 오버피팅을 재현하기 위한 신경망\n",
        "# 은닉층 6개층 100개 노드\n",
        "# 은닉층 활성화 함수 : ReLU\n",
        "# 가중치 초기설정 : He\n",
        "\n",
        "# 배치 사이즈 : 100\n",
        "# 최대 반복 회수 : 201 epoch\n",
        "# 훈련방식 : SGD   학습률 0.01\n",
        "\n",
        "\n",
        "# 훈련데이터 크기 : 300\n",
        "\n",
        "# 실험 가중치 규제 사용여부에 따른 비교\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) =\\\n",
        "         load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해서 학습 데이터를 제거 (300 개 )\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# weight decay의 설정 =======================\n",
        "weight_decay_lambda = 0 # weight decay를 설정하지 않는 경우\n",
        "# weight_decay_lambda = 0.1  # wd 설정한 경우\n",
        "# ====================================================\n",
        "\n",
        "# 7층 신경망 정의, 학습방식 SGD\n",
        "network = MultiLayerNet(\n",
        "            input_size=784,\n",
        "            hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "            output_size=10,\n",
        "            weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "# 반복\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]  # (300,784)\n",
        "batch_size = 100\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "# 성능지표 저장위치 초기화\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 학습 수행\n",
        "for i in range(1_000_000_000):\n",
        "    # 1.미니배치\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 2. 기울기\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    # 3. 갱신\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    # 에폭마다 훈련 데이터 시험데이터 정확도 산출\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(f'epoch: {epoch_cnt}, train acc: {train_acc:.4f}, test acc: {test_acc:.4f}')\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.4.1.2 오버피팅을 재현하기 위한 신경망에서의 정확도 비교\n",
        "\n",
        "# 3.그래프 작성=========\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "\n",
        "plt.title ('Train vs.Test OverFit w decay = 0')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.xlim(0,200)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1yxMyEJ-KyUN"
      },
      "id": "1yxMyEJ-KyUN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "internal-functionality",
      "metadata": {
        "id": "internal-functionality"
      },
      "source": [
        "### 6.4.2 가중치 감소"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### weight_decay_lambda = 0.1"
      ],
      "metadata": {
        "id": "yDqssRSKMT2I"
      },
      "id": "yDqssRSKMT2I"
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.4.2.2 , # 6.4.2.1과 동일, weight_decay_lambda 만 적용\n",
        "# 은닉층 6개층 100개 노드\n",
        "# 은닉층 활성화 함수 : ReLU\n",
        "# 가중치 초기설정 : He\n",
        "\n",
        "# 배치 사이즈 : 100\n",
        "# 최대 반복 회수 : 201 epoch\n",
        "# 훈련방식 : SGD   학습률 0.01\n",
        "\n",
        "\n",
        "# 훈련데이터 크기 : 300\n",
        "\n",
        "# 실험 가중치 규제 사용여부에 따른 비교\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) =\\\n",
        "         load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해서 학습 데이터를 제거 (300 개 )\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# weight decay의 설정 =======================\n",
        "# weight_decay_lambda = 0 # weight decay를 설정하지 않는 경우\n",
        "weight_decay_lambda = 0.1  # wd 설정한 경우  ###\n",
        "# ====================================================\n",
        "\n",
        "# 7층 신경망 정의, 학습방식 SGD\n",
        "network = MultiLayerNet(\n",
        "            input_size=784,\n",
        "            hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "            output_size=10,\n",
        "            weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "# 반복\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]  # (300,784)\n",
        "batch_size = 100\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "# 성능지표 저장위치 초기화\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 학습 수행\n",
        "for i in range(1_000_000_000):\n",
        "    # 1.미니배치\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 2. 기울기\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    # 3. 갱신\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    # 에폭마다 훈련 데이터 시험데이터 정확도 산출\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(f'epoch: {epoch_cnt}, train acc: {train_acc:.4f}, test acc: {test_acc:.4f}')\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break"
      ],
      "metadata": {
        "id": "j_6UX8wMMpMH"
      },
      "id": "j_6UX8wMMpMH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "healthy-judgment",
      "metadata": {
        "id": "healthy-judgment"
      },
      "outputs": [],
      "source": [
        "# 6.4.2.2 오버피팅을 재현하기 위한 신경망에서의 정확도 비교 (가중치 감소 적용)\n",
        "# 3.그래프 작성=========\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "\n",
        "plt.title ('Train vs.Test OverFit w decay = 0.1')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.xlim(0,200)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "obvious-bernard",
      "metadata": {
        "id": "obvious-bernard"
      },
      "source": [
        "### 6.4.3 드롭아웃"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "female-nevada",
      "metadata": {
        "id": "female-nevada"
      },
      "outputs": [],
      "source": [
        "# 6.4.3.1 Dropout 계층 정의\n",
        "class Dropout :\n",
        "    def __init__(self, dropout_ratio = 0.5) :\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward (self, x, train_flg = True) :\n",
        "        if train_flg :\n",
        "            self.mask = np.random.rand (*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else :\n",
        "            return x * (1-self.droupout_ratio)\n",
        "\n",
        "    def backward (self, dout):\n",
        "        return dout * self.mask"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### use_dropout = False"
      ],
      "metadata": {
        "id": "AaNTNSUkMB6N"
      },
      "id": "AaNTNSUkMB6N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "changed-dictionary",
      "metadata": {
        "scrolled": true,
        "id": "changed-dictionary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81c24c1-12ed-47ad-f56b-f36b134dc55a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.4046106491859542\n",
            "=== epoch:1, train acc:0.12, test acc:0.1166 ===\n",
            "train loss:2.408717915911773\n",
            "train loss:2.27057052236926\n",
            "train loss:2.287348162023296\n",
            "=== epoch:2, train acc:0.13666666666666666, test acc:0.127 ===\n",
            "train loss:2.2961497927443957\n",
            "train loss:2.284004933031544\n",
            "train loss:2.2660487404678697\n",
            "=== epoch:3, train acc:0.17333333333333334, test acc:0.1413 ===\n",
            "train loss:2.319979098853633\n",
            "train loss:2.260459836019299\n",
            "train loss:2.2749675108400673\n",
            "=== epoch:4, train acc:0.19, test acc:0.1555 ===\n",
            "train loss:2.274162832627062\n",
            "train loss:2.218420435442812\n",
            "train loss:2.1534581062156777\n",
            "=== epoch:5, train acc:0.20666666666666667, test acc:0.1603 ===\n",
            "train loss:2.1810329336767142\n",
            "train loss:2.1570712515027974\n",
            "train loss:2.2065757560802646\n",
            "=== epoch:6, train acc:0.22333333333333333, test acc:0.1756 ===\n",
            "train loss:2.13059393122809\n",
            "train loss:2.1765533889059823\n",
            "train loss:2.205139900168881\n",
            "=== epoch:7, train acc:0.24, test acc:0.1819 ===\n",
            "train loss:2.157400886686834\n",
            "train loss:2.2008120491257994\n",
            "train loss:2.112862314465469\n",
            "=== epoch:8, train acc:0.26666666666666666, test acc:0.1961 ===\n",
            "train loss:2.115317125308109\n",
            "train loss:2.1004069915621177\n",
            "train loss:2.1559661736810853\n",
            "=== epoch:9, train acc:0.26666666666666666, test acc:0.2083 ===\n",
            "train loss:2.1925835017591377\n",
            "train loss:2.0981945937248923\n",
            "train loss:2.0898650496920395\n",
            "=== epoch:10, train acc:0.28, test acc:0.2319 ===\n",
            "train loss:2.1165301580359737\n",
            "train loss:2.0352995355212506\n",
            "train loss:2.0714046084387987\n",
            "=== epoch:11, train acc:0.30333333333333334, test acc:0.2479 ===\n",
            "train loss:2.0916990302113523\n",
            "train loss:2.0052387061806622\n",
            "train loss:1.9644910167744465\n",
            "=== epoch:12, train acc:0.3233333333333333, test acc:0.2656 ===\n",
            "train loss:2.06186998483672\n",
            "train loss:2.0406222047108864\n",
            "train loss:2.0581164039995024\n",
            "=== epoch:13, train acc:0.35333333333333333, test acc:0.2836 ===\n",
            "train loss:2.0148776279169645\n",
            "train loss:2.019368545231398\n",
            "train loss:2.024703968974774\n",
            "=== epoch:14, train acc:0.39666666666666667, test acc:0.3021 ===\n",
            "train loss:1.9066872489037496\n",
            "train loss:1.9446884310980181\n",
            "train loss:1.9601940777664297\n",
            "=== epoch:15, train acc:0.38666666666666666, test acc:0.3017 ===\n",
            "train loss:1.866674647345122\n",
            "train loss:1.820979200544117\n",
            "train loss:1.9709305792115586\n",
            "=== epoch:16, train acc:0.41333333333333333, test acc:0.316 ===\n",
            "train loss:1.936333628968454\n",
            "train loss:1.9449069486820365\n",
            "train loss:1.996796903176699\n",
            "=== epoch:17, train acc:0.43333333333333335, test acc:0.3327 ===\n",
            "train loss:1.8349724167510806\n",
            "train loss:1.8607010502347134\n",
            "train loss:1.8097973757587869\n",
            "=== epoch:18, train acc:0.45, test acc:0.3451 ===\n",
            "train loss:1.820843154175852\n",
            "train loss:1.7961494819722739\n",
            "train loss:1.7546778791605424\n",
            "=== epoch:19, train acc:0.44333333333333336, test acc:0.3381 ===\n",
            "train loss:1.8092531099551832\n",
            "train loss:1.768560654190683\n",
            "train loss:1.7769120942983698\n",
            "=== epoch:20, train acc:0.48, test acc:0.3743 ===\n",
            "train loss:1.6518399920212585\n",
            "train loss:1.7878512137228249\n",
            "train loss:1.7147910295988873\n",
            "=== epoch:21, train acc:0.49666666666666665, test acc:0.3851 ===\n",
            "train loss:1.7250270353021897\n",
            "train loss:1.656183230478412\n",
            "train loss:1.7046857489771279\n",
            "=== epoch:22, train acc:0.5, test acc:0.3981 ===\n",
            "train loss:1.6459458189830707\n",
            "train loss:1.6748476422323781\n",
            "train loss:1.7057424834432653\n",
            "=== epoch:23, train acc:0.5066666666666667, test acc:0.4043 ===\n",
            "train loss:1.604787894296508\n",
            "train loss:1.685314312787308\n",
            "train loss:1.5567646170194793\n",
            "=== epoch:24, train acc:0.5133333333333333, test acc:0.42 ===\n",
            "train loss:1.6006317055430204\n",
            "train loss:1.6567996722108111\n",
            "train loss:1.6312143322411006\n",
            "=== epoch:25, train acc:0.5233333333333333, test acc:0.4313 ===\n",
            "train loss:1.6098946822714815\n",
            "train loss:1.643763065586992\n",
            "train loss:1.598988384785396\n",
            "=== epoch:26, train acc:0.53, test acc:0.4365 ===\n",
            "train loss:1.5661678351121022\n",
            "train loss:1.5310471502729417\n",
            "train loss:1.5068780321154216\n",
            "=== epoch:27, train acc:0.52, test acc:0.4331 ===\n",
            "train loss:1.6054567045174002\n",
            "train loss:1.4519567582906479\n",
            "train loss:1.4587011413247621\n",
            "=== epoch:28, train acc:0.5433333333333333, test acc:0.4505 ===\n",
            "train loss:1.3910879887004857\n",
            "train loss:1.5338404954488452\n",
            "train loss:1.3829404913067975\n",
            "=== epoch:29, train acc:0.54, test acc:0.4581 ===\n",
            "train loss:1.42201534978837\n",
            "train loss:1.4517927729198052\n",
            "train loss:1.3265506868426957\n",
            "=== epoch:30, train acc:0.57, test acc:0.469 ===\n",
            "train loss:1.3541989613547327\n",
            "train loss:1.470660924602231\n",
            "train loss:1.2215244279330544\n",
            "=== epoch:31, train acc:0.58, test acc:0.4726 ===\n",
            "train loss:1.2814508512863683\n",
            "train loss:1.3494690977866761\n",
            "train loss:1.3988962422787135\n",
            "=== epoch:32, train acc:0.5866666666666667, test acc:0.4766 ===\n",
            "train loss:1.1951141503134475\n",
            "train loss:1.31609761279435\n",
            "train loss:1.2047176147994925\n",
            "=== epoch:33, train acc:0.6, test acc:0.4867 ===\n",
            "train loss:1.3398184368293675\n",
            "train loss:1.1110885867684033\n",
            "train loss:1.3253220189445432\n",
            "=== epoch:34, train acc:0.6066666666666667, test acc:0.4915 ===\n",
            "train loss:1.3760486209885627\n",
            "train loss:1.2068515493790961\n",
            "train loss:1.2232656356393035\n",
            "=== epoch:35, train acc:0.6533333333333333, test acc:0.5094 ===\n",
            "train loss:1.1646563756111592\n",
            "train loss:1.1434558330612006\n",
            "train loss:1.210820084348773\n",
            "=== epoch:36, train acc:0.6866666666666666, test acc:0.5269 ===\n",
            "train loss:1.2494833691838634\n",
            "train loss:1.1991538541732458\n",
            "train loss:1.069905757958616\n",
            "=== epoch:37, train acc:0.7166666666666667, test acc:0.5495 ===\n",
            "train loss:1.1010638325074287\n",
            "train loss:1.123346918841089\n",
            "train loss:1.1314644928684343\n",
            "=== epoch:38, train acc:0.71, test acc:0.5503 ===\n",
            "train loss:1.22430530977711\n",
            "train loss:1.0332786634274793\n",
            "train loss:1.0804561593407715\n",
            "=== epoch:39, train acc:0.7333333333333333, test acc:0.5742 ===\n",
            "train loss:1.0939753165409782\n",
            "train loss:0.9653191265968749\n",
            "train loss:1.0907005268172916\n",
            "=== epoch:40, train acc:0.74, test acc:0.5819 ===\n",
            "train loss:0.8864174244030749\n",
            "train loss:1.0960394944702736\n",
            "train loss:1.121871461714008\n",
            "=== epoch:41, train acc:0.7566666666666667, test acc:0.5918 ===\n",
            "train loss:0.923146095130625\n",
            "train loss:0.884855061931502\n",
            "train loss:0.908782578712049\n",
            "=== epoch:42, train acc:0.7566666666666667, test acc:0.6 ===\n",
            "train loss:0.8423011460399645\n",
            "train loss:0.9243335082801165\n",
            "train loss:0.9686824899462665\n",
            "=== epoch:43, train acc:0.7766666666666666, test acc:0.6118 ===\n",
            "train loss:0.7558504113866175\n",
            "train loss:0.9074884219118918\n",
            "train loss:0.893582802311478\n",
            "=== epoch:44, train acc:0.7833333333333333, test acc:0.6187 ===\n",
            "train loss:0.7666940568981565\n",
            "train loss:0.9016246044849416\n",
            "train loss:0.8526099266247009\n",
            "=== epoch:45, train acc:0.81, test acc:0.6408 ===\n",
            "train loss:0.7745216805058971\n",
            "train loss:0.8071316744392099\n",
            "train loss:0.8996266070435384\n",
            "=== epoch:46, train acc:0.8166666666666667, test acc:0.6543 ===\n",
            "train loss:0.8890833598996928\n",
            "train loss:0.8067455269420052\n",
            "train loss:0.8160509274686676\n",
            "=== epoch:47, train acc:0.8333333333333334, test acc:0.6412 ===\n",
            "train loss:0.8470399002167919\n",
            "train loss:0.8002423890642835\n",
            "train loss:0.6848925308423226\n",
            "=== epoch:48, train acc:0.8366666666666667, test acc:0.6497 ===\n",
            "train loss:0.6335200012624344\n",
            "train loss:0.6038165034591183\n",
            "train loss:0.7390356794582154\n",
            "=== epoch:49, train acc:0.83, test acc:0.6526 ===\n",
            "train loss:0.8346919268854969\n",
            "train loss:0.7409330784328794\n",
            "train loss:0.6999679027154752\n",
            "=== epoch:50, train acc:0.8566666666666667, test acc:0.6564 ===\n",
            "train loss:0.6135580904653831\n",
            "train loss:0.7279945844430031\n",
            "train loss:0.6902113095229291\n",
            "=== epoch:51, train acc:0.85, test acc:0.6647 ===\n",
            "train loss:0.6715399080102634\n",
            "train loss:0.6538031040761613\n",
            "train loss:0.6369788870850976\n",
            "=== epoch:52, train acc:0.8766666666666667, test acc:0.6675 ===\n",
            "train loss:0.7241864315176688\n",
            "train loss:0.6510567711316874\n",
            "train loss:0.5585288256531973\n",
            "=== epoch:53, train acc:0.87, test acc:0.6723 ===\n",
            "train loss:0.6150544065535901\n",
            "train loss:0.5292606501525722\n",
            "train loss:0.5748633498460068\n",
            "=== epoch:54, train acc:0.8733333333333333, test acc:0.6748 ===\n",
            "train loss:0.6374422644232454\n",
            "train loss:0.5556379084905907\n",
            "train loss:0.7031001652063213\n",
            "=== epoch:55, train acc:0.8766666666666667, test acc:0.6718 ===\n",
            "train loss:0.5526672535506716\n",
            "train loss:0.5767314891938049\n",
            "train loss:0.6408148759241534\n",
            "=== epoch:56, train acc:0.8833333333333333, test acc:0.6848 ===\n",
            "train loss:0.6148483438115804\n",
            "train loss:0.6423673288621535\n",
            "train loss:0.5200329529753636\n",
            "=== epoch:57, train acc:0.8933333333333333, test acc:0.683 ===\n",
            "train loss:0.520238606690333\n",
            "train loss:0.44856376834924333\n",
            "train loss:0.5053822352798499\n",
            "=== epoch:58, train acc:0.9066666666666666, test acc:0.6764 ===\n",
            "train loss:0.5154077462721006\n",
            "train loss:0.5461862891741482\n",
            "train loss:0.4641485826959664\n",
            "=== epoch:59, train acc:0.9033333333333333, test acc:0.6835 ===\n",
            "train loss:0.3810675272638218\n",
            "train loss:0.43394777738777246\n",
            "train loss:0.5481427382448801\n",
            "=== epoch:60, train acc:0.91, test acc:0.6825 ===\n",
            "train loss:0.4847665245814276\n",
            "train loss:0.5086082358212614\n",
            "train loss:0.4964261405737386\n",
            "=== epoch:61, train acc:0.92, test acc:0.6821 ===\n",
            "train loss:0.3499702386797452\n",
            "train loss:0.47628575626421876\n",
            "train loss:0.46221098034960106\n",
            "=== epoch:62, train acc:0.9, test acc:0.6884 ===\n",
            "train loss:0.40777701914582354\n",
            "train loss:0.5426194969450809\n",
            "train loss:0.40511031057025854\n",
            "=== epoch:63, train acc:0.9133333333333333, test acc:0.6896 ===\n",
            "train loss:0.4087705446214215\n",
            "train loss:0.3917737863895794\n",
            "train loss:0.43393900323552065\n",
            "=== epoch:64, train acc:0.9233333333333333, test acc:0.6848 ===\n",
            "train loss:0.45011616801077453\n",
            "train loss:0.392357443135673\n",
            "train loss:0.4298153439096168\n",
            "=== epoch:65, train acc:0.92, test acc:0.7023 ===\n",
            "train loss:0.42261000018733397\n",
            "train loss:0.342121372778847\n",
            "train loss:0.3968347094203579\n",
            "=== epoch:66, train acc:0.9333333333333333, test acc:0.6931 ===\n",
            "train loss:0.3852103432791217\n",
            "train loss:0.3472485012647163\n",
            "train loss:0.42988006497360876\n",
            "=== epoch:67, train acc:0.93, test acc:0.6994 ===\n",
            "train loss:0.3504217012816918\n",
            "train loss:0.4317088969254965\n",
            "train loss:0.3277713722724291\n",
            "=== epoch:68, train acc:0.9266666666666666, test acc:0.6961 ===\n",
            "train loss:0.4341627219716761\n",
            "train loss:0.34395037099721953\n",
            "train loss:0.31140659219739614\n",
            "=== epoch:69, train acc:0.9266666666666666, test acc:0.6937 ===\n",
            "train loss:0.35366931421348374\n",
            "train loss:0.3787606877223881\n",
            "train loss:0.295730552233089\n",
            "=== epoch:70, train acc:0.9233333333333333, test acc:0.7028 ===\n",
            "train loss:0.34550712743763945\n",
            "train loss:0.3264522510759849\n",
            "train loss:0.32238345395377993\n",
            "=== epoch:71, train acc:0.9433333333333334, test acc:0.7 ===\n",
            "train loss:0.26975861182577643\n",
            "train loss:0.30021692418236573\n",
            "train loss:0.31676229235195236\n",
            "=== epoch:72, train acc:0.9333333333333333, test acc:0.6986 ===\n",
            "train loss:0.3289678180161837\n",
            "train loss:0.3241827482985722\n",
            "train loss:0.3205095918758514\n",
            "=== epoch:73, train acc:0.9433333333333334, test acc:0.7007 ===\n",
            "train loss:0.33326024232802437\n",
            "train loss:0.3460250674828267\n",
            "train loss:0.27711940760705456\n",
            "=== epoch:74, train acc:0.9433333333333334, test acc:0.7005 ===\n",
            "train loss:0.2756405632048551\n",
            "train loss:0.348612670318405\n",
            "train loss:0.24748006413579923\n",
            "=== epoch:75, train acc:0.9533333333333334, test acc:0.7015 ===\n",
            "train loss:0.2317184017590834\n",
            "train loss:0.22814884303100272\n",
            "train loss:0.31559033631952416\n",
            "=== epoch:76, train acc:0.9533333333333334, test acc:0.7066 ===\n",
            "train loss:0.23798221537469227\n",
            "train loss:0.3579674948458317\n",
            "train loss:0.28243585661842663\n",
            "=== epoch:77, train acc:0.95, test acc:0.6997 ===\n",
            "train loss:0.23769403635743977\n",
            "train loss:0.2733937408574396\n",
            "train loss:0.216356072724206\n",
            "=== epoch:78, train acc:0.9666666666666667, test acc:0.6988 ===\n",
            "train loss:0.26430817346888436\n",
            "train loss:0.2532087975209986\n",
            "train loss:0.20081711567440613\n",
            "=== epoch:79, train acc:0.9666666666666667, test acc:0.7116 ===\n",
            "train loss:0.2461619298083467\n",
            "train loss:0.30515306447858837\n",
            "train loss:0.20578929416835964\n",
            "=== epoch:80, train acc:0.96, test acc:0.7088 ===\n",
            "train loss:0.23021916719577062\n",
            "train loss:0.2092898324246371\n",
            "train loss:0.247841901784012\n",
            "=== epoch:81, train acc:0.96, test acc:0.7007 ===\n",
            "train loss:0.19064933397203654\n",
            "train loss:0.26533538851180355\n",
            "train loss:0.17963084689937173\n",
            "=== epoch:82, train acc:0.9733333333333334, test acc:0.708 ===\n",
            "train loss:0.19622707663766314\n",
            "train loss:0.19245502078284435\n",
            "train loss:0.149393413022186\n",
            "=== epoch:83, train acc:0.97, test acc:0.7098 ===\n",
            "train loss:0.2853427834397215\n",
            "train loss:0.23936043884228855\n",
            "train loss:0.2264063726798385\n",
            "=== epoch:84, train acc:0.9833333333333333, test acc:0.7026 ===\n",
            "train loss:0.23100685388454026\n",
            "train loss:0.21036843983990117\n",
            "train loss:0.22342063221639394\n",
            "=== epoch:85, train acc:0.98, test acc:0.7099 ===\n",
            "train loss:0.17968872945608996\n",
            "train loss:0.14937573430211848\n",
            "train loss:0.2295318776916288\n",
            "=== epoch:86, train acc:0.9766666666666667, test acc:0.7068 ===\n",
            "train loss:0.16441172868887485\n",
            "train loss:0.20815471669541558\n",
            "train loss:0.16064009994423067\n",
            "=== epoch:87, train acc:0.98, test acc:0.7053 ===\n",
            "train loss:0.24123447120062785\n",
            "train loss:0.19234711886990646\n",
            "train loss:0.17011481570144674\n",
            "=== epoch:88, train acc:0.9866666666666667, test acc:0.7124 ===\n",
            "train loss:0.17000395847610006\n",
            "train loss:0.22326264674574628\n",
            "train loss:0.12945141884723713\n",
            "=== epoch:89, train acc:0.98, test acc:0.7091 ===\n",
            "train loss:0.2129451712612933\n",
            "train loss:0.11365792662504762\n",
            "train loss:0.16438258069588169\n",
            "=== epoch:90, train acc:0.98, test acc:0.7107 ===\n",
            "train loss:0.14850054801555204\n",
            "train loss:0.19203152619569697\n",
            "train loss:0.1590093203767139\n",
            "=== epoch:91, train acc:0.9866666666666667, test acc:0.7126 ===\n",
            "train loss:0.15080328860888423\n",
            "train loss:0.129773587510442\n",
            "train loss:0.2052885326449033\n",
            "=== epoch:92, train acc:0.99, test acc:0.7089 ===\n",
            "train loss:0.18378751640778346\n",
            "train loss:0.1479055632388252\n",
            "train loss:0.1387135645499579\n",
            "=== epoch:93, train acc:0.9866666666666667, test acc:0.7161 ===\n",
            "train loss:0.12088442144565052\n",
            "train loss:0.1658508658925867\n",
            "train loss:0.12748994144231549\n",
            "=== epoch:94, train acc:0.9833333333333333, test acc:0.7191 ===\n",
            "train loss:0.1358453832720078\n",
            "train loss:0.1315784811155999\n",
            "train loss:0.14173440110232982\n",
            "=== epoch:95, train acc:0.99, test acc:0.7229 ===\n",
            "train loss:0.14700230585567078\n",
            "train loss:0.14719075661569364\n",
            "train loss:0.11774447356698067\n",
            "=== epoch:96, train acc:0.99, test acc:0.7177 ===\n",
            "train loss:0.136971579066972\n",
            "train loss:0.14274669709162055\n",
            "train loss:0.11263266963357048\n",
            "=== epoch:97, train acc:0.99, test acc:0.7164 ===\n",
            "train loss:0.1217944770995226\n",
            "train loss:0.12588906111659465\n",
            "train loss:0.12286944218239886\n",
            "=== epoch:98, train acc:0.99, test acc:0.7162 ===\n",
            "train loss:0.1445756974168264\n",
            "train loss:0.10173997253743984\n",
            "train loss:0.1284399206858618\n",
            "=== epoch:99, train acc:0.9866666666666667, test acc:0.7231 ===\n",
            "train loss:0.12586194350944965\n",
            "train loss:0.12424601075090143\n",
            "train loss:0.08793509825906186\n",
            "=== epoch:100, train acc:0.9866666666666667, test acc:0.7267 ===\n",
            "train loss:0.118417284951965\n",
            "train loss:0.10423743715358309\n",
            "train loss:0.16499226360748945\n",
            "=== epoch:101, train acc:0.99, test acc:0.723 ===\n",
            "train loss:0.08960776652073363\n",
            "train loss:0.08725940975346959\n",
            "train loss:0.11741753362935622\n",
            "=== epoch:102, train acc:0.99, test acc:0.7167 ===\n",
            "train loss:0.09001309396026719\n",
            "train loss:0.13695447179540093\n",
            "train loss:0.15269730090414466\n",
            "=== epoch:103, train acc:0.9933333333333333, test acc:0.7208 ===\n",
            "train loss:0.11699071208203185\n",
            "train loss:0.1256065419592983\n",
            "train loss:0.12862411743976115\n",
            "=== epoch:104, train acc:0.9933333333333333, test acc:0.7205 ===\n",
            "train loss:0.1271757287535662\n",
            "train loss:0.0910788317067217\n",
            "train loss:0.13211496238325612\n",
            "=== epoch:105, train acc:0.9966666666666667, test acc:0.7244 ===\n",
            "train loss:0.14884657795969955\n",
            "train loss:0.0882931652109649\n",
            "train loss:0.1373884756083314\n",
            "=== epoch:106, train acc:0.9966666666666667, test acc:0.728 ===\n",
            "train loss:0.0984315868650222\n",
            "train loss:0.08965869918820456\n",
            "train loss:0.15735569034525085\n",
            "=== epoch:107, train acc:0.9966666666666667, test acc:0.7276 ===\n",
            "train loss:0.13956403155522407\n",
            "train loss:0.10722993955363\n",
            "train loss:0.07976313632827382\n",
            "=== epoch:108, train acc:0.9966666666666667, test acc:0.728 ===\n",
            "train loss:0.08346034473117138\n",
            "train loss:0.09228784046865818\n",
            "train loss:0.08312972762490015\n",
            "=== epoch:109, train acc:0.9966666666666667, test acc:0.7265 ===\n",
            "train loss:0.13285626365042869\n",
            "train loss:0.08892089010007596\n",
            "train loss:0.09181573316934628\n",
            "=== epoch:110, train acc:0.9966666666666667, test acc:0.7288 ===\n",
            "train loss:0.12258133186422687\n",
            "train loss:0.11631833747374527\n",
            "train loss:0.08311218253384696\n",
            "=== epoch:111, train acc:0.9966666666666667, test acc:0.7295 ===\n",
            "train loss:0.0887853785153794\n",
            "train loss:0.07531111734787155\n",
            "train loss:0.07107565320893404\n",
            "=== epoch:112, train acc:0.9966666666666667, test acc:0.7238 ===\n",
            "train loss:0.07538766582409899\n",
            "train loss:0.10425960343086826\n",
            "train loss:0.14546969950889668\n",
            "=== epoch:113, train acc:0.9966666666666667, test acc:0.7233 ===\n",
            "train loss:0.08558225807136777\n",
            "train loss:0.07304442598970526\n",
            "train loss:0.08549925092238263\n",
            "=== epoch:114, train acc:0.9966666666666667, test acc:0.7245 ===\n",
            "train loss:0.08512050733276595\n",
            "train loss:0.139709959683944\n",
            "train loss:0.08634590745987206\n",
            "=== epoch:115, train acc:0.9966666666666667, test acc:0.7232 ===\n",
            "train loss:0.09418491270925332\n",
            "train loss:0.05340732794386148\n",
            "train loss:0.07815303184987375\n",
            "=== epoch:116, train acc:0.9966666666666667, test acc:0.7263 ===\n",
            "train loss:0.07210816937205618\n",
            "train loss:0.09062250533401789\n",
            "train loss:0.07406400707683035\n",
            "=== epoch:117, train acc:0.9966666666666667, test acc:0.7296 ===\n",
            "train loss:0.05646353731920042\n",
            "train loss:0.1200289700865272\n",
            "train loss:0.08985158675675044\n",
            "=== epoch:118, train acc:0.9966666666666667, test acc:0.7319 ===\n",
            "train loss:0.08755928773690763\n",
            "train loss:0.07139710552683798\n",
            "train loss:0.06505722670972446\n",
            "=== epoch:119, train acc:0.9966666666666667, test acc:0.7328 ===\n",
            "train loss:0.05915771253345301\n",
            "train loss:0.06878315072725898\n",
            "train loss:0.07671687822961151\n",
            "=== epoch:120, train acc:0.9966666666666667, test acc:0.7281 ===\n",
            "train loss:0.08077095713151866\n",
            "train loss:0.08712506730044144\n",
            "train loss:0.08772692783372582\n",
            "=== epoch:121, train acc:0.9966666666666667, test acc:0.7283 ===\n",
            "train loss:0.08049976460617789\n",
            "train loss:0.056402964352660984\n",
            "train loss:0.06807524386132872\n",
            "=== epoch:122, train acc:0.9966666666666667, test acc:0.7322 ===\n",
            "train loss:0.05835988184470642\n",
            "train loss:0.099869553738982\n",
            "train loss:0.06819693586812398\n",
            "=== epoch:123, train acc:0.9966666666666667, test acc:0.7283 ===\n",
            "train loss:0.07507676365476482\n",
            "train loss:0.0745682264693401\n",
            "train loss:0.06530340284851971\n",
            "=== epoch:124, train acc:0.9966666666666667, test acc:0.7304 ===\n",
            "train loss:0.05383033504431408\n",
            "train loss:0.08726342724916228\n",
            "train loss:0.0688434447568277\n",
            "=== epoch:125, train acc:0.9966666666666667, test acc:0.7294 ===\n",
            "train loss:0.07222126824056639\n",
            "train loss:0.06313904594602433\n",
            "train loss:0.05888145363428797\n",
            "=== epoch:126, train acc:0.9966666666666667, test acc:0.7354 ===\n",
            "train loss:0.09545735057100418\n",
            "train loss:0.06070137083935838\n",
            "train loss:0.07111749201017828\n",
            "=== epoch:127, train acc:0.9966666666666667, test acc:0.734 ===\n",
            "train loss:0.06935244902071046\n",
            "train loss:0.06174862725432371\n",
            "train loss:0.048889639251698404\n",
            "=== epoch:128, train acc:0.9966666666666667, test acc:0.7325 ===\n",
            "train loss:0.05193002564651697\n",
            "train loss:0.051257927858226617\n",
            "train loss:0.06185978973725539\n",
            "=== epoch:129, train acc:0.9966666666666667, test acc:0.7341 ===\n",
            "train loss:0.06407907310205394\n",
            "train loss:0.05201550120051621\n",
            "train loss:0.05702336646759498\n",
            "=== epoch:130, train acc:0.9966666666666667, test acc:0.7331 ===\n",
            "train loss:0.060163318467094086\n",
            "train loss:0.05370408734471514\n",
            "train loss:0.07673768618475489\n",
            "=== epoch:131, train acc:0.9966666666666667, test acc:0.7359 ===\n",
            "train loss:0.07232878330224375\n",
            "train loss:0.0756096524245926\n",
            "train loss:0.05519320277962968\n",
            "=== epoch:132, train acc:0.9966666666666667, test acc:0.7314 ===\n",
            "train loss:0.05483198576153682\n",
            "train loss:0.06754766781608992\n",
            "train loss:0.051454980596737414\n",
            "=== epoch:133, train acc:0.9966666666666667, test acc:0.7317 ===\n",
            "train loss:0.06204781223895388\n",
            "train loss:0.0791773464014775\n",
            "train loss:0.06954100875033596\n",
            "=== epoch:134, train acc:0.9966666666666667, test acc:0.7313 ===\n",
            "train loss:0.044655910339760026\n",
            "train loss:0.057542372446654264\n",
            "train loss:0.04405421846118623\n",
            "=== epoch:135, train acc:0.9966666666666667, test acc:0.7314 ===\n",
            "train loss:0.04872217305717951\n",
            "train loss:0.04699463440125982\n",
            "train loss:0.05091788590867099\n",
            "=== epoch:136, train acc:0.9966666666666667, test acc:0.7322 ===\n",
            "train loss:0.05707529314254039\n",
            "train loss:0.052865002573143735\n",
            "train loss:0.03893614223367447\n",
            "=== epoch:137, train acc:0.9966666666666667, test acc:0.7313 ===\n",
            "train loss:0.05982395658849114\n",
            "train loss:0.0496380216353403\n",
            "train loss:0.05297412825030426\n",
            "=== epoch:138, train acc:0.9966666666666667, test acc:0.732 ===\n",
            "train loss:0.062410226954554275\n",
            "train loss:0.0597903066249019\n",
            "train loss:0.06785769221327151\n",
            "=== epoch:139, train acc:0.9966666666666667, test acc:0.7319 ===\n",
            "train loss:0.049897510858166426\n",
            "train loss:0.04773618344192774\n",
            "train loss:0.06462745613463389\n",
            "=== epoch:140, train acc:1.0, test acc:0.7299 ===\n",
            "train loss:0.054022985922919\n",
            "train loss:0.05702678211602552\n",
            "train loss:0.05267305409274611\n",
            "=== epoch:141, train acc:0.9966666666666667, test acc:0.7325 ===\n",
            "train loss:0.044597571267737085\n",
            "train loss:0.0470396000801201\n",
            "train loss:0.041183001307451325\n",
            "=== epoch:142, train acc:0.9966666666666667, test acc:0.7335 ===\n",
            "train loss:0.042408962946917074\n",
            "train loss:0.05570671678213965\n",
            "train loss:0.055219404883066286\n",
            "=== epoch:143, train acc:0.9966666666666667, test acc:0.7319 ===\n",
            "train loss:0.036481190129660605\n",
            "train loss:0.038684285783158155\n",
            "train loss:0.054706713119509774\n",
            "=== epoch:144, train acc:0.9966666666666667, test acc:0.7325 ===\n",
            "train loss:0.06454059117566403\n",
            "train loss:0.046219368951540066\n",
            "train loss:0.042280545058036745\n",
            "=== epoch:145, train acc:0.9966666666666667, test acc:0.734 ===\n",
            "train loss:0.038091936181029025\n",
            "train loss:0.049649264964014464\n",
            "train loss:0.03387888706193737\n",
            "=== epoch:146, train acc:0.9966666666666667, test acc:0.7323 ===\n",
            "train loss:0.039398264658326416\n",
            "train loss:0.04014663059398884\n",
            "train loss:0.03699820617562196\n",
            "=== epoch:147, train acc:0.9966666666666667, test acc:0.7338 ===\n",
            "train loss:0.05345503550299828\n",
            "train loss:0.050734220140502294\n",
            "train loss:0.036593914344592166\n",
            "=== epoch:148, train acc:0.9966666666666667, test acc:0.7325 ===\n",
            "train loss:0.062063528762774496\n",
            "train loss:0.03675557712850857\n",
            "train loss:0.04467891013718428\n",
            "=== epoch:149, train acc:0.9966666666666667, test acc:0.7331 ===\n",
            "train loss:0.037204183251562474\n",
            "train loss:0.043241379119545904\n",
            "train loss:0.03515829266659076\n",
            "=== epoch:150, train acc:1.0, test acc:0.7315 ===\n",
            "train loss:0.036008342666883936\n",
            "train loss:0.055056380789935154\n",
            "train loss:0.039525112952205226\n",
            "=== epoch:151, train acc:1.0, test acc:0.7312 ===\n",
            "train loss:0.0342605809458535\n",
            "train loss:0.028686775813579767\n",
            "train loss:0.03144319951511114\n",
            "=== epoch:152, train acc:0.9966666666666667, test acc:0.7328 ===\n",
            "train loss:0.031231092315012036\n",
            "train loss:0.04169833827149691\n",
            "train loss:0.03403880609709977\n",
            "=== epoch:153, train acc:0.9966666666666667, test acc:0.737 ===\n",
            "train loss:0.029533339484939067\n",
            "train loss:0.052569267335782835\n",
            "train loss:0.030283140961672884\n",
            "=== epoch:154, train acc:0.9966666666666667, test acc:0.7334 ===\n",
            "train loss:0.0536379864028584\n",
            "train loss:0.04083236696858471\n",
            "train loss:0.0563939419200179\n",
            "=== epoch:155, train acc:1.0, test acc:0.7326 ===\n",
            "train loss:0.02929623387622138\n",
            "train loss:0.03539098646989343\n",
            "train loss:0.03366658804769141\n",
            "=== epoch:156, train acc:0.9966666666666667, test acc:0.7339 ===\n",
            "train loss:0.039900235622588\n",
            "train loss:0.03215626461826\n",
            "train loss:0.028145266387366853\n",
            "=== epoch:157, train acc:0.9966666666666667, test acc:0.7357 ===\n",
            "train loss:0.031774072925984026\n",
            "train loss:0.04742836058644397\n",
            "train loss:0.02622003275534825\n",
            "=== epoch:158, train acc:0.9966666666666667, test acc:0.7358 ===\n",
            "train loss:0.02681071032505663\n",
            "train loss:0.03188553573789418\n",
            "train loss:0.04105258527172142\n",
            "=== epoch:159, train acc:0.9966666666666667, test acc:0.7368 ===\n",
            "train loss:0.04981767974125931\n",
            "train loss:0.032484307465263124\n",
            "train loss:0.030131171444374015\n",
            "=== epoch:160, train acc:0.9966666666666667, test acc:0.7366 ===\n",
            "train loss:0.02792744431138364\n",
            "train loss:0.030331165833521512\n",
            "train loss:0.032944156404836805\n",
            "=== epoch:161, train acc:0.9966666666666667, test acc:0.7348 ===\n",
            "train loss:0.04269633276681097\n",
            "train loss:0.02317567509814171\n",
            "train loss:0.027415587909411964\n",
            "=== epoch:162, train acc:0.9966666666666667, test acc:0.735 ===\n",
            "train loss:0.03359624276799641\n",
            "train loss:0.03438870049543799\n",
            "train loss:0.04009641912243465\n",
            "=== epoch:163, train acc:0.9966666666666667, test acc:0.7353 ===\n",
            "train loss:0.033014235497402726\n",
            "train loss:0.028088101527267418\n",
            "train loss:0.04510740598103372\n",
            "=== epoch:164, train acc:1.0, test acc:0.7357 ===\n",
            "train loss:0.030402525262013872\n",
            "train loss:0.026430755448500768\n",
            "train loss:0.037102082989211645\n",
            "=== epoch:165, train acc:1.0, test acc:0.7336 ===\n",
            "train loss:0.03928215891559038\n",
            "train loss:0.027337220485272505\n",
            "train loss:0.024362964583146875\n",
            "=== epoch:166, train acc:0.9966666666666667, test acc:0.7336 ===\n",
            "train loss:0.03885714543538225\n",
            "train loss:0.03519982280267585\n",
            "train loss:0.034731209291031774\n",
            "=== epoch:167, train acc:1.0, test acc:0.7343 ===\n",
            "train loss:0.03639610786049798\n",
            "train loss:0.03073272871112967\n",
            "train loss:0.030474649381629217\n",
            "=== epoch:168, train acc:1.0, test acc:0.7361 ===\n",
            "train loss:0.025676817236937707\n",
            "train loss:0.03755877213011768\n",
            "train loss:0.023689714893680765\n",
            "=== epoch:169, train acc:1.0, test acc:0.7349 ===\n",
            "train loss:0.027437827901152425\n",
            "train loss:0.026611910075254142\n",
            "train loss:0.02557549333800871\n",
            "=== epoch:170, train acc:1.0, test acc:0.7375 ===\n",
            "train loss:0.03250761919741676\n",
            "train loss:0.02290848551499109\n",
            "train loss:0.028393828814185867\n",
            "=== epoch:171, train acc:1.0, test acc:0.738 ===\n",
            "train loss:0.022797880746500514\n",
            "train loss:0.023904543224306782\n",
            "train loss:0.02157868609725657\n",
            "=== epoch:172, train acc:0.9966666666666667, test acc:0.7376 ===\n",
            "train loss:0.028753949809239832\n",
            "train loss:0.03853379723110502\n",
            "train loss:0.027169123125854323\n",
            "=== epoch:173, train acc:1.0, test acc:0.7361 ===\n",
            "train loss:0.02801705691456725\n",
            "train loss:0.024400808511133312\n",
            "train loss:0.023963219309587225\n",
            "=== epoch:174, train acc:1.0, test acc:0.736 ===\n",
            "train loss:0.027235296564176507\n",
            "train loss:0.024793827258869628\n",
            "train loss:0.03452825746393592\n",
            "=== epoch:175, train acc:1.0, test acc:0.7356 ===\n",
            "train loss:0.04485007499992559\n",
            "train loss:0.02541359000056338\n",
            "train loss:0.03205784578979642\n",
            "=== epoch:176, train acc:1.0, test acc:0.7323 ===\n",
            "train loss:0.03244440895864395\n",
            "train loss:0.035445062748804775\n",
            "train loss:0.024592488596025405\n",
            "=== epoch:177, train acc:1.0, test acc:0.7352 ===\n",
            "train loss:0.02406245230662194\n",
            "train loss:0.026679026102340297\n",
            "train loss:0.032752768930686495\n",
            "=== epoch:178, train acc:1.0, test acc:0.7333 ===\n",
            "train loss:0.022326723112970834\n",
            "train loss:0.028621005494857618\n",
            "train loss:0.02497216823426104\n",
            "=== epoch:179, train acc:1.0, test acc:0.7377 ===\n",
            "train loss:0.02445979594086983\n",
            "train loss:0.035745404208783506\n",
            "train loss:0.029861198957119667\n",
            "=== epoch:180, train acc:1.0, test acc:0.7354 ===\n",
            "train loss:0.03212453286129951\n",
            "train loss:0.02496986047380132\n",
            "train loss:0.032346710978610346\n",
            "=== epoch:181, train acc:1.0, test acc:0.7353 ===\n",
            "train loss:0.02016618420627162\n",
            "train loss:0.022854504252827\n",
            "train loss:0.027313237761541386\n",
            "=== epoch:182, train acc:1.0, test acc:0.7368 ===\n",
            "train loss:0.0264858445415571\n",
            "train loss:0.02191937518126197\n",
            "train loss:0.02978961688146736\n",
            "=== epoch:183, train acc:1.0, test acc:0.7338 ===\n",
            "train loss:0.01791517541251191\n",
            "train loss:0.02938798936976729\n",
            "train loss:0.024722801261718483\n",
            "=== epoch:184, train acc:1.0, test acc:0.7353 ===\n",
            "train loss:0.02559826118610633\n",
            "train loss:0.02497238542492801\n",
            "train loss:0.017428396804637942\n",
            "=== epoch:185, train acc:1.0, test acc:0.7369 ===\n",
            "train loss:0.031002172844496717\n",
            "train loss:0.03162259507302601\n",
            "train loss:0.023027897058413106\n",
            "=== epoch:186, train acc:1.0, test acc:0.7375 ===\n",
            "train loss:0.02556985288068605\n",
            "train loss:0.02309811040168855\n",
            "train loss:0.022317980169948924\n",
            "=== epoch:187, train acc:1.0, test acc:0.7369 ===\n",
            "train loss:0.022363572044962643\n",
            "train loss:0.022201081802203836\n",
            "train loss:0.022749556406517837\n",
            "=== epoch:188, train acc:1.0, test acc:0.7367 ===\n",
            "train loss:0.025653590305129752\n",
            "train loss:0.03923866312727113\n",
            "train loss:0.025338094892328748\n",
            "=== epoch:189, train acc:1.0, test acc:0.7369 ===\n",
            "train loss:0.02046174815499842\n",
            "train loss:0.030952983914501738\n",
            "train loss:0.020160052704431553\n",
            "=== epoch:190, train acc:1.0, test acc:0.736 ===\n",
            "train loss:0.020114229107045623\n",
            "train loss:0.023591444146834468\n",
            "train loss:0.02703137900441315\n",
            "=== epoch:191, train acc:1.0, test acc:0.734 ===\n",
            "train loss:0.021490956040235917\n",
            "train loss:0.020816233087860667\n",
            "train loss:0.021286596963152684\n",
            "=== epoch:192, train acc:1.0, test acc:0.7354 ===\n",
            "train loss:0.019692496694049613\n",
            "train loss:0.0178362098657199\n",
            "train loss:0.02277336287383023\n",
            "=== epoch:193, train acc:1.0, test acc:0.7368 ===\n",
            "train loss:0.023826694728359632\n",
            "train loss:0.025393638012559398\n",
            "train loss:0.02085251391050483\n",
            "=== epoch:194, train acc:1.0, test acc:0.7353 ===\n",
            "train loss:0.022516695497659733\n",
            "train loss:0.02772858277690518\n",
            "train loss:0.02190226940130458\n",
            "=== epoch:195, train acc:1.0, test acc:0.7335 ===\n",
            "train loss:0.030865971240078897\n",
            "train loss:0.015721620943502903\n",
            "train loss:0.0217786925227284\n",
            "=== epoch:196, train acc:1.0, test acc:0.7305 ===\n",
            "train loss:0.028162635247566382\n",
            "train loss:0.02037455568100694\n",
            "train loss:0.024197792004855327\n",
            "=== epoch:197, train acc:1.0, test acc:0.7333 ===\n",
            "train loss:0.022443420353685434\n",
            "train loss:0.023701231912043035\n",
            "train loss:0.018074068867391026\n",
            "=== epoch:198, train acc:1.0, test acc:0.7371 ===\n",
            "train loss:0.02851636163779621\n",
            "train loss:0.01859741517572294\n",
            "train loss:0.025735807797441064\n",
            "=== epoch:199, train acc:1.0, test acc:0.7354 ===\n",
            "train loss:0.020746405436169107\n",
            "train loss:0.020711458508425457\n",
            "train loss:0.018366333632204395\n",
            "=== epoch:200, train acc:1.0, test acc:0.7379 ===\n",
            "train loss:0.01902557962447647\n",
            "train loss:0.02008901688189739\n",
            "train loss:0.017567261894212486\n",
            "=== epoch:201, train acc:1.0, test acc:0.7373 ===\n",
            "train loss:0.019789943485723865\n",
            "train loss:0.019473693470611224\n",
            "train loss:0.019732301377494186\n",
            "=== epoch:202, train acc:1.0, test acc:0.7379 ===\n",
            "train loss:0.022592214402936697\n",
            "train loss:0.02450468394589628\n",
            "train loss:0.01614565607028736\n",
            "=== epoch:203, train acc:1.0, test acc:0.7391 ===\n",
            "train loss:0.014815212520773757\n",
            "train loss:0.023248863733234384\n",
            "train loss:0.025237827193075928\n",
            "=== epoch:204, train acc:1.0, test acc:0.7378 ===\n",
            "train loss:0.019082352864582482\n",
            "train loss:0.016481946094463747\n",
            "train loss:0.018822941281923782\n",
            "=== epoch:205, train acc:1.0, test acc:0.7384 ===\n",
            "train loss:0.030213866615979326\n",
            "train loss:0.016813234212909978\n",
            "train loss:0.01787177613570587\n",
            "=== epoch:206, train acc:1.0, test acc:0.7383 ===\n",
            "train loss:0.014923927642790286\n",
            "train loss:0.0186977069033362\n",
            "train loss:0.018273324008727575\n",
            "=== epoch:207, train acc:1.0, test acc:0.7382 ===\n",
            "train loss:0.02152859069625887\n",
            "train loss:0.01811707265282332\n",
            "train loss:0.010522962736175734\n",
            "=== epoch:208, train acc:1.0, test acc:0.7369 ===\n",
            "train loss:0.015973316158768895\n",
            "train loss:0.021472016365419892\n",
            "train loss:0.018190812794407513\n",
            "=== epoch:209, train acc:1.0, test acc:0.7371 ===\n",
            "train loss:0.018866039562239905\n",
            "train loss:0.024404643961921994\n",
            "train loss:0.017553499429889433\n",
            "=== epoch:210, train acc:1.0, test acc:0.7368 ===\n",
            "train loss:0.019779794764000755\n",
            "train loss:0.021187532519573674\n",
            "train loss:0.020161551930965346\n",
            "=== epoch:211, train acc:1.0, test acc:0.7366 ===\n",
            "train loss:0.01550410204388115\n",
            "train loss:0.018043745733402217\n",
            "train loss:0.014244127362536767\n",
            "=== epoch:212, train acc:1.0, test acc:0.7369 ===\n",
            "train loss:0.01896679845877864\n",
            "train loss:0.014678183142052367\n",
            "train loss:0.016727117231989157\n",
            "=== epoch:213, train acc:1.0, test acc:0.7365 ===\n",
            "train loss:0.017201383676220303\n",
            "train loss:0.018550401496665744\n",
            "train loss:0.017251004946262898\n",
            "=== epoch:214, train acc:1.0, test acc:0.7376 ===\n",
            "train loss:0.01752835169906759\n",
            "train loss:0.016613779883332607\n",
            "train loss:0.010713570139825894\n",
            "=== epoch:215, train acc:1.0, test acc:0.7392 ===\n",
            "train loss:0.025986471496739164\n",
            "train loss:0.015681634538503304\n",
            "train loss:0.01761313163262301\n",
            "=== epoch:216, train acc:1.0, test acc:0.7373 ===\n",
            "train loss:0.01611444199845142\n",
            "train loss:0.01811024438357338\n",
            "train loss:0.014350505459802052\n",
            "=== epoch:217, train acc:1.0, test acc:0.7379 ===\n",
            "train loss:0.01441605153078081\n",
            "train loss:0.015659196636606983\n",
            "train loss:0.009315039281696726\n",
            "=== epoch:218, train acc:1.0, test acc:0.738 ===\n",
            "train loss:0.019063531152249305\n",
            "train loss:0.014345585785193076\n",
            "train loss:0.0207988205364079\n",
            "=== epoch:219, train acc:1.0, test acc:0.7368 ===\n",
            "train loss:0.02067768832959736\n",
            "train loss:0.015468313819282199\n",
            "train loss:0.016580675284549807\n",
            "=== epoch:220, train acc:1.0, test acc:0.7361 ===\n",
            "train loss:0.016980058014720342\n",
            "train loss:0.01914428764919397\n",
            "train loss:0.018551477200047604\n",
            "=== epoch:221, train acc:1.0, test acc:0.7332 ===\n",
            "train loss:0.02105771399164664\n",
            "train loss:0.021408048090550528\n",
            "train loss:0.012055560914189635\n",
            "=== epoch:222, train acc:1.0, test acc:0.7351 ===\n",
            "train loss:0.015939011041116138\n",
            "train loss:0.016813752304676344\n",
            "train loss:0.013747445328896744\n",
            "=== epoch:223, train acc:1.0, test acc:0.7374 ===\n",
            "train loss:0.01583089192621147\n",
            "train loss:0.016981523740230268\n",
            "train loss:0.01563601421932205\n",
            "=== epoch:224, train acc:1.0, test acc:0.7373 ===\n",
            "train loss:0.015161097839593804\n",
            "train loss:0.014696881276820945\n",
            "train loss:0.014883162350633986\n",
            "=== epoch:225, train acc:1.0, test acc:0.738 ===\n",
            "train loss:0.016487706820831106\n",
            "train loss:0.012861244703873948\n",
            "train loss:0.01774865767370409\n",
            "=== epoch:226, train acc:1.0, test acc:0.739 ===\n",
            "train loss:0.017812311348628554\n",
            "train loss:0.014083379524016247\n",
            "train loss:0.014646778725642082\n",
            "=== epoch:227, train acc:1.0, test acc:0.7387 ===\n",
            "train loss:0.022438640559721112\n",
            "train loss:0.013273098583412126\n",
            "train loss:0.012526904421597797\n",
            "=== epoch:228, train acc:1.0, test acc:0.738 ===\n",
            "train loss:0.015340854578578906\n",
            "train loss:0.016567718490634004\n",
            "train loss:0.015951319333774056\n",
            "=== epoch:229, train acc:1.0, test acc:0.737 ===\n",
            "train loss:0.01619201359242719\n",
            "train loss:0.01235206202845634\n",
            "train loss:0.012840104149348037\n",
            "=== epoch:230, train acc:1.0, test acc:0.7386 ===\n",
            "train loss:0.013630473076005496\n",
            "train loss:0.012950600146508338\n",
            "train loss:0.01705837825984244\n",
            "=== epoch:231, train acc:1.0, test acc:0.7389 ===\n",
            "train loss:0.01915170875555021\n",
            "train loss:0.016316722515261742\n",
            "train loss:0.013385361133205757\n",
            "=== epoch:232, train acc:1.0, test acc:0.7383 ===\n",
            "train loss:0.018190549633742926\n",
            "train loss:0.01405712986668529\n",
            "train loss:0.015663873562656826\n",
            "=== epoch:233, train acc:1.0, test acc:0.7381 ===\n",
            "train loss:0.01646714063186907\n",
            "train loss:0.010752381616184052\n",
            "train loss:0.011478059551102709\n",
            "=== epoch:234, train acc:1.0, test acc:0.7392 ===\n",
            "train loss:0.013370628427593274\n",
            "train loss:0.014120153803649733\n",
            "train loss:0.016700489085457463\n",
            "=== epoch:235, train acc:1.0, test acc:0.7377 ===\n",
            "train loss:0.015813071850601646\n",
            "train loss:0.012767609964993458\n",
            "train loss:0.010097938221987922\n",
            "=== epoch:236, train acc:1.0, test acc:0.7374 ===\n",
            "train loss:0.012663343432571687\n",
            "train loss:0.02304857225758718\n",
            "train loss:0.01571573025348292\n",
            "=== epoch:237, train acc:1.0, test acc:0.7353 ===\n",
            "train loss:0.017281836870389283\n",
            "train loss:0.013252054393440469\n",
            "train loss:0.018050496462477906\n",
            "=== epoch:238, train acc:1.0, test acc:0.7359 ===\n",
            "train loss:0.014583449703940739\n",
            "train loss:0.013843668203173946\n",
            "train loss:0.01147791822839412\n",
            "=== epoch:239, train acc:1.0, test acc:0.7378 ===\n",
            "train loss:0.01070214079759987\n",
            "train loss:0.014169197920287684\n",
            "train loss:0.019708914084704182\n",
            "=== epoch:240, train acc:1.0, test acc:0.7345 ===\n",
            "train loss:0.01618278199741229\n",
            "train loss:0.011521300667607507\n",
            "train loss:0.014783198219171538\n",
            "=== epoch:241, train acc:1.0, test acc:0.7371 ===\n",
            "train loss:0.009198656619962999\n",
            "train loss:0.009872565889246262\n",
            "train loss:0.010164733735465665\n",
            "=== epoch:242, train acc:1.0, test acc:0.7372 ===\n",
            "train loss:0.014746064402203978\n",
            "train loss:0.019791443498238713\n",
            "train loss:0.011022324399075537\n",
            "=== epoch:243, train acc:1.0, test acc:0.7368 ===\n",
            "train loss:0.011961572116180326\n",
            "train loss:0.015896609361422256\n",
            "train loss:0.012528828252274892\n",
            "=== epoch:244, train acc:1.0, test acc:0.7367 ===\n",
            "train loss:0.013896914024540647\n",
            "train loss:0.01483166282919476\n",
            "train loss:0.012071306006342177\n",
            "=== epoch:245, train acc:1.0, test acc:0.7386 ===\n",
            "train loss:0.011651335733345855\n",
            "train loss:0.011555915373920794\n",
            "train loss:0.01453188951419814\n",
            "=== epoch:246, train acc:1.0, test acc:0.738 ===\n",
            "train loss:0.015126081682774618\n",
            "train loss:0.011499232529883795\n",
            "train loss:0.015805712346986495\n",
            "=== epoch:247, train acc:1.0, test acc:0.7387 ===\n",
            "train loss:0.009163407898237323\n",
            "train loss:0.011943881880838154\n",
            "train loss:0.01768146263625529\n",
            "=== epoch:248, train acc:1.0, test acc:0.7372 ===\n",
            "train loss:0.014872588333475201\n",
            "train loss:0.012468201496913902\n",
            "train loss:0.012281352406274162\n",
            "=== epoch:249, train acc:1.0, test acc:0.7374 ===\n",
            "train loss:0.008702776808837938\n",
            "train loss:0.010209257957287516\n",
            "train loss:0.01119695036485132\n",
            "=== epoch:250, train acc:1.0, test acc:0.7387 ===\n",
            "train loss:0.010642917227701413\n",
            "train loss:0.010672854397648136\n",
            "train loss:0.010415250526850921\n",
            "=== epoch:251, train acc:1.0, test acc:0.7384 ===\n",
            "train loss:0.0127986195682978\n",
            "train loss:0.014009611019034987\n",
            "train loss:0.013607959100544954\n",
            "=== epoch:252, train acc:1.0, test acc:0.7378 ===\n",
            "train loss:0.01058525370538559\n",
            "train loss:0.011238618287064224\n",
            "train loss:0.014345977207472342\n",
            "=== epoch:253, train acc:1.0, test acc:0.7379 ===\n",
            "train loss:0.011444284577641581\n",
            "train loss:0.015105937884402556\n",
            "train loss:0.012986313711523756\n",
            "=== epoch:254, train acc:1.0, test acc:0.736 ===\n",
            "train loss:0.011471928126700347\n",
            "train loss:0.011111971883909819\n",
            "train loss:0.012205063782943552\n",
            "=== epoch:255, train acc:1.0, test acc:0.7376 ===\n",
            "train loss:0.012862414710145084\n",
            "train loss:0.012251608554572233\n",
            "train loss:0.014148551560252607\n",
            "=== epoch:256, train acc:1.0, test acc:0.7379 ===\n",
            "train loss:0.01073472641276314\n",
            "train loss:0.010878385484669866\n",
            "train loss:0.008084075304973267\n",
            "=== epoch:257, train acc:1.0, test acc:0.7382 ===\n",
            "train loss:0.011846988149337687\n",
            "train loss:0.011079366380981621\n",
            "train loss:0.012398067596478655\n",
            "=== epoch:258, train acc:1.0, test acc:0.7382 ===\n",
            "train loss:0.011619325830375848\n",
            "train loss:0.012731975233767972\n",
            "train loss:0.011205295252881366\n",
            "=== epoch:259, train acc:1.0, test acc:0.7391 ===\n",
            "train loss:0.013573910223657042\n",
            "train loss:0.014514107488326483\n",
            "train loss:0.008848391586727414\n",
            "=== epoch:260, train acc:1.0, test acc:0.7388 ===\n",
            "train loss:0.011638079789358127\n",
            "train loss:0.017268745566564197\n",
            "train loss:0.014685336168821768\n",
            "=== epoch:261, train acc:1.0, test acc:0.7357 ===\n",
            "train loss:0.012693816097398797\n",
            "train loss:0.009951941497874529\n",
            "train loss:0.009804175943683097\n",
            "=== epoch:262, train acc:1.0, test acc:0.7369 ===\n",
            "train loss:0.01317743389108543\n",
            "train loss:0.010545860632473784\n",
            "train loss:0.011808171980518523\n",
            "=== epoch:263, train acc:1.0, test acc:0.7382 ===\n",
            "train loss:0.011162787505708845\n",
            "train loss:0.012343616175967577\n",
            "train loss:0.011686818895904234\n",
            "=== epoch:264, train acc:1.0, test acc:0.7368 ===\n",
            "train loss:0.014319440394310348\n",
            "train loss:0.00875400925520653\n",
            "train loss:0.014341979159849336\n",
            "=== epoch:265, train acc:1.0, test acc:0.7379 ===\n",
            "train loss:0.008468411414917752\n",
            "train loss:0.012702304290247763\n",
            "train loss:0.015433072605340792\n",
            "=== epoch:266, train acc:1.0, test acc:0.7372 ===\n",
            "train loss:0.012382787396270863\n",
            "train loss:0.009359067552556437\n",
            "train loss:0.014382814695922197\n",
            "=== epoch:267, train acc:1.0, test acc:0.7384 ===\n",
            "train loss:0.01107134242887423\n",
            "train loss:0.01110277796365303\n",
            "train loss:0.009249468181825872\n",
            "=== epoch:268, train acc:1.0, test acc:0.7375 ===\n",
            "train loss:0.010754564577353307\n",
            "train loss:0.008251027109218233\n",
            "train loss:0.01197309502218863\n",
            "=== epoch:269, train acc:1.0, test acc:0.7384 ===\n",
            "train loss:0.00945799674143568\n",
            "train loss:0.009771306306986546\n",
            "train loss:0.013490182344026398\n",
            "=== epoch:270, train acc:1.0, test acc:0.7387 ===\n",
            "train loss:0.007158342866747802\n",
            "train loss:0.00850586198739336\n",
            "train loss:0.009955107485780578\n",
            "=== epoch:271, train acc:1.0, test acc:0.7395 ===\n",
            "train loss:0.007746664869533734\n",
            "train loss:0.010893214931591818\n",
            "train loss:0.010793374585066231\n",
            "=== epoch:272, train acc:1.0, test acc:0.7386 ===\n",
            "train loss:0.011259964492904086\n",
            "train loss:0.010687713296004405\n",
            "train loss:0.012638312647912608\n",
            "=== epoch:273, train acc:1.0, test acc:0.7395 ===\n",
            "train loss:0.009269403162365996\n",
            "train loss:0.011787807874720653\n",
            "train loss:0.00975411683481383\n",
            "=== epoch:274, train acc:1.0, test acc:0.7399 ===\n",
            "train loss:0.01400482193643992\n",
            "train loss:0.01103173954852031\n",
            "train loss:0.00980229756356806\n",
            "=== epoch:275, train acc:1.0, test acc:0.7394 ===\n",
            "train loss:0.013010922089904493\n",
            "train loss:0.012747835266686382\n",
            "train loss:0.010879235787954746\n",
            "=== epoch:276, train acc:1.0, test acc:0.7389 ===\n",
            "train loss:0.011580451884068612\n",
            "train loss:0.011729641596181702\n",
            "train loss:0.010642316833777532\n",
            "=== epoch:277, train acc:1.0, test acc:0.7394 ===\n",
            "train loss:0.007774042833516287\n",
            "train loss:0.01649286152179895\n",
            "train loss:0.013348704034946953\n",
            "=== epoch:278, train acc:1.0, test acc:0.739 ===\n",
            "train loss:0.012829911634108697\n",
            "train loss:0.008992507641801952\n",
            "train loss:0.012053203520005327\n",
            "=== epoch:279, train acc:1.0, test acc:0.7385 ===\n",
            "train loss:0.010260442923175343\n",
            "train loss:0.009325802983847081\n",
            "train loss:0.010169088431089374\n",
            "=== epoch:280, train acc:1.0, test acc:0.7393 ===\n",
            "train loss:0.00960818730680577\n",
            "train loss:0.010201099917832041\n",
            "train loss:0.011964896345550566\n",
            "=== epoch:281, train acc:1.0, test acc:0.7373 ===\n",
            "train loss:0.010493943125625273\n",
            "train loss:0.00856446058199301\n",
            "train loss:0.01082149458711295\n",
            "=== epoch:282, train acc:1.0, test acc:0.738 ===\n",
            "train loss:0.007921214671336005\n",
            "train loss:0.008714786479366915\n",
            "train loss:0.01219163133965158\n",
            "=== epoch:283, train acc:1.0, test acc:0.7382 ===\n",
            "train loss:0.00998825143460338\n",
            "train loss:0.010103988318935504\n",
            "train loss:0.009151177741108706\n",
            "=== epoch:284, train acc:1.0, test acc:0.7381 ===\n",
            "train loss:0.00934080129104901\n",
            "train loss:0.012307530913883586\n",
            "train loss:0.009026834082419496\n",
            "=== epoch:285, train acc:1.0, test acc:0.7378 ===\n",
            "train loss:0.01024583273791532\n",
            "train loss:0.008566276272576277\n",
            "train loss:0.009795303991242808\n",
            "=== epoch:286, train acc:1.0, test acc:0.7391 ===\n",
            "train loss:0.0074259598115601275\n",
            "train loss:0.007469849020574506\n",
            "train loss:0.011799980162306888\n",
            "=== epoch:287, train acc:1.0, test acc:0.7391 ===\n",
            "train loss:0.009814873281034088\n",
            "train loss:0.009524878832872416\n",
            "train loss:0.009518797064444744\n",
            "=== epoch:288, train acc:1.0, test acc:0.7387 ===\n",
            "train loss:0.010683009455885859\n",
            "train loss:0.010277828402068999\n",
            "train loss:0.008842309762137983\n",
            "=== epoch:289, train acc:1.0, test acc:0.739 ===\n",
            "train loss:0.0063999228422792365\n",
            "train loss:0.010733155079579131\n",
            "train loss:0.00755260360056904\n",
            "=== epoch:290, train acc:1.0, test acc:0.7387 ===\n",
            "train loss:0.009253610494834103\n",
            "train loss:0.011148740948456566\n",
            "train loss:0.009640268742161909\n",
            "=== epoch:291, train acc:1.0, test acc:0.7389 ===\n",
            "train loss:0.00836036507692639\n",
            "train loss:0.008281319326266908\n",
            "train loss:0.012015930068066371\n",
            "=== epoch:292, train acc:1.0, test acc:0.7389 ===\n",
            "train loss:0.010248039128149186\n",
            "train loss:0.008618699117679578\n",
            "train loss:0.010863118760775628\n",
            "=== epoch:293, train acc:1.0, test acc:0.7398 ===\n",
            "train loss:0.010060610681244397\n",
            "train loss:0.00898005780163725\n",
            "train loss:0.008850659050431874\n",
            "=== epoch:294, train acc:1.0, test acc:0.7395 ===\n",
            "train loss:0.008304446917296798\n",
            "train loss:0.009944673915851111\n",
            "train loss:0.014621554399816898\n",
            "=== epoch:295, train acc:1.0, test acc:0.7381 ===\n",
            "train loss:0.00772160519935481\n",
            "train loss:0.011236406605082667\n",
            "train loss:0.007734568507636292\n",
            "=== epoch:296, train acc:1.0, test acc:0.7386 ===\n",
            "train loss:0.008974950914224048\n",
            "train loss:0.008110440727510574\n",
            "train loss:0.0073015685212598205\n",
            "=== epoch:297, train acc:1.0, test acc:0.739 ===\n",
            "train loss:0.009756818833995215\n",
            "train loss:0.008335600480489593\n",
            "train loss:0.006835581268325944\n",
            "=== epoch:298, train acc:1.0, test acc:0.7386 ===\n",
            "train loss:0.008545395059945393\n",
            "train loss:0.010490233332025676\n",
            "train loss:0.009230306153389494\n",
            "=== epoch:299, train acc:1.0, test acc:0.7395 ===\n",
            "train loss:0.008445675671493942\n",
            "train loss:0.0076917367232142275\n",
            "train loss:0.011444444604721707\n",
            "=== epoch:300, train acc:1.0, test acc:0.7383 ===\n",
            "train loss:0.00720623678439679\n",
            "train loss:0.008297537474397822\n",
            "train loss:0.007651245943704227\n",
            "=== epoch:301, train acc:1.0, test acc:0.7392 ===\n",
            "train loss:0.00694004027485998\n",
            "train loss:0.010228196943749282\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.7397\n"
          ]
        }
      ],
      "source": [
        "# 6.4.3.2 Dropout 사용하지 않음 (사용본과 대조용)\n",
        "\n",
        "# 은닉층 6개층 100개 노드\n",
        "# 은닉층 활성화 함수 : ReLU\n",
        "# 가중치 초기설정 : He\n",
        "\n",
        "# 배치 사이즈 : 100\n",
        "# 최대 반복 회수 : 301 epoch\n",
        "# 훈련방식 : SGD   학습률 0.01\n",
        "\n",
        "\n",
        "# 훈련데이터 크기 : 300\n",
        "\n",
        "# 실험 드롭아웃 사용여부에 따른 비교 (dropout_ratio = 0.2)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해서 학습 데이터를 제거\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# Dropuout의 유무, 비율의 설정  ========================\n",
        "use_dropout = False  # Dropout을 사용하지 않을 때 False\n",
        "dropout_ratio = 0.2\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(\n",
        "            input_size=784,\n",
        "            hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "            output_size=10,\n",
        "            use_dropout=use_dropout,\n",
        "            dropout_ration=dropout_ratio)\n",
        "\n",
        "\n",
        "# Trainer 클래스는 간소화 된 훈련 로직 (훈련 로직을 담고 있는 클래스)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01},\n",
        "                  verbose=True)\n",
        "# 훈련 수행\n",
        "trainer.train()\n",
        "\n",
        "# 훈련 중 정확도 측정 정보\n",
        "train_acc_list, test_acc_list =\\\n",
        "   trainer.train_acc_list, trainer.test_acc_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.4.3.3 Dropout 사용하지 않은 경우\n",
        "# 그래프 생성=========\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.title ('Train vs.Test OverFit w dropout = False')\n",
        "plt.xlim(0, 300)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YkMoec76KXs2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "3d0071b1-2984-4907-a85b-abb58e4075ea"
      },
      "id": "YkMoec76KXs2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### use_dropout = True"
      ],
      "metadata": {
        "id": "1v6RAUTwLtWI"
      },
      "id": "1v6RAUTwLtWI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "turkish-interference",
      "metadata": {
        "scrolled": true,
        "id": "turkish-interference",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f5c079-bb69-47e7-f292-71fe86487244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.301224856414432\n",
            "=== epoch:1, train acc:0.12333333333333334, test acc:0.1062 ===\n",
            "train loss:2.307357100415924\n",
            "train loss:2.303165261612259\n",
            "train loss:2.29458053042421\n",
            "=== epoch:2, train acc:0.12333333333333334, test acc:0.1083 ===\n",
            "train loss:2.2955899230268937\n",
            "train loss:2.3102002774719326\n",
            "train loss:2.2962144940366516\n",
            "=== epoch:3, train acc:0.12333333333333334, test acc:0.1105 ===\n",
            "train loss:2.2991050864795866\n",
            "train loss:2.3042817517685363\n",
            "train loss:2.2958615863472094\n",
            "=== epoch:4, train acc:0.12666666666666668, test acc:0.1137 ===\n",
            "train loss:2.2998699166952576\n",
            "train loss:2.299047249147036\n",
            "train loss:2.3016578151765317\n",
            "=== epoch:5, train acc:0.12333333333333334, test acc:0.1142 ===\n",
            "train loss:2.30338623286743\n",
            "train loss:2.2939065436367136\n",
            "train loss:2.2975598512591136\n",
            "=== epoch:6, train acc:0.13, test acc:0.1146 ===\n",
            "train loss:2.2938115617406294\n",
            "train loss:2.3010843822838662\n",
            "train loss:2.2925592537529487\n",
            "=== epoch:7, train acc:0.12666666666666668, test acc:0.1167 ===\n",
            "train loss:2.2961230438924085\n",
            "train loss:2.294292415140689\n",
            "train loss:2.294537087771063\n",
            "=== epoch:8, train acc:0.13, test acc:0.1195 ===\n",
            "train loss:2.2965511633879303\n",
            "train loss:2.295471457503077\n",
            "train loss:2.29475356906858\n",
            "=== epoch:9, train acc:0.13666666666666666, test acc:0.1205 ===\n",
            "train loss:2.2941158289938715\n",
            "train loss:2.2951916832392354\n",
            "train loss:2.3077530966289195\n",
            "=== epoch:10, train acc:0.14, test acc:0.1182 ===\n",
            "train loss:2.294710990541776\n",
            "train loss:2.293987219931675\n",
            "train loss:2.2968346860460804\n",
            "=== epoch:11, train acc:0.14333333333333334, test acc:0.1215 ===\n",
            "train loss:2.2936692101690426\n",
            "train loss:2.2989684465814846\n",
            "train loss:2.3006196593018196\n",
            "=== epoch:12, train acc:0.14333333333333334, test acc:0.1236 ===\n",
            "train loss:2.2859147199586545\n",
            "train loss:2.2969921844868177\n",
            "train loss:2.291281781646137\n",
            "=== epoch:13, train acc:0.14666666666666667, test acc:0.1264 ===\n",
            "train loss:2.2853306595525353\n",
            "train loss:2.2908112671621548\n",
            "train loss:2.2940428471168004\n",
            "=== epoch:14, train acc:0.16, test acc:0.1312 ===\n",
            "train loss:2.288430591891194\n",
            "train loss:2.2904336728019046\n",
            "train loss:2.2915797636307627\n",
            "=== epoch:15, train acc:0.15333333333333332, test acc:0.1348 ===\n",
            "train loss:2.296967991455684\n",
            "train loss:2.2881062497659177\n",
            "train loss:2.2933270501961918\n",
            "=== epoch:16, train acc:0.17, test acc:0.1371 ===\n",
            "train loss:2.2929754946823797\n",
            "train loss:2.2862762876786293\n",
            "train loss:2.29311809117543\n",
            "=== epoch:17, train acc:0.15666666666666668, test acc:0.1402 ===\n",
            "train loss:2.289988041785953\n",
            "train loss:2.295831008008237\n",
            "train loss:2.2918533671520187\n",
            "=== epoch:18, train acc:0.15333333333333332, test acc:0.1438 ===\n",
            "train loss:2.2909406760008486\n",
            "train loss:2.2936690180780266\n",
            "train loss:2.2948428293111562\n",
            "=== epoch:19, train acc:0.16333333333333333, test acc:0.1438 ===\n",
            "train loss:2.286612961831325\n",
            "train loss:2.2871723709164304\n",
            "train loss:2.289073034868905\n",
            "=== epoch:20, train acc:0.17, test acc:0.1456 ===\n",
            "train loss:2.29487090265018\n",
            "train loss:2.283960097683098\n",
            "train loss:2.2865423866534482\n",
            "=== epoch:21, train acc:0.17666666666666667, test acc:0.1491 ===\n",
            "train loss:2.292443165198387\n",
            "train loss:2.2879944536106724\n",
            "train loss:2.2875735456385597\n",
            "=== epoch:22, train acc:0.17666666666666667, test acc:0.1549 ===\n",
            "train loss:2.2850227389638063\n",
            "train loss:2.2902104737507667\n",
            "train loss:2.287696411116525\n",
            "=== epoch:23, train acc:0.16666666666666666, test acc:0.1573 ===\n",
            "train loss:2.2818913662584692\n",
            "train loss:2.2926995326917865\n",
            "train loss:2.2841198071284183\n",
            "=== epoch:24, train acc:0.18, test acc:0.162 ===\n",
            "train loss:2.28414367724547\n",
            "train loss:2.2870445247222926\n",
            "train loss:2.278621572029346\n",
            "=== epoch:25, train acc:0.18333333333333332, test acc:0.1638 ===\n",
            "train loss:2.286046916297221\n",
            "train loss:2.2861951715847217\n",
            "train loss:2.2889251529840084\n",
            "=== epoch:26, train acc:0.18333333333333332, test acc:0.1659 ===\n",
            "train loss:2.2774569248385923\n",
            "train loss:2.2907834644107274\n",
            "train loss:2.286447006517181\n",
            "=== epoch:27, train acc:0.18666666666666668, test acc:0.1694 ===\n",
            "train loss:2.2895124039523442\n",
            "train loss:2.2912670924289955\n",
            "train loss:2.2822314749165\n",
            "=== epoch:28, train acc:0.19333333333333333, test acc:0.1713 ===\n",
            "train loss:2.287295220796423\n",
            "train loss:2.2857622270367037\n",
            "train loss:2.2879754370783987\n",
            "=== epoch:29, train acc:0.19666666666666666, test acc:0.1741 ===\n",
            "train loss:2.2912223022587868\n",
            "train loss:2.292146477679041\n",
            "train loss:2.282025211842565\n",
            "=== epoch:30, train acc:0.19666666666666666, test acc:0.1757 ===\n",
            "train loss:2.286290745322407\n",
            "train loss:2.2818556229723193\n",
            "train loss:2.2774895707487444\n",
            "=== epoch:31, train acc:0.20666666666666667, test acc:0.1772 ===\n",
            "train loss:2.2815085156246493\n",
            "train loss:2.2806721728470736\n",
            "train loss:2.2857634784187595\n",
            "=== epoch:32, train acc:0.21333333333333335, test acc:0.1796 ===\n",
            "train loss:2.2785878791897733\n",
            "train loss:2.2848810433821876\n",
            "train loss:2.282489234014573\n",
            "=== epoch:33, train acc:0.21333333333333335, test acc:0.1812 ===\n",
            "train loss:2.2790477059582206\n",
            "train loss:2.293941583938462\n",
            "train loss:2.280339855224992\n",
            "=== epoch:34, train acc:0.22333333333333333, test acc:0.1835 ===\n",
            "train loss:2.284237283699629\n",
            "train loss:2.280698658121921\n",
            "train loss:2.284148644268852\n",
            "=== epoch:35, train acc:0.22333333333333333, test acc:0.1843 ===\n",
            "train loss:2.2823861165150316\n",
            "train loss:2.2857212156862423\n",
            "train loss:2.2803699495042626\n",
            "=== epoch:36, train acc:0.22333333333333333, test acc:0.1873 ===\n",
            "train loss:2.2788714136235515\n",
            "train loss:2.2732887198794276\n",
            "train loss:2.28115957505418\n",
            "=== epoch:37, train acc:0.23666666666666666, test acc:0.1902 ===\n",
            "train loss:2.277570354581707\n",
            "train loss:2.28038584046725\n",
            "train loss:2.272602450905579\n",
            "=== epoch:38, train acc:0.23666666666666666, test acc:0.1931 ===\n",
            "train loss:2.289163732159293\n",
            "train loss:2.2774539257615665\n",
            "train loss:2.280417974486219\n",
            "=== epoch:39, train acc:0.23, test acc:0.1927 ===\n",
            "train loss:2.28567323844782\n",
            "train loss:2.2884458017346008\n",
            "train loss:2.271029001194802\n",
            "=== epoch:40, train acc:0.23333333333333334, test acc:0.1963 ===\n",
            "train loss:2.2770826533821147\n",
            "train loss:2.273762278820411\n",
            "train loss:2.2808276140224546\n",
            "=== epoch:41, train acc:0.23666666666666666, test acc:0.1956 ===\n",
            "train loss:2.278971224833992\n",
            "train loss:2.265730643811309\n",
            "train loss:2.2849712655498142\n",
            "=== epoch:42, train acc:0.24, test acc:0.1978 ===\n",
            "train loss:2.282626986325755\n",
            "train loss:2.280615102187613\n",
            "train loss:2.2732819966335476\n",
            "=== epoch:43, train acc:0.24333333333333335, test acc:0.1997 ===\n",
            "train loss:2.281142284122735\n",
            "train loss:2.273955203728587\n",
            "train loss:2.28009306696705\n",
            "=== epoch:44, train acc:0.24666666666666667, test acc:0.2027 ===\n",
            "train loss:2.267257439508547\n",
            "train loss:2.2764218860422027\n",
            "train loss:2.263323852044662\n",
            "=== epoch:45, train acc:0.24, test acc:0.2044 ===\n",
            "train loss:2.286379775494783\n",
            "train loss:2.2697706607447588\n",
            "train loss:2.280769559819369\n",
            "=== epoch:46, train acc:0.24333333333333335, test acc:0.2069 ===\n",
            "train loss:2.277217200736433\n",
            "train loss:2.2731745364846128\n",
            "train loss:2.2736945776171855\n",
            "=== epoch:47, train acc:0.24333333333333335, test acc:0.2077 ===\n",
            "train loss:2.2682256913618426\n",
            "train loss:2.2676082193283484\n",
            "train loss:2.272119393896558\n",
            "=== epoch:48, train acc:0.24666666666666667, test acc:0.2081 ===\n",
            "train loss:2.274480546936202\n",
            "train loss:2.275057733880368\n",
            "train loss:2.2868957643110157\n",
            "=== epoch:49, train acc:0.24666666666666667, test acc:0.2079 ===\n",
            "train loss:2.26945149386255\n",
            "train loss:2.2773329988057376\n",
            "train loss:2.2624913163638993\n",
            "=== epoch:50, train acc:0.24666666666666667, test acc:0.2107 ===\n",
            "train loss:2.268595225116754\n",
            "train loss:2.270370931915747\n",
            "train loss:2.2837480988319667\n",
            "=== epoch:51, train acc:0.25, test acc:0.2098 ===\n",
            "train loss:2.2742548575766937\n",
            "train loss:2.2822219469319416\n",
            "train loss:2.268257451342291\n",
            "=== epoch:52, train acc:0.25, test acc:0.2113 ===\n",
            "train loss:2.2723659078621403\n",
            "train loss:2.2627590144552276\n",
            "train loss:2.2649292568883905\n",
            "=== epoch:53, train acc:0.25333333333333335, test acc:0.2126 ===\n",
            "train loss:2.2667715828716926\n",
            "train loss:2.2672415274714495\n",
            "train loss:2.2695232453448786\n",
            "=== epoch:54, train acc:0.25333333333333335, test acc:0.2152 ===\n",
            "train loss:2.278587514662631\n",
            "train loss:2.2781427620526493\n",
            "train loss:2.2686307855562258\n",
            "=== epoch:55, train acc:0.25, test acc:0.2155 ===\n",
            "train loss:2.27361447888796\n",
            "train loss:2.2709508571256145\n",
            "train loss:2.275128951757485\n",
            "=== epoch:56, train acc:0.25333333333333335, test acc:0.217 ===\n",
            "train loss:2.2666610210597966\n",
            "train loss:2.2758100544771684\n",
            "train loss:2.271326207743983\n",
            "=== epoch:57, train acc:0.25, test acc:0.2174 ===\n",
            "train loss:2.278073623119733\n",
            "train loss:2.261932323503864\n",
            "train loss:2.274317230484758\n",
            "=== epoch:58, train acc:0.25333333333333335, test acc:0.2181 ===\n",
            "train loss:2.272081942185656\n",
            "train loss:2.2604508778503147\n",
            "train loss:2.2682815211072964\n",
            "=== epoch:59, train acc:0.25333333333333335, test acc:0.2191 ===\n",
            "train loss:2.270561861899409\n",
            "train loss:2.2760577257100043\n",
            "train loss:2.268604079449441\n",
            "=== epoch:60, train acc:0.25666666666666665, test acc:0.2211 ===\n",
            "train loss:2.2648752722302254\n",
            "train loss:2.2655188118717273\n",
            "train loss:2.2666777149724253\n",
            "=== epoch:61, train acc:0.25333333333333335, test acc:0.22 ===\n",
            "train loss:2.264000119785299\n",
            "train loss:2.25942430869564\n",
            "train loss:2.2644484980648882\n",
            "=== epoch:62, train acc:0.25333333333333335, test acc:0.2228 ===\n",
            "train loss:2.2619865375937436\n",
            "train loss:2.255934764711871\n",
            "train loss:2.2666200733488866\n",
            "=== epoch:63, train acc:0.25, test acc:0.221 ===\n",
            "train loss:2.2669755718166713\n",
            "train loss:2.2799349192874057\n",
            "train loss:2.252240491089402\n",
            "=== epoch:64, train acc:0.25333333333333335, test acc:0.2226 ===\n",
            "train loss:2.274696012896704\n",
            "train loss:2.2694655220368802\n",
            "train loss:2.2803844718375283\n",
            "=== epoch:65, train acc:0.25666666666666665, test acc:0.2239 ===\n",
            "train loss:2.2622614834858736\n",
            "train loss:2.2642847161460655\n",
            "train loss:2.2700844940136746\n",
            "=== epoch:66, train acc:0.2733333333333333, test acc:0.2261 ===\n",
            "train loss:2.2611765036507596\n",
            "train loss:2.262759649817038\n",
            "train loss:2.272171121535256\n",
            "=== epoch:67, train acc:0.2733333333333333, test acc:0.229 ===\n",
            "train loss:2.2651332102183184\n",
            "train loss:2.2498283642971417\n",
            "train loss:2.2527844878070864\n",
            "=== epoch:68, train acc:0.26666666666666666, test acc:0.2285 ===\n",
            "train loss:2.270053018839961\n",
            "train loss:2.2623600227967176\n",
            "train loss:2.2615697924866645\n",
            "=== epoch:69, train acc:0.2733333333333333, test acc:0.2302 ===\n",
            "train loss:2.2628481773228177\n",
            "train loss:2.2557307651853606\n",
            "train loss:2.251116792330466\n",
            "=== epoch:70, train acc:0.2866666666666667, test acc:0.2323 ===\n",
            "train loss:2.2591420950523577\n",
            "train loss:2.2626457443485206\n",
            "train loss:2.260208796461734\n",
            "=== epoch:71, train acc:0.2833333333333333, test acc:0.2328 ===\n",
            "train loss:2.2611139206146578\n",
            "train loss:2.2486959583006607\n",
            "train loss:2.254529424615529\n",
            "=== epoch:72, train acc:0.2866666666666667, test acc:0.2326 ===\n",
            "train loss:2.244696955331821\n",
            "train loss:2.2475051036226246\n",
            "train loss:2.256980318038794\n",
            "=== epoch:73, train acc:0.2866666666666667, test acc:0.2339 ===\n",
            "train loss:2.2483794751030475\n",
            "train loss:2.245649144335414\n",
            "train loss:2.243268688738276\n",
            "=== epoch:74, train acc:0.29, test acc:0.234 ===\n",
            "train loss:2.270851119973854\n",
            "train loss:2.2564929871303168\n",
            "train loss:2.2467175470764396\n",
            "=== epoch:75, train acc:0.29333333333333333, test acc:0.2344 ===\n",
            "train loss:2.252237004751002\n",
            "train loss:2.268203785811736\n",
            "train loss:2.239136968638457\n",
            "=== epoch:76, train acc:0.2866666666666667, test acc:0.2337 ===\n",
            "train loss:2.258664231437831\n",
            "train loss:2.2556952201913294\n",
            "train loss:2.243018353312024\n",
            "=== epoch:77, train acc:0.2866666666666667, test acc:0.2323 ===\n",
            "train loss:2.2446943655497127\n",
            "train loss:2.2359272791396894\n",
            "train loss:2.2435017377238338\n",
            "=== epoch:78, train acc:0.29333333333333333, test acc:0.2315 ===\n",
            "train loss:2.247394781789568\n",
            "train loss:2.255272352200432\n",
            "train loss:2.2526540391470746\n",
            "=== epoch:79, train acc:0.29333333333333333, test acc:0.2325 ===\n",
            "train loss:2.2443026361870504\n",
            "train loss:2.259455073438491\n",
            "train loss:2.2487055163427843\n",
            "=== epoch:80, train acc:0.29333333333333333, test acc:0.2348 ===\n",
            "train loss:2.254007556264988\n",
            "train loss:2.247224030048165\n",
            "train loss:2.2548003422590472\n",
            "=== epoch:81, train acc:0.3, test acc:0.2351 ===\n",
            "train loss:2.260082975429194\n",
            "train loss:2.2436642400888234\n",
            "train loss:2.250861539538624\n",
            "=== epoch:82, train acc:0.3, test acc:0.236 ===\n",
            "train loss:2.244777762087329\n",
            "train loss:2.24464415172249\n",
            "train loss:2.228912476474474\n",
            "=== epoch:83, train acc:0.2966666666666667, test acc:0.2378 ===\n",
            "train loss:2.2423560178585245\n",
            "train loss:2.2434379902970463\n",
            "train loss:2.2386239778573107\n",
            "=== epoch:84, train acc:0.2966666666666667, test acc:0.2372 ===\n",
            "train loss:2.2532860606320098\n",
            "train loss:2.239533873943433\n",
            "train loss:2.23728141284664\n",
            "=== epoch:85, train acc:0.29333333333333333, test acc:0.2389 ===\n",
            "train loss:2.25620271101506\n",
            "train loss:2.250917262107643\n",
            "train loss:2.2344387862020825\n",
            "=== epoch:86, train acc:0.29333333333333333, test acc:0.2388 ===\n",
            "train loss:2.2453188301840443\n",
            "train loss:2.248862947210684\n",
            "train loss:2.2525135122565803\n",
            "=== epoch:87, train acc:0.2966666666666667, test acc:0.2422 ===\n",
            "train loss:2.2474335554870657\n",
            "train loss:2.252119380123865\n",
            "train loss:2.2333239654575503\n",
            "=== epoch:88, train acc:0.2966666666666667, test acc:0.2401 ===\n",
            "train loss:2.2376248146890574\n",
            "train loss:2.245366881404499\n",
            "train loss:2.2345810254938727\n",
            "=== epoch:89, train acc:0.2966666666666667, test acc:0.2389 ===\n",
            "train loss:2.2430280426649376\n",
            "train loss:2.228895185335506\n",
            "train loss:2.240515130072684\n",
            "=== epoch:90, train acc:0.2966666666666667, test acc:0.2429 ===\n",
            "train loss:2.2320820944322644\n",
            "train loss:2.2203593612120343\n",
            "train loss:2.2346578225209366\n",
            "=== epoch:91, train acc:0.3, test acc:0.244 ===\n",
            "train loss:2.220798943972612\n",
            "train loss:2.24243848422026\n",
            "train loss:2.221749165458958\n",
            "=== epoch:92, train acc:0.30333333333333334, test acc:0.246 ===\n",
            "train loss:2.2263432637316303\n",
            "train loss:2.227757014484502\n",
            "train loss:2.224914442298349\n",
            "=== epoch:93, train acc:0.30333333333333334, test acc:0.2469 ===\n",
            "train loss:2.2209957400202978\n",
            "train loss:2.2350957300330516\n",
            "train loss:2.2461963843384734\n",
            "=== epoch:94, train acc:0.30333333333333334, test acc:0.2466 ===\n",
            "train loss:2.2318941698699577\n",
            "train loss:2.235916019777431\n",
            "train loss:2.229912786531007\n",
            "=== epoch:95, train acc:0.30333333333333334, test acc:0.2463 ===\n",
            "train loss:2.2272628576050026\n",
            "train loss:2.220012704920972\n",
            "train loss:2.243433111242554\n",
            "=== epoch:96, train acc:0.30333333333333334, test acc:0.2486 ===\n",
            "train loss:2.2284824981231908\n",
            "train loss:2.231569242573943\n",
            "train loss:2.2243720111174468\n",
            "=== epoch:97, train acc:0.30333333333333334, test acc:0.2476 ===\n",
            "train loss:2.2268470594835805\n",
            "train loss:2.2309446021257404\n",
            "train loss:2.2302676651887254\n",
            "=== epoch:98, train acc:0.30333333333333334, test acc:0.2465 ===\n",
            "train loss:2.23057554909123\n",
            "train loss:2.223611247213673\n",
            "train loss:2.2172675805492905\n",
            "=== epoch:99, train acc:0.30333333333333334, test acc:0.2463 ===\n",
            "train loss:2.240166583603373\n",
            "train loss:2.2093938364977426\n",
            "train loss:2.2264662410747276\n",
            "=== epoch:100, train acc:0.30333333333333334, test acc:0.2457 ===\n",
            "train loss:2.1952200937592106\n",
            "train loss:2.2272943527279185\n",
            "train loss:2.206954321853649\n",
            "=== epoch:101, train acc:0.30333333333333334, test acc:0.244 ===\n",
            "train loss:2.2213600481390494\n",
            "train loss:2.2228137927645797\n",
            "train loss:2.2260661273874773\n",
            "=== epoch:102, train acc:0.30333333333333334, test acc:0.2453 ===\n",
            "train loss:2.229976694910883\n",
            "train loss:2.22110881278559\n",
            "train loss:2.2451349457348777\n",
            "=== epoch:103, train acc:0.3, test acc:0.2487 ===\n",
            "train loss:2.2268256835160747\n",
            "train loss:2.194403317744048\n",
            "train loss:2.225871702913922\n",
            "=== epoch:104, train acc:0.30666666666666664, test acc:0.2513 ===\n",
            "train loss:2.2005653597996617\n",
            "train loss:2.217394856568546\n",
            "train loss:2.206995656884583\n",
            "=== epoch:105, train acc:0.3, test acc:0.2471 ===\n",
            "train loss:2.246990536630187\n",
            "train loss:2.2188150048157227\n",
            "train loss:2.215367859134176\n",
            "=== epoch:106, train acc:0.30666666666666664, test acc:0.2502 ===\n",
            "train loss:2.236629647642641\n",
            "train loss:2.184491474961784\n",
            "train loss:2.2055792481576164\n",
            "=== epoch:107, train acc:0.30666666666666664, test acc:0.25 ===\n",
            "train loss:2.203849454108722\n",
            "train loss:2.195286529550172\n",
            "train loss:2.2287183540140294\n",
            "=== epoch:108, train acc:0.30666666666666664, test acc:0.2509 ===\n",
            "train loss:2.2020998301312047\n",
            "train loss:2.2158750534103238\n",
            "train loss:2.189383537170165\n",
            "=== epoch:109, train acc:0.30333333333333334, test acc:0.2526 ===\n",
            "train loss:2.2260737254740675\n",
            "train loss:2.2185870311082496\n",
            "train loss:2.198261554995094\n",
            "=== epoch:110, train acc:0.30666666666666664, test acc:0.2518 ===\n",
            "train loss:2.1786992905729745\n",
            "train loss:2.222973170476391\n",
            "train loss:2.2411438743313283\n",
            "=== epoch:111, train acc:0.30666666666666664, test acc:0.2572 ===\n",
            "train loss:2.1986287591018754\n",
            "train loss:2.1863645366903266\n",
            "train loss:2.21557694075954\n",
            "=== epoch:112, train acc:0.30666666666666664, test acc:0.2569 ===\n",
            "train loss:2.2030112418256373\n",
            "train loss:2.2110688028065373\n",
            "train loss:2.204691641991864\n",
            "=== epoch:113, train acc:0.31, test acc:0.2557 ===\n",
            "train loss:2.197137948769333\n",
            "train loss:2.1640111691994077\n",
            "train loss:2.2149328823734247\n",
            "=== epoch:114, train acc:0.30666666666666664, test acc:0.2553 ===\n",
            "train loss:2.2061687234979606\n",
            "train loss:2.1973073925803406\n",
            "train loss:2.1869540058144783\n",
            "=== epoch:115, train acc:0.30666666666666664, test acc:0.2555 ===\n",
            "train loss:2.1747600137312078\n",
            "train loss:2.197592814092492\n",
            "train loss:2.2166910844017362\n",
            "=== epoch:116, train acc:0.31, test acc:0.2572 ===\n",
            "train loss:2.207840138972708\n",
            "train loss:2.2174048002230826\n",
            "train loss:2.1758220393942116\n",
            "=== epoch:117, train acc:0.32, test acc:0.2588 ===\n",
            "train loss:2.2272283709778744\n",
            "train loss:2.2051995076955246\n",
            "train loss:2.196307317439251\n",
            "=== epoch:118, train acc:0.32, test acc:0.2603 ===\n",
            "train loss:2.195724293461768\n",
            "train loss:2.158129151060003\n",
            "train loss:2.196670192971939\n",
            "=== epoch:119, train acc:0.32, test acc:0.2596 ===\n",
            "train loss:2.186836295793573\n",
            "train loss:2.21530968027299\n",
            "train loss:2.198057321516337\n",
            "=== epoch:120, train acc:0.31333333333333335, test acc:0.261 ===\n",
            "train loss:2.197470547111203\n",
            "train loss:2.2146064210673813\n",
            "train loss:2.1425530059872075\n",
            "=== epoch:121, train acc:0.31333333333333335, test acc:0.2622 ===\n",
            "train loss:2.1650205548471093\n",
            "train loss:2.1895254305076093\n",
            "train loss:2.197980190969176\n",
            "=== epoch:122, train acc:0.3233333333333333, test acc:0.2623 ===\n",
            "train loss:2.191546677765868\n",
            "train loss:2.1763665595191424\n",
            "train loss:2.182032701594298\n",
            "=== epoch:123, train acc:0.31666666666666665, test acc:0.2659 ===\n",
            "train loss:2.1799171583240975\n",
            "train loss:2.21446593846557\n",
            "train loss:2.1329320923543467\n",
            "=== epoch:124, train acc:0.32, test acc:0.2658 ===\n",
            "train loss:2.179191349383974\n",
            "train loss:2.1377831645586456\n",
            "train loss:2.218668222775986\n",
            "=== epoch:125, train acc:0.32, test acc:0.2659 ===\n",
            "train loss:2.191394146770412\n",
            "train loss:2.1702435599541388\n",
            "train loss:2.1757006749083687\n",
            "=== epoch:126, train acc:0.32666666666666666, test acc:0.2678 ===\n",
            "train loss:2.182047794644871\n",
            "train loss:2.122611059060921\n",
            "train loss:2.1685261222562393\n",
            "=== epoch:127, train acc:0.32666666666666666, test acc:0.2679 ===\n",
            "train loss:2.1368398071727195\n",
            "train loss:2.1749277134284317\n",
            "train loss:2.125539428457748\n",
            "=== epoch:128, train acc:0.3233333333333333, test acc:0.2675 ===\n",
            "train loss:2.1719453196680614\n",
            "train loss:2.1318834340686377\n",
            "train loss:2.1714186506794384\n",
            "=== epoch:129, train acc:0.3233333333333333, test acc:0.268 ===\n",
            "train loss:2.1578909383184564\n",
            "train loss:2.1954965122049996\n",
            "train loss:2.1893529050313547\n",
            "=== epoch:130, train acc:0.32666666666666666, test acc:0.2685 ===\n",
            "train loss:2.147357744883376\n",
            "train loss:2.1615551527838166\n",
            "train loss:2.154760535978217\n",
            "=== epoch:131, train acc:0.3233333333333333, test acc:0.2666 ===\n",
            "train loss:2.1251453819610533\n",
            "train loss:2.16736386511886\n",
            "train loss:2.1613115402229113\n",
            "=== epoch:132, train acc:0.32666666666666666, test acc:0.2672 ===\n",
            "train loss:2.1782331237578103\n",
            "train loss:2.1364892781665485\n",
            "train loss:2.1366593209613254\n",
            "=== epoch:133, train acc:0.33, test acc:0.2674 ===\n",
            "train loss:2.108771922928484\n",
            "train loss:2.1492357393050936\n",
            "train loss:2.1392910301237746\n",
            "=== epoch:134, train acc:0.32666666666666666, test acc:0.2686 ===\n",
            "train loss:2.1194786833997825\n",
            "train loss:2.155509993121582\n",
            "train loss:2.2087050388746388\n",
            "=== epoch:135, train acc:0.33, test acc:0.2698 ===\n",
            "train loss:2.1724677317994256\n",
            "train loss:2.1107983906687777\n",
            "train loss:2.173840341319101\n",
            "=== epoch:136, train acc:0.32666666666666666, test acc:0.2688 ===\n",
            "train loss:2.154522625291278\n",
            "train loss:2.166471869406296\n",
            "train loss:2.164785859335565\n",
            "=== epoch:137, train acc:0.33, test acc:0.2718 ===\n",
            "train loss:2.1954008778751244\n",
            "train loss:2.144759460461264\n",
            "train loss:2.144534246342076\n",
            "=== epoch:138, train acc:0.3333333333333333, test acc:0.2758 ===\n",
            "train loss:2.142220504159789\n",
            "train loss:2.1300489480201246\n",
            "train loss:2.149323281702117\n",
            "=== epoch:139, train acc:0.3333333333333333, test acc:0.2766 ===\n",
            "train loss:2.120996721035328\n",
            "train loss:2.090817463144782\n",
            "train loss:2.1684813457635075\n",
            "=== epoch:140, train acc:0.33666666666666667, test acc:0.2781 ===\n",
            "train loss:2.1587435705168083\n",
            "train loss:2.1329581175148915\n",
            "train loss:2.1486270840761827\n",
            "=== epoch:141, train acc:0.33666666666666667, test acc:0.2797 ===\n",
            "train loss:2.113567678553833\n",
            "train loss:2.1068205625486884\n",
            "train loss:2.0865371729494457\n",
            "=== epoch:142, train acc:0.3333333333333333, test acc:0.2797 ===\n",
            "train loss:2.0557382107342836\n",
            "train loss:2.1372497049960195\n",
            "train loss:2.0693791194072757\n",
            "=== epoch:143, train acc:0.3333333333333333, test acc:0.2786 ===\n",
            "train loss:2.1455077905012847\n",
            "train loss:2.1280772124629923\n",
            "train loss:2.1018434009148153\n",
            "=== epoch:144, train acc:0.3333333333333333, test acc:0.2789 ===\n",
            "train loss:2.0783444897802683\n",
            "train loss:2.1328449942696928\n",
            "train loss:2.178130177942823\n",
            "=== epoch:145, train acc:0.3333333333333333, test acc:0.2795 ===\n",
            "train loss:2.1132266038301815\n",
            "train loss:2.1341808071823207\n",
            "train loss:2.186939178225436\n",
            "=== epoch:146, train acc:0.3333333333333333, test acc:0.2809 ===\n",
            "train loss:2.090925658947201\n",
            "train loss:2.108161436008109\n",
            "train loss:2.125226205751969\n",
            "=== epoch:147, train acc:0.3333333333333333, test acc:0.2812 ===\n",
            "train loss:2.135989238332617\n",
            "train loss:2.090020477040937\n",
            "train loss:2.1041352777619853\n",
            "=== epoch:148, train acc:0.33, test acc:0.282 ===\n",
            "train loss:2.12998961377313\n",
            "train loss:2.1002591578471024\n",
            "train loss:2.0972721597494335\n",
            "=== epoch:149, train acc:0.3333333333333333, test acc:0.282 ===\n",
            "train loss:2.1542456176355826\n",
            "train loss:2.0474818059216178\n",
            "train loss:2.1249688745498996\n",
            "=== epoch:150, train acc:0.33666666666666667, test acc:0.2831 ===\n",
            "train loss:2.0149291273779615\n",
            "train loss:2.0411547930457075\n",
            "train loss:2.057972920502764\n",
            "=== epoch:151, train acc:0.32666666666666666, test acc:0.2826 ===\n",
            "train loss:2.1052747883237943\n",
            "train loss:2.05055140738571\n",
            "train loss:2.1090785299162222\n",
            "=== epoch:152, train acc:0.33, test acc:0.2829 ===\n",
            "train loss:2.1134954452686374\n",
            "train loss:2.0940030558641505\n",
            "train loss:2.0799436635182884\n",
            "=== epoch:153, train acc:0.33, test acc:0.2828 ===\n",
            "train loss:2.127009120162083\n",
            "train loss:2.021376181149244\n",
            "train loss:2.05448899397035\n",
            "=== epoch:154, train acc:0.33, test acc:0.2837 ===\n",
            "train loss:2.076176377455204\n",
            "train loss:2.096842586824761\n",
            "train loss:2.09153002618382\n",
            "=== epoch:155, train acc:0.33, test acc:0.2843 ===\n",
            "train loss:2.029724058729135\n",
            "train loss:1.9843346677924882\n",
            "train loss:2.125367133119242\n",
            "=== epoch:156, train acc:0.3233333333333333, test acc:0.2838 ===\n",
            "train loss:2.09281735748407\n",
            "train loss:2.0026514426654036\n",
            "train loss:2.0162970636722286\n",
            "=== epoch:157, train acc:0.33666666666666667, test acc:0.2835 ===\n",
            "train loss:2.093138944696123\n",
            "train loss:2.0480212830312516\n",
            "train loss:2.0241182810886795\n",
            "=== epoch:158, train acc:0.3233333333333333, test acc:0.2827 ===\n",
            "train loss:2.0364458476930607\n",
            "train loss:2.0963782058547604\n",
            "train loss:1.9385587813579017\n",
            "=== epoch:159, train acc:0.3333333333333333, test acc:0.2831 ===\n",
            "train loss:2.1422850256200765\n",
            "train loss:2.07113657960551\n",
            "train loss:2.102845025898308\n",
            "=== epoch:160, train acc:0.33666666666666667, test acc:0.2843 ===\n",
            "train loss:2.0520576388368257\n",
            "train loss:1.9913197642596758\n",
            "train loss:2.071629275242106\n",
            "=== epoch:161, train acc:0.33666666666666667, test acc:0.284 ===\n",
            "train loss:2.0999452108677725\n",
            "train loss:2.008477433685521\n",
            "train loss:1.9851435053501327\n",
            "=== epoch:162, train acc:0.33, test acc:0.2847 ===\n",
            "train loss:2.1053896587313083\n",
            "train loss:2.000878491750463\n",
            "train loss:2.067605113393762\n",
            "=== epoch:163, train acc:0.33, test acc:0.2864 ===\n",
            "train loss:2.0992681975984695\n",
            "train loss:2.014301456820454\n",
            "train loss:2.021391632297807\n",
            "=== epoch:164, train acc:0.33, test acc:0.2866 ===\n",
            "train loss:2.0546606990848244\n",
            "train loss:2.1037370549329855\n",
            "train loss:2.100172061232167\n",
            "=== epoch:165, train acc:0.33, test acc:0.2875 ===\n",
            "train loss:2.003654767418017\n",
            "train loss:2.0744291028453237\n",
            "train loss:2.0104362167736824\n",
            "=== epoch:166, train acc:0.33, test acc:0.2874 ===\n",
            "train loss:2.0573718512637207\n",
            "train loss:2.0480052651781047\n",
            "train loss:2.075034346253985\n",
            "=== epoch:167, train acc:0.3333333333333333, test acc:0.2894 ===\n",
            "train loss:1.9513742798583877\n",
            "train loss:2.0154150783482345\n",
            "train loss:2.1277380636997956\n",
            "=== epoch:168, train acc:0.33, test acc:0.288 ===\n",
            "train loss:2.048484774039596\n",
            "train loss:1.96241970413511\n",
            "train loss:2.062735856487435\n",
            "=== epoch:169, train acc:0.33, test acc:0.2884 ===\n",
            "train loss:2.0547298667733447\n",
            "train loss:1.9799016567462298\n",
            "train loss:1.9811013749050386\n",
            "=== epoch:170, train acc:0.3333333333333333, test acc:0.2888 ===\n",
            "train loss:1.9585486078815495\n",
            "train loss:2.0104817934835015\n",
            "train loss:1.9901452937849728\n",
            "=== epoch:171, train acc:0.33, test acc:0.2888 ===\n",
            "train loss:2.00884483049486\n",
            "train loss:1.9810699407780625\n",
            "train loss:1.8742154875515034\n",
            "=== epoch:172, train acc:0.3333333333333333, test acc:0.2893 ===\n",
            "train loss:2.0278888137991\n",
            "train loss:1.9638440880307115\n",
            "train loss:1.961152840163108\n",
            "=== epoch:173, train acc:0.3333333333333333, test acc:0.2895 ===\n",
            "train loss:1.9591501674632412\n",
            "train loss:1.9414341662017767\n",
            "train loss:2.00240237663899\n",
            "=== epoch:174, train acc:0.33, test acc:0.2897 ===\n",
            "train loss:2.0331792374296036\n",
            "train loss:1.9263194115834799\n",
            "train loss:1.882982968888828\n",
            "=== epoch:175, train acc:0.33, test acc:0.2893 ===\n",
            "train loss:1.9486340132757778\n",
            "train loss:1.973915645438075\n",
            "train loss:1.9120256768508077\n",
            "=== epoch:176, train acc:0.3333333333333333, test acc:0.29 ===\n",
            "train loss:2.002988946283778\n",
            "train loss:2.0143937167224406\n",
            "train loss:1.9145270284308862\n",
            "=== epoch:177, train acc:0.3333333333333333, test acc:0.29 ===\n",
            "train loss:1.9561683970046944\n",
            "train loss:1.9371670782899089\n",
            "train loss:2.023896608766337\n",
            "=== epoch:178, train acc:0.3333333333333333, test acc:0.2902 ===\n",
            "train loss:1.8168503082000997\n",
            "train loss:2.0050154144528474\n",
            "train loss:1.9706411877608512\n",
            "=== epoch:179, train acc:0.3333333333333333, test acc:0.2904 ===\n",
            "train loss:1.968454953723879\n",
            "train loss:2.0083727193945227\n",
            "train loss:1.8790640374534298\n",
            "=== epoch:180, train acc:0.3333333333333333, test acc:0.2914 ===\n",
            "train loss:2.0575185721337497\n",
            "train loss:1.9418848592066436\n",
            "train loss:1.948121747705719\n",
            "=== epoch:181, train acc:0.34, test acc:0.2921 ===\n",
            "train loss:1.9092708289388272\n",
            "train loss:2.0157635711453823\n",
            "train loss:1.9629608134079635\n",
            "=== epoch:182, train acc:0.34, test acc:0.2934 ===\n",
            "train loss:1.9090800689940515\n",
            "train loss:1.9808845264654236\n",
            "train loss:2.017825517660338\n",
            "=== epoch:183, train acc:0.34, test acc:0.2953 ===\n",
            "train loss:1.9117195092079158\n",
            "train loss:1.8721087929469444\n",
            "train loss:1.9706705065452652\n",
            "=== epoch:184, train acc:0.34, test acc:0.296 ===\n",
            "train loss:1.9492867121419406\n",
            "train loss:1.9682395291982584\n",
            "train loss:1.9163128294417848\n",
            "=== epoch:185, train acc:0.34, test acc:0.2963 ===\n",
            "train loss:1.975965263309025\n",
            "train loss:1.9913119919511741\n",
            "train loss:1.8764794363800952\n",
            "=== epoch:186, train acc:0.34, test acc:0.2967 ===\n",
            "train loss:2.0105600922446016\n",
            "train loss:1.9364761885988904\n",
            "train loss:1.929370037191063\n",
            "=== epoch:187, train acc:0.34, test acc:0.2966 ===\n",
            "train loss:1.8487446347639007\n",
            "train loss:1.934569282249393\n",
            "train loss:1.8790233449171077\n",
            "=== epoch:188, train acc:0.34, test acc:0.2969 ===\n",
            "train loss:1.826417934488864\n",
            "train loss:1.8617841377511184\n",
            "train loss:1.926153805692854\n",
            "=== epoch:189, train acc:0.34, test acc:0.2967 ===\n",
            "train loss:1.997717418319433\n",
            "train loss:1.8901171582119738\n",
            "train loss:1.8248391239565354\n",
            "=== epoch:190, train acc:0.34, test acc:0.2972 ===\n",
            "train loss:1.9082095237807244\n",
            "train loss:1.8722485354199088\n",
            "train loss:1.8754327279556775\n",
            "=== epoch:191, train acc:0.34, test acc:0.298 ===\n",
            "train loss:1.8996159706065547\n",
            "train loss:1.949984945528045\n",
            "train loss:1.7679955276427481\n",
            "=== epoch:192, train acc:0.34, test acc:0.2965 ===\n",
            "train loss:1.9532502420196396\n",
            "train loss:1.861351200172007\n",
            "train loss:1.8719758736021157\n",
            "=== epoch:193, train acc:0.34, test acc:0.2988 ===\n",
            "train loss:1.7853724003579046\n",
            "train loss:1.9190628396767042\n",
            "train loss:1.9657884029179016\n",
            "=== epoch:194, train acc:0.34, test acc:0.2992 ===\n",
            "train loss:1.9317003477238266\n",
            "train loss:1.7644013600987707\n",
            "train loss:1.7274581654355086\n",
            "=== epoch:195, train acc:0.34, test acc:0.2991 ===\n",
            "train loss:1.8564781251789546\n",
            "train loss:1.8920057808461126\n",
            "train loss:1.8126094083401751\n",
            "=== epoch:196, train acc:0.34, test acc:0.2986 ===\n",
            "train loss:1.9002297694836687\n",
            "train loss:1.938001782127191\n",
            "train loss:1.8491731370746083\n",
            "=== epoch:197, train acc:0.34, test acc:0.2996 ===\n",
            "train loss:1.8849960833998167\n",
            "train loss:1.8759269721428518\n",
            "train loss:1.8832079170766107\n",
            "=== epoch:198, train acc:0.34, test acc:0.3009 ===\n",
            "train loss:1.8542232716165612\n",
            "train loss:1.9095964991817456\n",
            "train loss:1.7589659054988942\n",
            "=== epoch:199, train acc:0.34, test acc:0.3011 ===\n",
            "train loss:1.8508050405082634\n",
            "train loss:1.8319669179349183\n",
            "train loss:1.8014898020802954\n",
            "=== epoch:200, train acc:0.34, test acc:0.3014 ===\n",
            "train loss:1.8025932190874467\n",
            "train loss:1.827381444114806\n",
            "train loss:1.8127771486650466\n",
            "=== epoch:201, train acc:0.34, test acc:0.3009 ===\n",
            "train loss:1.7993460750176562\n",
            "train loss:1.8494417379831043\n",
            "train loss:1.9176695713888932\n",
            "=== epoch:202, train acc:0.3433333333333333, test acc:0.3029 ===\n",
            "train loss:1.8056711166984951\n",
            "train loss:1.884775307104775\n",
            "train loss:1.887477590524044\n",
            "=== epoch:203, train acc:0.3433333333333333, test acc:0.3037 ===\n",
            "train loss:1.8539857219347475\n",
            "train loss:1.8849106410608079\n",
            "train loss:1.8791852964601998\n",
            "=== epoch:204, train acc:0.3466666666666667, test acc:0.3069 ===\n",
            "train loss:1.9508182819760749\n",
            "train loss:1.8944406322022964\n",
            "train loss:1.8720525911260244\n",
            "=== epoch:205, train acc:0.3466666666666667, test acc:0.3076 ===\n",
            "train loss:1.8359817303367154\n",
            "train loss:1.7560442411235178\n",
            "train loss:1.8393610573957875\n",
            "=== epoch:206, train acc:0.35, test acc:0.3075 ===\n",
            "train loss:1.8804550874310684\n",
            "train loss:1.856923612313053\n",
            "train loss:1.8821127322573559\n",
            "=== epoch:207, train acc:0.35333333333333333, test acc:0.3082 ===\n",
            "train loss:1.853990313627813\n",
            "train loss:1.7886440885154322\n",
            "train loss:1.9077838341883306\n",
            "=== epoch:208, train acc:0.35333333333333333, test acc:0.3099 ===\n",
            "train loss:1.7853456682373718\n",
            "train loss:1.8514752178585707\n",
            "train loss:1.7958754388426603\n",
            "=== epoch:209, train acc:0.35333333333333333, test acc:0.3098 ===\n",
            "train loss:1.83115914455362\n",
            "train loss:1.8597500691599802\n",
            "train loss:1.887753564916671\n",
            "=== epoch:210, train acc:0.35, test acc:0.3102 ===\n",
            "train loss:1.8220572726059723\n",
            "train loss:1.6910228840816182\n",
            "train loss:1.8333690949881847\n",
            "=== epoch:211, train acc:0.35, test acc:0.3095 ===\n",
            "train loss:1.8781912822409805\n",
            "train loss:1.9635816037790514\n",
            "train loss:1.8266622863317254\n",
            "=== epoch:212, train acc:0.35333333333333333, test acc:0.3119 ===\n",
            "train loss:1.7756143325607736\n",
            "train loss:1.8881897447021132\n",
            "train loss:1.8257709783707552\n",
            "=== epoch:213, train acc:0.35333333333333333, test acc:0.3122 ===\n",
            "train loss:1.8909994258590965\n",
            "train loss:1.8117884263602448\n",
            "train loss:1.7884663410423445\n",
            "=== epoch:214, train acc:0.3566666666666667, test acc:0.3133 ===\n",
            "train loss:1.80386809500294\n",
            "train loss:1.8912611892899447\n",
            "train loss:1.7125334739886657\n",
            "=== epoch:215, train acc:0.3566666666666667, test acc:0.3139 ===\n",
            "train loss:1.754083934448904\n",
            "train loss:1.8012685360594651\n",
            "train loss:1.731255162674306\n",
            "=== epoch:216, train acc:0.3566666666666667, test acc:0.3143 ===\n",
            "train loss:1.8196132531050246\n",
            "train loss:1.8516973537300998\n",
            "train loss:1.7719044631291043\n",
            "=== epoch:217, train acc:0.3566666666666667, test acc:0.3157 ===\n",
            "train loss:1.6766923136036436\n",
            "train loss:1.8151412490275367\n",
            "train loss:1.8524991224200051\n",
            "=== epoch:218, train acc:0.36, test acc:0.3159 ===\n",
            "train loss:1.7357118988306943\n",
            "train loss:1.659896770222551\n",
            "train loss:1.696657639937266\n",
            "=== epoch:219, train acc:0.3566666666666667, test acc:0.3157 ===\n",
            "train loss:1.8784438941051094\n",
            "train loss:1.8221639430572767\n",
            "train loss:1.752170047960597\n",
            "=== epoch:220, train acc:0.3566666666666667, test acc:0.3174 ===\n",
            "train loss:1.7024414877075076\n",
            "train loss:1.7275307147114634\n",
            "train loss:1.7051535569654426\n",
            "=== epoch:221, train acc:0.3566666666666667, test acc:0.3195 ===\n",
            "train loss:1.8530406027918553\n",
            "train loss:1.761061630295368\n",
            "train loss:1.7353850108124178\n",
            "=== epoch:222, train acc:0.35333333333333333, test acc:0.3201 ===\n",
            "train loss:1.7357683353725364\n",
            "train loss:1.76716314014532\n",
            "train loss:1.765919577690665\n",
            "=== epoch:223, train acc:0.35333333333333333, test acc:0.3217 ===\n",
            "train loss:1.7133238341469672\n",
            "train loss:1.898520566088843\n",
            "train loss:1.742245935285389\n",
            "=== epoch:224, train acc:0.3566666666666667, test acc:0.3226 ===\n",
            "train loss:1.7916235104336606\n",
            "train loss:1.697847130158141\n",
            "train loss:1.7518427999056343\n",
            "=== epoch:225, train acc:0.36, test acc:0.3236 ===\n",
            "train loss:1.75408062346556\n",
            "train loss:1.6689649567667337\n",
            "train loss:1.7361291856558063\n",
            "=== epoch:226, train acc:0.36, test acc:0.3247 ===\n",
            "train loss:1.6372733334723444\n",
            "train loss:1.757380684022079\n",
            "train loss:1.7462230611054108\n",
            "=== epoch:227, train acc:0.36333333333333334, test acc:0.3238 ===\n",
            "train loss:1.8280277743259055\n",
            "train loss:1.616869994787311\n",
            "train loss:1.6349490520381549\n",
            "=== epoch:228, train acc:0.36333333333333334, test acc:0.3243 ===\n",
            "train loss:1.7341733197793767\n",
            "train loss:1.8093233657518524\n",
            "train loss:1.6549653694763564\n",
            "=== epoch:229, train acc:0.36666666666666664, test acc:0.3259 ===\n",
            "train loss:1.8349253482013241\n",
            "train loss:1.7340387143776552\n",
            "train loss:1.7658266781451233\n",
            "=== epoch:230, train acc:0.36666666666666664, test acc:0.3266 ===\n",
            "train loss:1.6907324549468905\n",
            "train loss:1.7279951983813706\n",
            "train loss:1.7947851472123728\n",
            "=== epoch:231, train acc:0.36666666666666664, test acc:0.3273 ===\n",
            "train loss:1.7944746465888723\n",
            "train loss:1.8765793741616144\n",
            "train loss:1.7095727548792192\n",
            "=== epoch:232, train acc:0.36333333333333334, test acc:0.3303 ===\n",
            "train loss:1.7132314876639978\n",
            "train loss:1.7769176175912218\n",
            "train loss:1.7170242556616042\n",
            "=== epoch:233, train acc:0.36333333333333334, test acc:0.3307 ===\n",
            "train loss:1.7350892827081312\n",
            "train loss:1.773661110846701\n",
            "train loss:1.6949916763541242\n",
            "=== epoch:234, train acc:0.37333333333333335, test acc:0.3315 ===\n",
            "train loss:1.773913808449914\n",
            "train loss:1.7530100679893232\n",
            "train loss:1.7392757354584538\n",
            "=== epoch:235, train acc:0.38, test acc:0.332 ===\n",
            "train loss:1.5935267374477242\n",
            "train loss:1.6524153346841743\n",
            "train loss:1.6329823674480008\n",
            "=== epoch:236, train acc:0.38, test acc:0.3337 ===\n",
            "train loss:1.6440601460843751\n",
            "train loss:1.7098192989111716\n",
            "train loss:1.6106705910492303\n",
            "=== epoch:237, train acc:0.38, test acc:0.3329 ===\n",
            "train loss:1.6358396754008973\n",
            "train loss:1.7214446687948362\n",
            "train loss:1.6851532801085936\n",
            "=== epoch:238, train acc:0.37666666666666665, test acc:0.3361 ===\n",
            "train loss:1.7831065742597523\n",
            "train loss:1.678442303170898\n",
            "train loss:1.596670027897954\n",
            "=== epoch:239, train acc:0.38, test acc:0.3367 ===\n",
            "train loss:1.7310501780854712\n",
            "train loss:1.6753372873477628\n",
            "train loss:1.7072710864197254\n",
            "=== epoch:240, train acc:0.37666666666666665, test acc:0.3366 ===\n",
            "train loss:1.644558716762149\n",
            "train loss:1.7243578745439578\n",
            "train loss:1.7265334994045625\n",
            "=== epoch:241, train acc:0.37666666666666665, test acc:0.3386 ===\n",
            "train loss:1.7418001113700623\n",
            "train loss:1.6933055902109655\n",
            "train loss:1.667031208201964\n",
            "=== epoch:242, train acc:0.38333333333333336, test acc:0.3404 ===\n",
            "train loss:1.7757787070509554\n",
            "train loss:1.578416546393472\n",
            "train loss:1.7509637721452846\n",
            "=== epoch:243, train acc:0.38333333333333336, test acc:0.3416 ===\n",
            "train loss:1.7128556010567755\n",
            "train loss:1.721829297207313\n",
            "train loss:1.5789875595393132\n",
            "=== epoch:244, train acc:0.38, test acc:0.3394 ===\n",
            "train loss:1.6544736333814074\n",
            "train loss:1.6292426795611932\n",
            "train loss:1.5995679490002268\n",
            "=== epoch:245, train acc:0.38666666666666666, test acc:0.3424 ===\n",
            "train loss:1.753294144367133\n",
            "train loss:1.596976133593117\n",
            "train loss:1.701272421854386\n",
            "=== epoch:246, train acc:0.39, test acc:0.3445 ===\n",
            "train loss:1.6534056181921537\n",
            "train loss:1.6145633877904533\n",
            "train loss:1.6791800916575017\n",
            "=== epoch:247, train acc:0.39, test acc:0.3414 ===\n",
            "train loss:1.619572370006608\n",
            "train loss:1.6900269518529831\n",
            "train loss:1.7481778633277085\n",
            "=== epoch:248, train acc:0.39, test acc:0.3435 ===\n",
            "train loss:1.6944609582836256\n",
            "train loss:1.6788026479615945\n",
            "train loss:1.6549836919011376\n",
            "=== epoch:249, train acc:0.39, test acc:0.3454 ===\n",
            "train loss:1.5994946402729568\n",
            "train loss:1.6534092429325054\n",
            "train loss:1.6146044520001082\n",
            "=== epoch:250, train acc:0.4, test acc:0.3495 ===\n",
            "train loss:1.539908214102757\n",
            "train loss:1.754632316324678\n",
            "train loss:1.5708833415184031\n",
            "=== epoch:251, train acc:0.3933333333333333, test acc:0.3496 ===\n",
            "train loss:1.7205642394924803\n",
            "train loss:1.738602823203957\n",
            "train loss:1.7806825497746936\n",
            "=== epoch:252, train acc:0.4033333333333333, test acc:0.3527 ===\n",
            "train loss:1.547761593781448\n",
            "train loss:1.6931260843555123\n",
            "train loss:1.7321523076804128\n",
            "=== epoch:253, train acc:0.4066666666666667, test acc:0.3564 ===\n",
            "train loss:1.6674001230373119\n",
            "train loss:1.6559918075478735\n",
            "train loss:1.6586620425471978\n",
            "=== epoch:254, train acc:0.41, test acc:0.3559 ===\n",
            "train loss:1.741375656950954\n",
            "train loss:1.686492926985961\n",
            "train loss:1.5381702679322737\n",
            "=== epoch:255, train acc:0.41333333333333333, test acc:0.3565 ===\n",
            "train loss:1.818229136710665\n",
            "train loss:1.7844989524899575\n",
            "train loss:1.6399947145851215\n",
            "=== epoch:256, train acc:0.41333333333333333, test acc:0.3605 ===\n",
            "train loss:1.7131157251587632\n",
            "train loss:1.5875123695955407\n",
            "train loss:1.5122666323501006\n",
            "=== epoch:257, train acc:0.41, test acc:0.361 ===\n",
            "train loss:1.6317051491170995\n",
            "train loss:1.6211108865205106\n",
            "train loss:1.5347082183497378\n",
            "=== epoch:258, train acc:0.4066666666666667, test acc:0.3571 ===\n",
            "train loss:1.5308871432825983\n",
            "train loss:1.5398190580847697\n",
            "train loss:1.6295240993654483\n",
            "=== epoch:259, train acc:0.4166666666666667, test acc:0.3604 ===\n",
            "train loss:1.5221644249122945\n",
            "train loss:1.6538592239296304\n",
            "train loss:1.512891409334875\n",
            "=== epoch:260, train acc:0.41333333333333333, test acc:0.3616 ===\n",
            "train loss:1.658658384919683\n",
            "train loss:1.5590675991982714\n",
            "train loss:1.6583447124053263\n",
            "=== epoch:261, train acc:0.41333333333333333, test acc:0.3615 ===\n",
            "train loss:1.5958712566745998\n",
            "train loss:1.620214424517467\n",
            "train loss:1.664356035770117\n",
            "=== epoch:262, train acc:0.42333333333333334, test acc:0.363 ===\n",
            "train loss:1.534796766218944\n",
            "train loss:1.7735236199457116\n",
            "train loss:1.4530833106619017\n",
            "=== epoch:263, train acc:0.4266666666666667, test acc:0.3654 ===\n",
            "train loss:1.6464395827956022\n",
            "train loss:1.5891457470983659\n",
            "train loss:1.579934153913872\n",
            "=== epoch:264, train acc:0.4266666666666667, test acc:0.3694 ===\n",
            "train loss:1.4505939983047569\n",
            "train loss:1.6348742365270357\n",
            "train loss:1.5783374120876967\n",
            "=== epoch:265, train acc:0.43, test acc:0.3702 ===\n",
            "train loss:1.607448694724691\n",
            "train loss:1.552222810144055\n",
            "train loss:1.522976714903799\n",
            "=== epoch:266, train acc:0.44, test acc:0.3725 ===\n",
            "train loss:1.563372078887965\n",
            "train loss:1.576978366884235\n",
            "train loss:1.5146365601241973\n",
            "=== epoch:267, train acc:0.43333333333333335, test acc:0.3715 ===\n",
            "train loss:1.515770486081202\n",
            "train loss:1.5379210046534069\n",
            "train loss:1.5416496383179323\n",
            "=== epoch:268, train acc:0.43666666666666665, test acc:0.373 ===\n",
            "train loss:1.4855230285589354\n",
            "train loss:1.5705617259444509\n",
            "train loss:1.5195749517198143\n",
            "=== epoch:269, train acc:0.44, test acc:0.3737 ===\n",
            "train loss:1.632257398933988\n",
            "train loss:1.5582442198015911\n",
            "train loss:1.6496404641978843\n",
            "=== epoch:270, train acc:0.44333333333333336, test acc:0.3761 ===\n",
            "train loss:1.5595481378526062\n",
            "train loss:1.3830185841799867\n",
            "train loss:1.561669963765495\n",
            "=== epoch:271, train acc:0.44333333333333336, test acc:0.3764 ===\n",
            "train loss:1.5099543774350856\n",
            "train loss:1.5708845988659643\n",
            "train loss:1.5003481242832828\n",
            "=== epoch:272, train acc:0.44333333333333336, test acc:0.3795 ===\n",
            "train loss:1.5155239893921402\n",
            "train loss:1.4209331656446258\n",
            "train loss:1.779844326639114\n",
            "=== epoch:273, train acc:0.4533333333333333, test acc:0.3838 ===\n",
            "train loss:1.6144438103966579\n",
            "train loss:1.5524737016146488\n",
            "train loss:1.5353125036618476\n",
            "=== epoch:274, train acc:0.46, test acc:0.3909 ===\n",
            "train loss:1.586008203563536\n",
            "train loss:1.5518593124135123\n",
            "train loss:1.4691221717866774\n",
            "=== epoch:275, train acc:0.45666666666666667, test acc:0.3905 ===\n",
            "train loss:1.565371562959944\n",
            "train loss:1.6285760403760543\n",
            "train loss:1.6306894427859513\n",
            "=== epoch:276, train acc:0.46, test acc:0.3926 ===\n",
            "train loss:1.465408206012157\n",
            "train loss:1.4841647213765041\n",
            "train loss:1.6095764421682446\n",
            "=== epoch:277, train acc:0.46, test acc:0.3892 ===\n",
            "train loss:1.6603238164229936\n",
            "train loss:1.5653363290154214\n",
            "train loss:1.5016043541898965\n",
            "=== epoch:278, train acc:0.46, test acc:0.3925 ===\n",
            "train loss:1.5335386794214285\n",
            "train loss:1.46293104560829\n",
            "train loss:1.524093406177189\n",
            "=== epoch:279, train acc:0.4633333333333333, test acc:0.3942 ===\n",
            "train loss:1.6057411064126064\n",
            "train loss:1.53840230737272\n",
            "train loss:1.5209824173456834\n",
            "=== epoch:280, train acc:0.4666666666666667, test acc:0.3976 ===\n",
            "train loss:1.5921263938177856\n",
            "train loss:1.4620200506611172\n",
            "train loss:1.5565096862084702\n",
            "=== epoch:281, train acc:0.46, test acc:0.3956 ===\n",
            "train loss:1.5525085651510886\n",
            "train loss:1.5219177461101234\n",
            "train loss:1.5646001050544363\n",
            "=== epoch:282, train acc:0.4666666666666667, test acc:0.4011 ===\n",
            "train loss:1.4587270321485926\n",
            "train loss:1.5825804604170406\n",
            "train loss:1.3792849760768064\n",
            "=== epoch:283, train acc:0.46, test acc:0.3969 ===\n",
            "train loss:1.662981476779712\n",
            "train loss:1.484169819798052\n",
            "train loss:1.441046967936015\n",
            "=== epoch:284, train acc:0.46, test acc:0.4014 ===\n",
            "train loss:1.6932224361539001\n",
            "train loss:1.5455651614085602\n",
            "train loss:1.5075061716103135\n",
            "=== epoch:285, train acc:0.47333333333333333, test acc:0.4068 ===\n",
            "train loss:1.6064612388749444\n",
            "train loss:1.5782483952470867\n",
            "train loss:1.575581746609566\n",
            "=== epoch:286, train acc:0.48, test acc:0.412 ===\n",
            "train loss:1.5466131957350282\n",
            "train loss:1.4246583465480713\n",
            "train loss:1.5164586095793497\n",
            "=== epoch:287, train acc:0.4766666666666667, test acc:0.4086 ===\n",
            "train loss:1.6013585630958673\n",
            "train loss:1.4819548739165904\n",
            "train loss:1.494850240914965\n",
            "=== epoch:288, train acc:0.4766666666666667, test acc:0.4134 ===\n",
            "train loss:1.632105897631557\n",
            "train loss:1.401205624580798\n",
            "train loss:1.6883352759419414\n",
            "=== epoch:289, train acc:0.48, test acc:0.4184 ===\n",
            "train loss:1.4625429059577377\n",
            "train loss:1.465275490590987\n",
            "train loss:1.497210679606962\n",
            "=== epoch:290, train acc:0.48, test acc:0.4186 ===\n",
            "train loss:1.5277317304799434\n",
            "train loss:1.5512910652742944\n",
            "train loss:1.5778252298535642\n",
            "=== epoch:291, train acc:0.4866666666666667, test acc:0.419 ===\n",
            "train loss:1.5323836929306498\n",
            "train loss:1.6638776731456988\n",
            "train loss:1.537682737698141\n",
            "=== epoch:292, train acc:0.49666666666666665, test acc:0.4243 ===\n",
            "train loss:1.3541718446177433\n",
            "train loss:1.4701286431238438\n",
            "train loss:1.4385126679728768\n",
            "=== epoch:293, train acc:0.5, test acc:0.4211 ===\n",
            "train loss:1.4667246389311865\n",
            "train loss:1.388373568805994\n",
            "train loss:1.3319441262771488\n",
            "=== epoch:294, train acc:0.5, test acc:0.4232 ===\n",
            "train loss:1.6017477148967318\n",
            "train loss:1.501183582291347\n",
            "train loss:1.5493928954984875\n",
            "=== epoch:295, train acc:0.5, test acc:0.4321 ===\n",
            "train loss:1.4876060385501395\n",
            "train loss:1.6174962555636214\n",
            "train loss:1.3570406540012567\n",
            "=== epoch:296, train acc:0.49666666666666665, test acc:0.4331 ===\n",
            "train loss:1.4950070461398024\n",
            "train loss:1.5450587555393462\n",
            "train loss:1.424233996914085\n",
            "=== epoch:297, train acc:0.5066666666666667, test acc:0.4354 ===\n",
            "train loss:1.4418318959453966\n",
            "train loss:1.458166714743467\n",
            "train loss:1.6424971169217872\n",
            "=== epoch:298, train acc:0.51, test acc:0.4376 ===\n",
            "train loss:1.558582867233442\n",
            "train loss:1.4994088103755903\n",
            "train loss:1.4719495167538803\n",
            "=== epoch:299, train acc:0.51, test acc:0.4415 ===\n",
            "train loss:1.439954090746478\n",
            "train loss:1.433625963561915\n",
            "train loss:1.483146104076375\n",
            "=== epoch:300, train acc:0.5166666666666667, test acc:0.4414 ===\n",
            "train loss:1.504970117001809\n",
            "train loss:1.414659617866742\n",
            "train loss:1.427430388840918\n",
            "=== epoch:301, train acc:0.5166666666666667, test acc:0.4441 ===\n",
            "train loss:1.4974424668330142\n",
            "train loss:1.6524586973157174\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.4485\n"
          ]
        }
      ],
      "source": [
        "# 6.4.3.4 Dropout 사용하지 않음 (# 6.4.3.2 미사용본과 비교)\n",
        "\n",
        "# 은닉층 6개층 100개 노드\n",
        "# 은닉층 활성화 함수 : ReLU\n",
        "# 가중치 초기설정 : He\n",
        "\n",
        "# 배치 사이즈 : 100\n",
        "# 최대 반복 회수 : 301 epoch\n",
        "# 훈련방식 : SGD   학습률 0.01\n",
        "\n",
        "\n",
        "# 훈련데이터 크기 : 300\n",
        "\n",
        "# 실험 드롭아웃 사용여부에 따른 비교 (dropout_ratio = 0.2)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해서 학습 데이터를 제거\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# Dropuout의 유무, 비율의 설정  ========================\n",
        "use_dropout = True  # Dropout을 사용 ***\n",
        "dropout_ratio = 0.2\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(\n",
        "            input_size=784,\n",
        "            hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "            output_size=10,\n",
        "            use_dropout=use_dropout,\n",
        "            dropout_ration=dropout_ratio)\n",
        "\n",
        "\n",
        "# Trainer 클래스는 간소화 된 훈련 로직 (훈련 로직을 담고 있는 클래스)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01},\n",
        "                  verbose=True)\n",
        "# 훈련 수행\n",
        "trainer.train()\n",
        "\n",
        "# 훈련 중 정확도 측정 정보\n",
        "train_acc_list, test_acc_list =\\\n",
        "   trainer.train_acc_list, trainer.test_acc_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shared-manor",
      "metadata": {
        "id": "shared-manor",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "8d3ac0a1-eac4-4e0d-d32c-e0bf7cc5ef44"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAFNCAYAAADxUUMiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xW5f3/8dcnISRhb5QNiihKFUUcWNwCausetbSuih12WipW21rbWlf70/Zr3bbWVpQ6EBXFiVsUBJEpiCgJG9kkIePz++OcwJ3kvu9zZ9xkvZ+PRx7c55zrvs51cpPzuc81zd0RERFJJqO+CyAiIg2fgoWIiERSsBARkUgKFiIiEknBQkREIilYiIhIJAWLJsbMXjCzi+u7HM2NmfUxs21mlrmHz9vPzNzMWuzJ80rzo2DRAIQ3mfKfMjMriNn+dnXycvcx7v5wusqaTBioystdbGY7Y7bvqUF+N5jZf1JId4mZfWJmO8xstZndbWYdanYVKZXLzWx7zLVtcvcv3b2Nu5eGaaab2ffSVYaGzsyOM7O8NOQ7P+b3XmpmhTHbv67r88lu+jbSALh7m/LXZrYc+J67v1I5nZm1cPeSPVm26nD3MeWvzexfQJ67X5/Oc5rZ1cCvgIuBV4GewD+Al81shLvvrMNzxf7+D3b3pXWVdzqYmQHm7mX1XZa64u4Hlr82s+nAf9z9gcrpGvrfSmOkJ4sGrPzbmZldY2argX+aWUcze87M1pnZxvB1r5j37PpGG37jftvMbg/Tfm5mYxKc6xoze6LSvjvN7G8xeS0zs61hPtV64jGz081sjpltMrN3zexrlc6dH+a92MxONLPRwK+BC8JvjR/HybMd8Hvgx+7+orsXu/ty4HygHzDWzHqET2qdYt431MzWm1lWuH2ZmS0Mf0fTzKxvTFo3sx+Z2RJgSZLr21UdZGZ/Ar4O/F9Y9v+Lk/7hMNBhZj3LzxNu72NmX5lZlb9PM8sMP8/1ZrYMOK3S8elm9iczewfYAQwws6PN7EMz2xz+e3Sl9H82sw/MbIuZPVPpd/XN8Nv8pjDtAZV+N/vGbP/LzP5oZq2BF4AeMd/6eyT63dWFmN//5Wb2JfBavKcbM1tuZieFrzPMbIKZfWZmG8xsUuy1S0UKFg3fXkAnoC8wjuAz+2e43QcoAKrcjGIcASwGugC3Ag+G3zgreww41czaQnBTIrjpPhr+8f8NGOPubYGjgTmpXoCZDQUeAq4EOgP3AlPMLNvMBgFXAYeHeY8Clrv7i8BNwONh9c7BcbI+GsgBnord6e7bgKnAye6+EngPOCcmyUXAE+5ebGZnEASls4GuwFvAxErnOZPg9zg4let19+vCfK4Ky35VnGRvAMeFr48FlgEjY7bfSvBEcAVwOjAUGAacGyfNdwj+r7QFtgLPE3x+nYG/As+bWeeY9N8FLgP2BkrCtJjZfgS/i58R/G6mAs+aWcuI698OjAFWhtffJvwcKghv1JsS/SQ7RxLHAgcQ/D+K8mOCz/ZYoAewEbirhudt8hQsGr4y4HfuXuTuBe6+wd2fdPcd7r4V+BPBf/ZEvnD3+8O69IcJbgjdKydy9y+Aj4Czwl0nADvc/f2YchxkZrnuvsrd51fjGsYB97r7DHcvDdtUioAjgVIgGxhsZlnuvtzdP0sx3y7A+gTVDavC4wCPAt+CXVUzF4b7AL4P/NndF4b53AQcEvt0ER7/yt0LYvZ9FHNj+1uK5Y31BnBM+PQwkiCQjwiPHRsej+d84A53X+HuXwF/jpPmX+4+P7yeU4Al7v6Iu5e4+0RgEfCNmPSPuPu88Cb/G+D88MvCBcDz7v6yuxcDtwO5BEG61tz9ZnfvkOinhtne4O7bK31WiXwfuM7d89y9CLgBONfUWSAuBYuGb527F5ZvmFkrM7vXzL4wsy3Am0AHS9wLZ3X5C3ffEb5skyDtrpsqwbfvR8P3bSe4cXwfWGVmz5vZ/tW4hr7A1ZW+NfYGeoT1/j8j+ENda2aPVaPKYj3QJcEf997hcYAngaPMbG+CG3MZwTf/8rLdGVOurwAjaPsotyJO/ofG3Nh+kmJ5dwkD4nbgEIIqq+eAleGTVrJg0aNSeb6Ikyb2eI84ab4g8fV9AWQRBNoK7w2fdFZUem9DE++zSqQv8HTMZ7+Q4MtLlS9TomDRGFSeFvhqYBBwhLu3Y3fVRbyqper6H3CcBW0gZ7H72zfuPs3dTya4CS8C7q9GviuAP1X65tgq/JaLuz/q7scQ/PE6cEv5aSPyfY/gCeXs2J1m1oagGuTVMP+NwEsEAe8i4DHfPd3yCuDKSmXLdfd3Y7KsydTMqbznDYJqpJbunh9uXwx0JHE13yqCQFuuT8S5VxL8XmP1AfJjtivnV0wQaCu8N3wq6x3z3h1Aq5j37pWgDHGZ2a+tYk/ACj9R708g9rzbY8sXfqHqGnN8BUHVauxnnxN+FlKJgkXj05agnWJT2Bj3u7rK2N3XAdMJ2kQ+d/eFAGbW3czOCNsuioBtBN/OU3U/8H0zO8ICrc3sNDNra2aDzOwEM8sGCsNrK897DdAvXkNvWN7NBA3cfzez0WaWZWb9gElAHvBITPJHCermzyUmCAL3ANea2YHhtbY3s/OqcW2JrAEGRKR5g6C95s1we3q4/XZ5F9w4JgE/MbNeZtYRmBBxjqnAfmZ2kQWN7xcQtL08F5NmrJkNNrNWwI0E7Tml4blOs6DDQRbBF5UioDyQzgEusqDRfTQVq0PXAJ3NrH2igrn7TTFtGlV+Iq4rFZ8COeH/tSzgeoIqz3L3AH8qr3I0s65hG5bEoWDR+NxBUG+8HngfeLGO838UOImKN9QM4BcE3zS/Irgp/ADAzL4e9S3Q3WcSNMz+H0Ej4lLgkvBwNnAzwfWsBroB14bH/hf+u8HMPkqQ960EDdS3A1uAGQTfGE8M66HLTQEGAqvd/eOY9z9N8CTzWFitN4/gqaS27iSo/96YpE3jDYLgXx4s3ib4JvxmgvQQBN5pwMcEbUxPJUmLu28gaBC/GthA0M34dHdfH5PsEeBfBL//HOAn4XsXA2OBvxN8Pt8AvhHTHfmn4b5NwLeByTHnXUTQOL4srOZJa2+oeMIvEz8EHiB4GtpO8CWi3J0E/y9eMrOtBH9PR+zpcjYW5lr8SKTZsiRjFURi6clCREQipS1YmNlDZrbWzOYlOG5m9jczW2pmc83s0HSVRUREaiedTxb/AkYnOT6GoA55IEE//LvTWBYRicPdj1MVlKQibcHC3d8kaAxN5Azg3x54n2CswN7pKo+IiNRcfbZZ9KTiAJo8GvZgHxGRZqtRDGs3s3EEVVW0bt36sP33r87gYRERmTVr1np37xqdMr76DBb5VBw52ouKo0p3cff7gPsAhg0b5jNnzkx/6UREmhAzizc1TMrqsxpqCvDdsFfUkcBmd19Vj+UREZEE0vZkYWYTCaZg7mLBnPK/I5igDHe/h2AaglMJRvPuAC5NV1lERKR20hYs3P1bEccd+FG6zi8iInVHI7hFRCSSgoWIiERSsBARkUgKFiIiEknBQkREIilYiIhIJAULERGJpGAhIiKRFCxERCSSgoWIiERSsBARkUgKFiIiEknBQkREIilYiIhIJAULERGJpGAhIiKRFCxERCSSgoWIiERSsBARkUgKFiIiEknBQkREIilYiIhIJAULERGJpGAhIiKRFCxERCSSgoWIiERSsBARkUgKFiIiEknBQkREIilYiIhIJAULERGJpGAhIiKRFCxERCSSgoWIiERSsBARkUgKFiIiEknBQkREIilYiIhIJAULERGJpGAhIiKRFCxERCSSgoWIiERSsBARkUgKFiIiEknBQkREIilYiIhIpLQGCzMbbWaLzWypmU2Ic7yPmb1uZrPNbK6ZnZrO8oiISM2kLViYWSZwFzAGGAx8y8wGV0p2PTDJ3YcCFwL/SFd5RESk5tL5ZDEcWOruy9x9J/AYcEalNA60C1+3B1amsTwiIlJD6QwWPYEVMdt54b5YNwBjzSwPmAr8OF5GZjbOzGaa2cx169alo6wiIpJEfTdwfwv4l7v3Ak4FHjGzKmVy9/vcfZi7D+vateseL6SISHOXzmCRD/SO2e4V7ot1OTAJwN3fA3KALmksk4iI1EA6g8WHwEAz629mLQkasKdUSvMlcCKAmR1AECxUzyQi0sCkLVi4ewlwFTANWEjQ62m+md1oZt8Mk10NXGFmHwMTgUvc3dNVJhERqZkW6czc3acSNFzH7vttzOsFwIh0lkFERGqvvhu4RUSkEVCwEBGRSAoWIiISScFCREQiKViIiEgkBQsREYmkYCEiIpEULEREJJKChYiIRFKwEBGRSAoWIiISScFCREQiKViIiEgkBQsREYmkYCEiIpEULEREJJKChYiIRFKwEBGRSAoWIiISScFCREQiKViIiEgkBQsREYmkYCEiIpEULEREJJKChYiIRFKwEBGRSAoWIiISScFCREQiKViIiEgkBQsREYmkYCEiIpEULEREJJKChYiIRFKwEBGRSAoWIiISScFCREQiKViIiEgkBQsREYmkYCEiIpEULEREJJKChYiIRFKwEBGRSAoWIiISScFCREQiKViIiEiktAYLMxttZovNbKmZTUiQ5nwzW2Bm883s0XSWR0REaqZFujI2s0zgLuBkIA/40MymuPuCmDQDgWuBEe6+0cy6pas8IiJSc+l8shgOLHX3Ze6+E3gMOKNSmiuAu9x9I4C7r01jeUREpIbSGSx6AititvPCfbH2A/Yzs3fM7H0zG53G8oiISA2lrRqqGucfCBwH9ALeNLMh7r4pNpGZjQPGAfTp02dPl1FEpNlL55NFPtA7ZrtXuC9WHjDF3Yvd/XPgU4LgUYG73+fuw9x9WNeuXdNWYBERiS+dweJDYKCZ9TezlsCFwJRKaSYTPFVgZl0IqqWWpbFMIiJSA2kLFu5eAlwFTAMWApPcfb6Z3Whm3wyTTQM2mNkC4HVgvLtvSFeZRESkZszd67sM1TJs2DCfOXNmfRdDRKRRMbNZ7j6spu/XCG4REYmkYCEiIpFSChZm9pSZnWZmCi4iIs1Qqjf/fwAXAUvM7GYzG5TGMomISAOTUrBw91fc/dvAocBy4BUze9fMLjWzrHQWUERE6l/K1Upm1hm4BPgeMBu4kyB4vJyWkomISIOR0nQfZvY0MAh4BPiGu68KDz1uZurHKiLSxKU6N9Tf3P31eAdq029XREQah1SroQabWYfyDTPraGY/TFOZRESkgUk1WFwROxNsuP7EFekpkoiINDSpBotMM7PyjXAVvJbpKZKIiDQ0qbZZvEjQmH1vuH1luE9ERJqBVIPFNQQB4gfh9svAA2kpkYiINDgpBQt3LwPuDn9ERKSZSXWcxUDgz8BgIKd8v7sPSFO5RESkDkyenc9t0xbTcq99D6tNPqk2cP+T4KmiBDge+Dfwn9qcWERE0mvy7HyufeoT8jcV1DqvVINFrru/SrBY0hfufgNwWq3PLiIiaXPbtMUUFJfWSV6pNnAXhdOTLzGzq4B8oE2dlEBERNJiZR08UZRL9cnip0Ar4CfAYcBY4OI6K4WIiNSZDduKuG3aIjIzLDpxiiKfLMIBeBe4+y+BbcCldXZ2ERGpU/mbCjj/nvdYvaWQVlkZFJU4O0vLap1vZLBw91IzO6bWZxIRkbS7/81lrNtaxFM/OJqDe3fY1RtqVfRbk0q1zWK2mU0B/gdsL9/p7k/V8vwiIlJDWwqL+c3keczN2wxAv86tmLl8I6cO2YuDewdzv545tCdnDu2JXbt0Vm3OlWqwyAE2ACfE7HNAwUJEpB6UlTnfvn8GC1dt4ZQDu5OZkcGrC9ewY2cpY4/sW+fnS3UEt9opREQaiMmz8/nDcwvYsH0nHXKzOGXwXpw5tCdL1mxl1hcbOaxvxzo/Z6ojuP9J8CRRgbtfVuclEhGRhMoH2pWPn9hUUMy1T30CBFVOA7u3Tct5U+06+xzwfPjzKtCOoGeUiIjsQfEG2hUUl3LbtMVpPW+q1VBPxm6b2UTg7bSUSEREdsnfVED+xgKG9+9EYXFpwqk76nIAXjypNnBXNhDoVpcFERGRinaWlHHJQx+wZO02zjusFwtWbUmYtkeH3LSWJaVqKDPbamZbyn+AZwnWuBARkTR56J3PWbJ2GyP368pTs/NZtbmQS0f0Iyer4q07NyuT8aMGpbUsqVZDpafFRERE4nJ3HnnvC47Ztwv/vmx4hWMH9+rAbdMWs3JTAT065DJ+1CDOHNozreVJtTfUWcBr7r453O4AHOfuk9NZOBGR5uCdpeu55YWFrNlaxJotRXRtk83FI/qSv6mAn5y4b5X05QPt9qRUe0P9rjxQALj7JuB36SmSiEjz4O489PbnjH1gBnPzt7BmSxEA67YVccfLSwA4Yf/u9VnEXVJt4I4XVGraOC4i0uxs3lHMBfe9x7B+Hfl0zTZmf7kRdygpc3JaZFBYUnGyv5IyJyvT6No2u55KXFGqN/yZZvZX4K5w+0dAreYZERFpTm6dtojFa7ayaPVWWrXM5OKj+tGyRQZ9OrXaNaiuspLSKmOh602qweLHwG+AxwlGcr9MEDBERCSO8tleV24qoGvbbNZuLeLSEf04cf/u9OiQw4Cuu9eP+/trS+OOn6h1d9jbBsL2tQActndGrdbgTrU31HZgQm1OJCLSXFSekmPt1qAtYkDX1hwzsEuV9ONHDaqQHuqoO2wYKOpCquMsXg57QJVvdzSzaXVWChGRJiTR2tf3TF8WN/2ZQ3vy57OH0LNDLgb07JDLn88essd7PCWTajVUl7AHFADuvtHMNIJbRCQ0eXY+++/dlv33apdw6o1kU3LUeXfY/LptVk41WJSZWR93/xLAzPoRZxZaEZHmaMVXO/j5pDn069ya7x7VN+HNMd1TcgBQshNmPgiv/alOs001WFwHvG1mbwAGfB0YV6clERFppP4740sAPl+/nd8/u4DBe7dl2frtFBbv7g6b9ik5tq0LgsTHE2HjchhwHCybXmfZp9rA/aKZDSMIELOByUB6pzgUEWkElq/fzqSZKzj5gO4M7tGOrMwMvn/sPjz78co9OyXH5O/D0leh1zA47S+wz4lw+3511sht7tG1SWb2PeCnQC9gDnAk8J67n5D0jWkwbNgwnzlz5p4+rYhIBc/MyeeWFxaxfttOWmVn8p/Lj+Cgnu3Tf+KY7rBVnPAbGPnLuIfMbJa7D6vpaVOthvopcDjwvrsfb2b7AzfV9KQiIo1R+diJ8jERfTrl8t2j+nLx0f3o3anVnilEsieFwy9P22lTDRaF7l5oZphZtrsvMrP0zocrItKAPP7Bl/zu2fkV2iHWbi3ioJ7t91yg2Jyf/Hhu3a+9XS7ViQTzwnEWk4GXzewZ4Iu0lUpEpB65OzOWbeDNT9dRGI6X+M0zFQMFQGFxWdqXM92laBs8esGeOVccqTZwnxW+vMHMXgfaAy9Gvc/MRgN3ApnAA+5+c4J05wBPAIe7uxokRKTebC8qYfwTHzP1k9UAfK1Xe646fl92lpbFTZ/u5Ux59Q/w5XvQsjWsnZ/ecyVR7Zlj3f2NVNKZWSbBxIMnA3nAh2Y2xd0XVErXlqBNZEZ1yyIiUpdWfLWDK/49k0/XbGX8qEH06JDD9U/PY9wjszDiDy5L69iJT56At24HywAvg9G3wIv1s0hpOqcZHw4sdfdlAGb2GHAGsKBSuj8AtwDj01gWEZGk5qzYxKX//IDSMueflw7n2P26AnBgj/b8ZOJsenbI5d3PNtT9/E2QvIdTn6Pg9DuCp4oDz4a3/hI/bev0TqqRzmDRE1gRs50HHBGbwMwOBXq7+/NmpmAhInvchCfnMm3+ajbuKCbD4NoxB+wKFAD7dW/Liz8bCVScSbZOx04k6+F07j+h3d7Qbf9ge/yS2p+vBuptASMzywD+ClySQtpxhCPG+/Tpk96CiUiz8eiML3jsw93facsc/vryp3Rtmx03CKRlOdOosW7t9q7b89VQqr2haiIf6B2z3SvcV64tcBAw3cyWEwz0mxKOFK/A3e9z92HuPqxr166VD4uI1MgtLy6qsq+guDR9PZy2rIT1S+CNW+HDB2HJK3DPMek5Vx1L55PFh8BAM+tPECQuBC4qPxiu6b1rYnczmw78Ur2hRCRdnpyVx59fWMiOnUG7Q/m/laWlh9PG5XDXkVBSKe8u+9X9udIgbcHC3UvM7CpgGkHX2Yfcfb6Z3QjMdPcp6Tq3iDRvsW0L3dpl8/OT9qNliwyu/t/HDOvbkaF9guV5Jn7wJduKqgaMulyhrorT7wjmb1rycvCkcfKNcFPDqGpKJq1tFu4+FZhaad9vE6Q9Lp1lEZGmp7TMmbNiE2XutMluwQF7t+PBt5Zx20uLdw2gW7OliAlPfUJuVgbD+nZk4rgjycoMauAP7NF+z69QN+zS4N+9huze17pbvfRwqo56a+AWEamtX0yawzNzVu7aPqJ/J2Z8/lXctAXFZfzxrIN2BQpgV2P1Hp0dNp566uFUHQoWItIoVO62+o2D9+aZOSu55Oh+nHRAd95aso5734y/bCkEC/Hsv1e7KvvrrIfTtrWQ2RI259U+rwZIwUJEGrzJs/MrVBflbyrg3jeX0al1Fteeuj/ZLTI5ZmAXrhg5gDP+723yNxVWySMtI63dYeGzsOSlYNGhspK6P0cDoWAhIg3ami2FXDe5YrsCBPfp0jInu0Xmrn1d2mQzftT+6WmHSNZo3SIHhn4H2vWEVh3h+atrd64GSMFCROpNshHR0xev5T/vf8mcFZvYHqfHEsCWgqrf5Ou8HaJkJ7RombzR+to8yMzavT39lgbfYF1dChYisse4OxM/WMHbS9dxRP9O3PzC4gpVSxOemstzc1eyeksh81duYa92OQzaqw0Za4K1IypLVLVUJ+0Qqz+Bx8cG4yOGnJc8bWyggEbRYF1dChYiUmcqPylccnQ/XlqwmhVfBQPRytxZu7UIM3jhk9VVZnEtLC7jlYVrOXJAJy4b0Z9fnjKI3JaZVdosIM1VS5lZ0CIXstvCYZfArH/V7jxNgIKFiNTYqs0F/GTibD76chNlZcGtvzwA5G8q4E9TF5KblcE3Du6BYQAM7N6GEft2YcydbyXM97FxR1XYTlsX10RVS6XFMPhMOOF66NgP9hsNEy+s3bkaOQULkSaiOjOi1jbt8P6dWLR6C7964hMKdpbwvWP6898ZX7KtqGobQrvcLG499+Aq+3t2yN21lnXl/fGkZRK/ZM55YPfrQWP23HkbKAULkTjSNhV1mvKO17X02qc+AaiSd23T/vJ/H1MSPkX07dyKR684gv26t+W+BGMc1m6p2tYAMH7UoD1btdS6G3znaVgwGYZdBus/rV6+jWCUdTopWIhUUp2baUPJ+7Zpi6t0LS0oLuX3z86nrNIU2H94bkGt0paUOblZmdxx4SEctU9n2uUEjbs9EjwpJGuELi/7Hqla2r4W7hkRvH7nTijdWb18m2CjdXUoWIhUkujGe9u0xXFvZOu3FTHri42cMrg7Zlbl+IqvdrDiqx0cvW8XbnlxUbXyTvUpJNEsqRt3FPOLSR8nvd6apC0sLmXUgXtV2FeTJ4U6rVpyh2WvJ09z+PfgaxfCu3fCgOOa5HiIdFGwEInx/NxVcb8dQ/AU8Mfndq8KPKxfR3p0yOU7D85gc9jfv2ecG/ofn1/A64vX8eovjmXV5qoji2H3zX7Zum28tmgtY4/sy7Mfr+TXT39CcanvOv+vnghu5of26cjED7+kuCSYLK91dmbc2VO7t8tm0pUVG4vPv/c91sSpGqpO2nhPC2mdZylR1VKrznDKn+DL92DrqmAkdTKn/SX494L/BP82wfEQ6WIetUpTAzNs2DCfOVNLXkj1xX5L79o2mwN7tOXXpw5m0swVfOPgHjwxK49/v/dFwvcb0KplMFq41J3C4jLKnyNi/4oyzTi4d3t6d2rFmIP24uePf0xBcSmDurdl8ZqtcfPOzcrk5MHdeX3RWrYWlbD/Xm1ZunbbrraBWNktMshtmcm2whKyWwST4hWXlrGz1Kvk+eezh0S2Q9RV2rS6oX3y4y3bQmlR0Hvp5bgTW4f5bK7bcjUiZjbL3assLpfy+xUspCnauH0nv5syn407dnLxUf3YWlDM+Cfnxr35xrpy5AAGdW/LdZPnJb1BlpSWcdu0xTz87nIKw2/3sTIzjHY5Ldi4o7jC/iE927F07fYKeRvQuXUWbXKy6N2pFacO2ZuH313OotXxAwvAUQM6c/M5Q+jbufWufXuyN1RanxZad4MfzYCCjdChL6yZB/cdmzifS1+EPkdCWSlktkgeWBQsFCykefvvjC+Yl7+Fm846CDPjmifm8uRHeezdIYcVXxWQaUZpnP/rOVkZ3D32MJ6Znc/x+3fjjEOCm2CqN8j+E56vMrAMggDw5q+O5+T/9wYtMjI4blBXnpu7ikcuH86GbTtTynvEza8l7Fr6zoQTqv07anCinhZSzqdSAEgWhJpxI3Vtg4XaLKRR+WLDdi66fwbbd5aQacYpB3bnjcXrWBm2BSxYuYn8TUWs31bElSMH8POT9+POV5dw9/TP4uZXVFzG8YO6cfyginXUqTa8JusB1LtTK/5y3iFsLyrh4N4d6Ne5NSP26UJGhqWUd9q6ljYGJ1wPrbvC5vxgUNwzP0z9vc04IKSTgoU0OMm+1f/r3eWs3VrIt4b34bWFa5j4wYoK7/04bwuDurfhO0f2ZdzIAeRkZXLN6P2ZMmdltbp1pirqhn7a13Yvlzlor+rd5BvMwjypSviNvit8+3+w8Qv4/E1YuzA6r5HjK25XJ1hIWihYyB6RarVOvHEIE56aC8CoA/fiyVl5jDpwL2484yCmzV8d91zbikr46UkDK+xL17f0dN/Q9/io5Zoq2ppkfMM6uO+44HVWK+gxFCwjftpEmvmAuIZAwUJ2qW5DZm0DwJbCYi48vA/vfLae0lKnVctMbp1WdRxCYXEZN0yZzwvzVrGlsITvHNkXSDwyeGWchW/SeVNvNDf0mkj0tJDdHoZ/LxjYtjkPPo3osnrBf6FDb+g8EFq2CvZVp81CVUv1TsFCgOqPLI6XfvwTH/PuZ+sZ2qdjhYlElV4AABl+SURBVLS3vJA4ALy1ZD0vL1gTWb5NBcW8vGANvzl9MEcM6AzUbMRwk72pV0d1GoATPS0UbQ5GQWe0gDbd4MAzYc5/E5/zgNPjn09PC42GgoUA1R+1HC99cakzaWYek2amtgZxmcPLC9YwbuQAvvG1Hsz4fAN/fD5+fXbXttlM/tGICpPMNesG4MrqIgBsXwv5H0HLNpA/Cz57Lfk5r1tdcR2HZMEiHj0tNCoKFgIkni4if1MB81du5sAe7Xlx3mreXrpu1/54DHjv2hMr7DvzrndYvaVq1VCb7Bbs170NV5+yH9ktMhnSqz3tc7P47TPzqwSA6049oMpspI2uAbi66ioATL85aDcoi7/aXAX3Hx9znq7J01Ze8EeaNAWLZmjDtiL+/tpSxh7Zh327teWZOflkZ2VQWFx1cBnAOXe/y7C+nXh76Xra5rSgZWYGGRY8GVTWo0Mue7XPqbBvwpj4ayL/8cyDOOOQHhXmUzpvWG+yMjNSDgBNumopWQCojul/htxOQZVRlAsnQuFm6D4Yug+BGztGv6ecqpWaNAWLJq5yI/S3jujNxBkryN9UwEdfbuTWc7/G1ZM+pnV2JkXFZRUGmOVmZfLr0/bnnSUbWL5hO1eOHMAvRw0iKzOjWiuXVfcJoFEFgOoOAEvWvfQH78HKj+CLd4MRyck8+b2gK2pxAZTEn29ql+vXBWtIl0vWsLz/qcnzSkbVSk2agkUTNnl2PuOf+LjCRHS3T/uU9rkt+PEJ+/L315Zy9j/epXV2C167+ljeWrI+7g39O0f2q5K3AkAo2bf/tQuDLqId+kBWbkT6dXD7vru334ko45czoFM/aN0FWuTAhiQ36thAUV16WpCQgkUTduu0RbsCRazcrBb84uT9aJPdgsWrt3LesN50bpNd7Rt6owoA1ZEsALjDqjmw4bPgJ5l/xDwdtNkLWnVKnn7MbcHNf98TYfU8+FeSb/k//6Ti9vynkucdqzoBQE8LElKwaMJWxRlvALBmSyFmxpXH7rOHS9QE3D4weAoAoOraFRWc/QDgsHF5UGVUsBHWLkic/ohxu1/3G1G9cikASJopWDQxi1dvZVtRCUN7d6BNdgu2xlkTubZTXDRKqVQt7dwBT12RPJ+Bo6DvUdDrcGjXE/6c5Mnqa+dV3VedgWgKANKAKFg0IZ+v386oO94E4OyhPcnMoEqvpWY7DiFZ1dKXM6Bla5j6S1gxI3k+Z95V92VLRAFAGhAFiybknaXrAThlcHeemp0PwI9P2JenPspvXuMQcjsGK6LNnQSFWyCnXfJ8Hjol+DerFZzzIDxxaeplqG4DsBqMpZFSsGhCZnz+Fd3aZvP/LjiEM+56h2P27cLVpwzi6lMa0ZNEXfREKtgIT1wG7XsH01tvzk9+zvMfgW1rYMDx0GVfeOGa9FX/6GlBGikFi0Yo3gR+ZxzSgxnLNnDkgM60zm7BtJ+NJDMjogG2IYoaiFZaHKyzvOCZ5PlcMhV6HxGsnAbJ2woGf7Pitm7oIlUoWDQQqc7g+sTMFUx46pNdy4OWT/i3fMN21m4t4ogBQffMRhkoorz8O/h4YvAUkBvRDbW6vYlEJCkFiwYg3gyuP580hwffXsYVI/fhyVl5XHh4bx6fuYK3Pl1fZXnQguJS7nxlCV3bZnPyAd33TKGrU12UKG2rLvCrcKxCWRnkRyyX++7fYb9RcOh3Yd+T4Q+dUy+v2gpEakXBogGIN4OrO8zL38JPJs7GDN74dB3tc7PiriMN4MCzVx1Dt3Y5cY/XuWTVRUtegQ1Loe/R0P2gxGl3rIdnroKFzwbVS8Xbk5/zV8sgt0PNyquqJZFaUbBIo9iqpb3a59AhN4uu7XL41yWHs6mgmJfmr+aPzy9kW5yxEOV+dtJAzjm0F5NmruDcw3px0f0z4q/h0D6nygR+9ea/5+x+3bJN8rSzH4EDz4Y23aHHIfD0lYnTVg4UeloQ2WMULNKkctXSqs2FrNpcyMLVWznt72+zcNUWAI7o34kFK7ckHDz3s5P2A9jVoynRGg6/Gr1/7QocVa1UVgqv3wRbV8HRP06e12l/hX1Pgi/fC9ZH+ODexGkvewn6HLF7+6XfaCCaSAOkYFGHCnaWMvOLr+jWNidu1RJAyxYZLFq9hStHDmBwj3acNmRvnpu7Kj0zuNbVhHj3nwCbVgSvM1tGL3Jz+OXBvx37wsEXJg8WsYECFABEGigFizp056tLuOeN5JPLFZeUMe3nI9mve9td+9I2g2uyAPDk94LpKgYcC/2+njyfkp2wzwm7fxZMDkY7i0izoWBRR9ydafNXM7xfJ9ZuLWT5hh1x0/XokFshUJSr0xlct66OXuNgxQewZSW8cwdkZidP+/23IGaBIoZfAW/cmnp1kdoWRBo9BYs68tm6bXy+fjuXHdOffbq05vKHP6SkzCtMEV4n8zIlq1r68Ux47y5483bwiCU0fzY3WDhn6SvBfEjv/j1xWoszZqM61UWqWhJp9BQsaumR95bzwfKNrPgqeJI46YBu7N0+lwU3juaZOSvrfn3oZFVLfzkg6H465Pyg2+pzP0ueV1YuHPCN4CdZsBCRZk/BohbWbink988uoEOrlrTLacF5h/Vi7/bB9N9mVndVS+6wY0P0YjsHngWHXwY9Dwu2o4JFLFUViUgSCha18PiHKygpc/73/aPo36V1zTNKVLXUsjXsfzosfhGKNkfnU3n6bK2HICJ1RMGimibPzufmFxexenPQgDyoe5vaBQpIXLW0czt8Oi2oJtrroGAG1YkXpp6vAoCI1JG0BgszGw3cCWQCD7j7zZWO/wL4HlACrAMuc/cv0lmm2qg80A5g+YYdTJ6dX73qptISWDkbNiwJxi8kc83y+A3MIiJ7UNqChZllAncBJwN5wIdmNsXdYxchng0Mc/cdZvYD4FbggnSVqbbiDbQrKinjtmmLUwsWZaVB76NXb4Q181I7aeVAobYFEakH6XyyGA4sdfdlAGb2GHAGsCtYuPvrMenfB8amsTy1tjLOnEwJ9ydqhwBo1wvOvAd6D4cOfeAPXVIvhKqWRKQepDNY9ARi61jygCMSpAW4HHgh3gEzGweMA+jTp09dla9aNm7fSfvcLDYVFFc51qNDbtU3JAoUAD+ZDS1a1mHpRETSK6O+CwBgZmOBYcBt8Y67+33uPszdh3Xt2nXPFg5YsmYrp//97biBospAu+3rYdbDyTOsHCiSrdcsItIApPPJIh/oHbPdK9xXgZmdBFwHHOvuRWksT42UlJbx44mzKSopY9KVR/HZuq3832ufxR9ot+QVmPyD5E8V8ahqSUQauHQGiw+BgWbWnyBIXAhcFJvAzIYC9wKj3b2ad9j0c3f+9tpSFq3eyj1jD2N4/04M79+Jbw3vWzFhcSG8cgPMuBu6DYZvTYQHTqyXMouIpEPagoW7l5jZVcA0gq6zD7n7fDO7EZjp7lMIqp3aAP+zoNfPl+7+zXSVKZ54a1+fcEA3/vDsAj7J38yi1Vs585AejDqwe/JGa4Ajvg8n3RBMoyEi0oSkdZyFu08Fplba99uY1yel8/xR4q19fe1Tc+mQm8W6bTs5ap/OnHFIT64cOQAzSx4ovjslmO67nLq4ikgT0qxHcMcbN1FQXEZBcRG3nDOECw6P6XlVHDHld2ygALVDiEiT0iB6Q+1J24pKKCktAxKPmwA4dcjewQt3WDYd7h25B0onItIwNatgUVbmnPLXN/jLy58CCcZHANktMmibkwWFW+DRC+DfZwTzNImINFPNKlgsWr2VlZsLmTJnJe7ORUfEH+B37tc6wYx74R9HwWevwqib4Mez9nBpRUQajmbRZuHuTP1kNV98FTwd5G8qYMGqLcxZsYlWWRm0b9WS1ZsL6dEhl6uGtuTC+ZfBgjzofSScc3+wkBCo0VpEmq1mESze+2wDP3r0IzIzjM6tW/LVjp089PZybvrsbLpmboYiIAcoBN4DMLj4WehfqZ1CjdYi0kw1i2qolxasAaC0zDluUDdOGNSNJz/Ko6slWlDIqwYKEZFmrEk+WVQcaJfD9qJSjhrQmYwMOPvQnhzapyN/eH4BfFzfJRURaRyaXLCoOtAuGB/Rp3Mut5xz8K50N515kIKFiEiKmlw1VLyBdgBvfrp+98aWVfDQqD1YKhGRxq3JPVkkGmhXvmY2eTPhsW9D0dY9WCoRkcatSQWLsjInKzODneEI7Vj92mfC87+EmQ9C+97wnafg32eqK6xIM1FcXExeXh6FhRFT9zRyOTk59OrVi6ysrDrNt9EFi0/yNzPi5tcqrCOxdO02enfK5YlZeewsLSMr0ygu9V3v6Zy1k/+1+Tt8OAuGj4PjroVWndQVVqQZycvLo23btvTr1w+rvLZ9E+HubNiwgby8PPr371+neTe6YAHBoLoJT81lwcrNHDuoG99+YAb7dW/D8vU7mNPqR3Qo2wiVg+oG4JwHYci59VFkEalnhYWFTTpQAJgZnTt3Zt26dXWed6MMFgCFxWXc99bnPPTOcrq3yyZ/YwHD+3eiQ97GxG9SoBBp1ppyoCiXrmts9L2h2uVmcft5BzPz+pN5ZGSiQXYiIvVr06ZN/OMf/6j2+0499VQ2bdqUhhJVT6MOFj075DLruhP4esdN5D5zOfboefVdJBFpIibPzmfEza/Rf8LzjLj5NSbPzq9VfomCRUlJSdL3TZ06lQ4dOtTq3HWh0VVDDbFlzMwJlvIuK8zA/tQCSndCixw47tcw/aZ6LqGINHbxV9H8BGBXx5rqmjBhAp999hmHHHIIWVlZ5OTk0LFjRxYtWsSnn37KmWeeyYoVKygsLOSnP/0p48aNA6Bfv37MnDmTbdu2MWbMGI455hjeffddevbsyTPPPENu7p5ZxrnRBYtYGZTBkT+E9r1g/9Oh3d4KFiIS6ffPzmfByi0Jj8/+clOVLvgFxaX86om5TPzgy7jvGdyjHb/7xoEJ87z55puZN28ec+bMYfr06Zx22mnMmzdvV6+lhx56iE6dOlFQUMDhhx/OOeecQ+fOnSvksWTJEiZOnMj999/P+eefz5NPPsnYsWNTvexaadTBAoCTf19xW9OIi0gtxRurlWx/TQwfPrxC99a//e1vPP300wCsWLGCJUuWVAkW/fv355BDDgHgsMMOY/ny5XVWniiNP1hUprETIhIh2RMAwIibXyM/zmwQPTvk8viVR9VJGVq3br3r9fTp03nllVd47733aNWqFccdd1zcwYPZ2dm7XmdmZlJQkHhp6LrWqBu4RUTSYfyoQeRmZVbYl5uVyfhRg2qcZ9u2bdm6Nf40Q5s3b6Zjx460atWKRYsW8f7779f4POnS9J4sRERqqbwRe/dSB7kVZo2oic6dOzNixAgOOuggcnNz6d69+65jo0eP5p577uGAAw5g0KBBHHnkkbW+hrpm7h6dqgEZ1iPTZ45rE2y07qZqJxFJycKFCznggAPquxh7RLxrNbNZ7j6spnk2vieLHkPhhpn1XQoRkWZFbRYiIhJJwUJERCIpWIiISCQFCxERiaRgISIikRQsRET2gJpOUQ5wxx13sGPHjjouUfU0vq6zIiLpdtvAxHPM1XBsV3mw+OEPf1jt995xxx2MHTuWVq1a1ejcdUHBQkSksniBItn+FMROUX7yySfTrVs3Jk2aRFFREWeddRa///3v2b59O+effz55eXmUlpbym9/8hjVr1rBy5UqOP/54unTpwuuvv17jMtSGgoWIND8vTIDVn9Tsvf88Lf7+vYbAmJsTvi12ivKXXnqJJ554gg8++AB355vf/CZvvvkm69ato0ePHjz//PNAMGdU+/bt+etf/8rrr79Oly5dalbmOqA2CxGRPeyll17ipZdeYujQoRx66KEsWrSIJUuWMGTIEF5++WWuueYa3nrrLdq3b1/fRd1FTxYi0vwkeQIA4IYkN+lLn6/16d2da6+9liuvvLLKsY8++oipU6dy/fXXc+KJJ/Lb3/621uerC3qyEBHZA2KnKB81ahQPPfQQ27ZtAyA/P5+1a9eycuVKWrVqxdixYxk/fjwfffRRlffWFz1ZiIhUloYVN2OnKB8zZgwXXXQRRx0VLKTUpk0b/vOf/7B06VLGjx9PRkYGWVlZ3H333QCMGzeO0aNH06NHj3pr4G58U5QPG+YzZ2rWWRGpHk1RXrspylUNJSIikRQsREQkkoKFiIhEUrAQkWajsbXR1kS6rlHBQkSahZycHDZs2NCkA4a7s2HDBnJycuo8b3WdFZFmoVevXuTl5bFu3br6Lkpa5eTk0KtXrzrPN63BwsxGA3cCmcAD7n5zpePZwL+Bw4ANwAXuvjydZRKR5ikrK4v+/fvXdzEarbRVQ5lZJnAXMAYYDHzLzAZXSnY5sNHd9wX+H3BLusojIiI1l842i+HAUndf5u47gceAMyqlOQN4OHz9BHCimVkayyQiIjWQzmDRE1gRs50X7oubxt1LgM1A5zSWSUREaqBRNHCb2ThgXLhZZGbz6rM8adYFWF/fhUijpnx9TfnaQNfX2A2qzZvTGSzygd4x273CffHS5JlZC6A9QUN3Be5+H3AfgJnNrM38Jg2drq/xasrXBrq+xs7MajWpXjqroT4EBppZfzNrCVwITKmUZgpwcfj6XOA1b8qdoEVEGqm0PVm4e4mZXQVMI+g6+5C7zzezG4GZ7j4FeBB4xMyWAl8RBBQREWlg0tpm4e5TgamV9v025nUhcF41s72vDorWkOn6Gq+mfG2g62vsanV9jW49CxER2fM0N5SIiERqVMHCzEab2WIzW2pmE+q7PLVlZsvN7BMzm1PeU8HMOpnZy2a2JPy3Y32XM1Vm9pCZrY3t2pzoeizwt/CznGtmh9ZfyVOT4PpuMLP88DOcY2anxhy7Nry+xWY2qn5KnToz621mr5vZAjObb2Y/Dfc3+s8wybU1ic/PzHLM7AMz+zi8vt+H+/ub2YzwOh4POxthZtnh9tLweL/Ik7h7o/ghaCT/DBgAtAQ+BgbXd7lqeU3LgS6V9t0KTAhfTwBuqe9yVuN6RgKHAvOirgc4FXgBMOBIYEZ9l7+G13cD8Ms4aQeH/0ezgf7h/93M+r6GiOvbGzg0fN0W+DS8jkb/GSa5tibx+YWfQZvwdRYwI/xMJgEXhvvvAX4Qvv4hcE/4+kLg8ahzNKYni1SmD2kKYqdAeRg4sx7LUi3u/iZBr7ZYia7nDODfHngf6GBme++ZktZMgutL5AzgMXcvcvfPgaUE/4cbLHdf5e4fha+3AgsJZllo9J9hkmtLpFF9fuFnsC3czAp/HDiBYColqPrZVWuqpcYULFKZPqSxceAlM5sVjlIH6O7uq8LXq4Hu9VO0OpPoeprS53lVWA3zUEy1YaO+vrBaYijBN9Qm9RlWujZoIp+fmWWa2RxgLfAywdPQJg+mUoKK11DtqZYaU7Boio5x90MJZub9kZmNjD3owTNik+mu1tSuJ3Q3sA9wCLAK+Ev9Fqf2zKwN8CTwM3ffEnussX+Gca6tyXx+7l7q7ocQzJYxHNi/LvNvTMEilelDGhV3zw//XQs8TfABryl/lA//XVt/JawTia6nSXye7r4m/CMtA+5nd1VFo7w+M8siuJn+192fCnc3ic8w3rU1tc8PwN03Aa8DRxFUDZaPp4u9hl3XZ0mmWorVmIJFKtOHNBpm1trM2pa/Bk4B5lFxCpSLgWfqp4R1JtH1TAG+G/aoORLYHFPV0WhUqqM/i+AzhOD6Lgx7nfQHBgIf7OnyVUdYZ/0gsNDd/xpzqNF/homural8fmbW1cw6hK9zgZMJ2mVeJ5hKCap+dtWbaqm+W/Gr2eJ/KkEvhs+A6+q7PLW8lgEEvS0+BuaXXw9BveGrwBLgFaBTfZe1Gtc0keBRvpigfvTyRNdD0HvjrvCz/AQYVt/lr+H1PRKWf274B7h3TPrrwutbDIyp7/KncH3HEFQxzQXmhD+nNoXPMMm1NYnPD/gaMDu8jnnAb8P9AwiC3FLgf0B2uD8n3F4aHh8QdQ6N4BYRkUiNqRpKRETqiYKFiIhEUrAQEZFIChYiIhJJwUJERCIpWIikmZkdZ2bP1Xc5RGpDwUJERCIpWIiEzGxsuCbAHDO7N5yYbZuZ/b9wjYBXzaxrmPYQM3s/nIDu6Zg1HvY1s1fCdQU+MrN9wuzbmNkTZrbIzP5bPsOnmd0crrEw18xur6dLF4mkYCECmNkBwAXACA8mYysFvg20Bma6+4HAG8Dvwrf8G7jG3b9GMAK4fP9/gbvc/WDgaIIR3xDMcvozgnUSBgAjzKwzwRQTB4b5/DG9VylScwoWIoETgcOAD8Npnk8kuKmXAY+Haf4DHGNm7YEO7v5GuP9hYGQ411dPd38awN0L3X1HmOYDd8/zYMK6OUA/gmmhC4EHzexsoDytSIOjYCESMOBhdz8k/Bnk7jfESVfT+XGKYl6XAi08WEdgOMHiM6cDL9Ywb5G0U7AQCbwKnGtm3WDXutN9Cf5GymftvAh42903AxvN7Ovh/u8Ab3iwAluemZ0Z5pFtZq0SnTBcW6G9u08Ffg4cnI4LE6kLLaKTiDR97r7AzK4nWLkwg2Bm2R8B24Hh4bG1BO0aEEzvfE8YDJYBl4b7vwPca2Y3hnmcl+S0bYFnzCyH4MnmF3V8WSJ1RrPOiiRhZtvcvU19l0OkvqkaSkREIunJQkREIunJQkREIilYiIhIJAULERGJpGAhIiKRFCxERCSSgoWIiET6/48aLr3dDtDlAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 6.4.3.5 Dropout 사용한 경우\n",
        "# 그래프 생성=========\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.title ('Train vs.Test OverFit w dropout = True')\n",
        "plt.xlim(0, 300)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "certain-circus",
      "metadata": {
        "id": "certain-circus"
      },
      "source": [
        "## 6.5 적절한 하이퍼파라미터 값 찾기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5.1 검증데이터"
      ],
      "metadata": {
        "id": "Dbvb8lRxToFS"
      },
      "id": "Dbvb8lRxToFS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "supreme-trail",
      "metadata": {
        "id": "supreme-trail"
      },
      "outputs": [],
      "source": [
        "# 6.5.1.1 훈련데이터에서 검증 데이터 분할 수행을 위해 섞는 (shuffle) 함수\n",
        "\n",
        "def shuffle_dataset(x, t):\n",
        "    permutation = np.random.permutation(x.shape[0])\n",
        "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
        "    t = t[permutation]\n",
        "    return x, t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "outstanding-adrian",
      "metadata": {
        "id": "outstanding-adrian"
      },
      "outputs": [],
      "source": [
        "# 6.5.1.2 검증데이터 분할\n",
        "# MNIST 적재\n",
        "(x_train, t_train) , (x_test, t_test) = load_mnist()\n",
        "\n",
        "# 훈련데이터를 셔플\n",
        "x_train, t_train = shuffle_dataset (x_train, t_train)\n",
        "\n",
        "# 검증데이터의 분할\n",
        "validation_rate = 0.20\n",
        "validation_num = int(x_train.shape[0] * validation_rate)\n",
        "\n",
        "x_val = x_train[:validation_num]\n",
        "t_val = t_train[:validation_num]\n",
        "x_train = x_train[validation_num:]\n",
        "t_train = t_train[validation_num:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fancy-colon",
      "metadata": {
        "id": "fancy-colon"
      },
      "source": [
        "### 6.5.2 하이퍼파라미터의 최적화\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 0단계 : 하이퍼파리미터 값의 범위를 설정\n",
        "* 1단계 : 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출\n",
        "* 2단계 : 1단계에서 샘플링한 하이퍼파라미터의 값을 사용하여 학습하고, 검증데이터로 정확도를 평가. (에폭을 작게 설정)\n",
        "* 3단계 : 1단계와 2단계를 특정 횟수 (100 회 등) 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁혀나간다."
      ],
      "metadata": {
        "id": "bbM4fCAaf3SX"
      },
      "id": "bbM4fCAaf3SX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.5.3 하이퍼파라미터의 최적화 구현하기"
      ],
      "metadata": {
        "id": "DQ6m7marTlDm"
      },
      "id": "DQ6m7marTlDm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 하이퍼파라미터 중 가중치 감소 계수 weight_decay와 학습률 learning rate의 적절한 값을 찾아본다.\n"
      ],
      "metadata": {
        "id": "lnfA9u1Kehvh"
      },
      "id": "lnfA9u1Kehvh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "identified-klein",
      "metadata": {
        "id": "identified-klein",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da55fc1-1077-4ff9-e370-a0a724ae1a17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.798313472566174e-08, 0.00010225852113635806)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# 6.5.3.1  가중치감소계수와 학습률 초기 값 무작위 설정 예\n",
        "weight_decay = 10 ** np.random.uniform(-8, -4) # 0.00000001 ~0.0001\n",
        "lr = 10 ** np.random.uniform (-6, -2) #  0.000001 ~ 0.01\n",
        "\n",
        "weight_decay, lr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Various weight_decay, lr combination : Accuracy on train vs. validation"
      ],
      "metadata": {
        "id": "4cxCQEh8NbzK"
      },
      "id": "4cxCQEh8NbzK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "otherwise-cache",
      "metadata": {
        "scrolled": false,
        "id": "otherwise-cache",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03684bd-7948-4b01-db32-ec37d2e4b851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0th trial] val acc:  0.07 |  weight decay:0.0000013642, lr: 0.0000046376\n",
            "[  1th trial] val acc:  0.55 |  weight decay:0.0000192384, lr: 0.0040826933\n",
            "[  2th trial] val acc:  0.09 |  weight decay:0.0000001474, lr: 0.0000060533\n",
            "[  3th trial] val acc:  0.14 |  weight decay:0.0000014514, lr: 0.0000813291\n",
            "[  4th trial] val acc:  0.37 |  weight decay:0.0000002718, lr: 0.0023570356\n",
            "[  5th trial] val acc:  0.09 |  weight decay:0.0000033990, lr: 0.0000513198\n",
            "[  6th trial] val acc:  0.16 |  weight decay:0.0000271802, lr: 0.0013213007\n",
            "[  7th trial] val acc:  0.09 |  weight decay:0.0000000847, lr: 0.0000935750\n",
            "[  8th trial] val acc:  0.15 |  weight decay:0.0000000135, lr: 0.0000373729\n",
            "[  9th trial] val acc:  0.05 |  weight decay:0.0000601866, lr: 0.0000024037\n",
            "[ 10th trial] val acc:  0.18 |  weight decay:0.0000000573, lr: 0.0006748809\n",
            "[ 11th trial] val acc:  0.75 |  weight decay:0.0000012689, lr: 0.0080082831\n",
            "[ 12th trial] val acc:  0.25 |  weight decay:0.0000859577, lr: 0.0017073896\n",
            "[ 13th trial] val acc:  0.26 |  weight decay:0.0000000457, lr: 0.0022891326\n",
            "[ 14th trial] val acc:  0.06 |  weight decay:0.0000162505, lr: 0.0000146802\n",
            "[ 15th trial] val acc:  0.16 |  weight decay:0.0000019379, lr: 0.0000059787\n",
            "[ 16th trial] val acc:   0.1 |  weight decay:0.0000018945, lr: 0.0000126954\n",
            "[ 17th trial] val acc:  0.08 |  weight decay:0.0000019482, lr: 0.0004939633\n",
            "[ 18th trial] val acc:  0.11 |  weight decay:0.0000024636, lr: 0.0000073789\n",
            "[ 19th trial] val acc:   0.1 |  weight decay:0.0000002196, lr: 0.0005224954\n",
            "[ 20th trial] val acc:  0.53 |  weight decay:0.0000001634, lr: 0.0050733508\n",
            "[ 21th trial] val acc:  0.09 |  weight decay:0.0000000145, lr: 0.0000041301\n",
            "[ 22th trial] val acc:  0.23 |  weight decay:0.0000017889, lr: 0.0009859941\n",
            "[ 23th trial] val acc:   0.1 |  weight decay:0.0000103735, lr: 0.0003497560\n",
            "[ 24th trial] val acc:   0.1 |  weight decay:0.0000000236, lr: 0.0002229477\n",
            "[ 25th trial] val acc:  0.12 |  weight decay:0.0000000368, lr: 0.0000045281\n",
            "[ 26th trial] val acc:  0.72 |  weight decay:0.0000013212, lr: 0.0050659800\n",
            "[ 27th trial] val acc:  0.53 |  weight decay:0.0000002756, lr: 0.0023386852\n",
            "[ 28th trial] val acc:  0.58 |  weight decay:0.0000036704, lr: 0.0051290722\n",
            "[ 29th trial] val acc:  0.12 |  weight decay:0.0000001514, lr: 0.0000494450\n",
            "[ 30th trial] val acc:  0.14 |  weight decay:0.0000065580, lr: 0.0000372524\n",
            "[ 31th trial] val acc:  0.18 |  weight decay:0.0000200293, lr: 0.0002920466\n",
            "[ 32th trial] val acc:  0.54 |  weight decay:0.0000065644, lr: 0.0037225505\n",
            "[ 33th trial] val acc:  0.09 |  weight decay:0.0000637855, lr: 0.0010647583\n",
            "[ 34th trial] val acc:  0.08 |  weight decay:0.0000441816, lr: 0.0000258724\n",
            "[ 35th trial] val acc:   0.1 |  weight decay:0.0000000412, lr: 0.0000035311\n",
            "[ 36th trial] val acc:  0.38 |  weight decay:0.0000410896, lr: 0.0019512147\n",
            "[ 37th trial] val acc:  0.08 |  weight decay:0.0000000156, lr: 0.0004685358\n",
            "[ 38th trial] val acc:  0.27 |  weight decay:0.0000156264, lr: 0.0019126706\n",
            "[ 39th trial] val acc:  0.13 |  weight decay:0.0000000101, lr: 0.0000600345\n",
            "[ 40th trial] val acc:  0.13 |  weight decay:0.0000195172, lr: 0.0000014532\n",
            "[ 41th trial] val acc:  0.13 |  weight decay:0.0000036679, lr: 0.0000165291\n",
            "[ 42th trial] val acc:   0.1 |  weight decay:0.0000000263, lr: 0.0001211566\n",
            "[ 43th trial] val acc:  0.09 |  weight decay:0.0000745619, lr: 0.0001316492\n",
            "[ 44th trial] val acc:  0.06 |  weight decay:0.0000000710, lr: 0.0000012805\n",
            "[ 45th trial] val acc:  0.04 |  weight decay:0.0000343332, lr: 0.0000032656\n",
            "[ 46th trial] val acc:  0.13 |  weight decay:0.0000135147, lr: 0.0000018489\n",
            "[ 47th trial] val acc:  0.05 |  weight decay:0.0000626203, lr: 0.0003675991\n",
            "[ 48th trial] val acc:  0.13 |  weight decay:0.0000011032, lr: 0.0002643030\n",
            "[ 49th trial] val acc:  0.11 |  weight decay:0.0000337183, lr: 0.0000023189\n",
            "[ 50th trial] val acc:  0.11 |  weight decay:0.0000000187, lr: 0.0000264957\n",
            "[ 51th trial] val acc:  0.06 |  weight decay:0.0000001312, lr: 0.0000061956\n",
            "[ 52th trial] val acc:  0.11 |  weight decay:0.0000000200, lr: 0.0003623916\n",
            "[ 53th trial] val acc:  0.09 |  weight decay:0.0000000383, lr: 0.0007732613\n",
            "[ 54th trial] val acc:  0.27 |  weight decay:0.0000000216, lr: 0.0014135447\n",
            "[ 55th trial] val acc:  0.08 |  weight decay:0.0000006230, lr: 0.0000096521\n",
            "[ 56th trial] val acc:  0.09 |  weight decay:0.0000084000, lr: 0.0000010785\n",
            "[ 57th trial] val acc:  0.14 |  weight decay:0.0000883926, lr: 0.0000079221\n",
            "[ 58th trial] val acc:  0.76 |  weight decay:0.0000002179, lr: 0.0082609590\n",
            "[ 59th trial] val acc:  0.56 |  weight decay:0.0000008746, lr: 0.0029970969\n",
            "[ 60th trial] val acc:   0.1 |  weight decay:0.0000000139, lr: 0.0005810372\n",
            "[ 61th trial] val acc:  0.36 |  weight decay:0.0000000915, lr: 0.0023430973\n",
            "[ 62th trial] val acc:  0.11 |  weight decay:0.0000110828, lr: 0.0003733268\n",
            "[ 63th trial] val acc:   0.4 |  weight decay:0.0000001215, lr: 0.0029235685\n",
            "[ 64th trial] val acc:  0.05 |  weight decay:0.0000192275, lr: 0.0000056218\n",
            "[ 65th trial] val acc:  0.35 |  weight decay:0.0000193841, lr: 0.0010666690\n",
            "[ 66th trial] val acc:  0.05 |  weight decay:0.0000051634, lr: 0.0000042101\n",
            "[ 67th trial] val acc:   0.1 |  weight decay:0.0000267585, lr: 0.0000051589\n",
            "[ 68th trial] val acc:  0.12 |  weight decay:0.0000000384, lr: 0.0000051214\n",
            "[ 69th trial] val acc:  0.15 |  weight decay:0.0000015166, lr: 0.0006219642\n",
            "[ 70th trial] val acc:  0.13 |  weight decay:0.0000061420, lr: 0.0009472733\n",
            "[ 71th trial] val acc:  0.11 |  weight decay:0.0000000486, lr: 0.0000034377\n",
            "[ 72th trial] val acc:  0.12 |  weight decay:0.0000031566, lr: 0.0000513768\n",
            "[ 73th trial] val acc:  0.41 |  weight decay:0.0000076089, lr: 0.0019349308\n",
            "[ 74th trial] val acc:  0.12 |  weight decay:0.0000004668, lr: 0.0005700292\n",
            "[ 75th trial] val acc:  0.18 |  weight decay:0.0000009870, lr: 0.0006007417\n",
            "[ 76th trial] val acc:  0.06 |  weight decay:0.0000000242, lr: 0.0000090917\n",
            "[ 77th trial] val acc:  0.06 |  weight decay:0.0000008617, lr: 0.0000288255\n",
            "[ 78th trial] val acc:  0.09 |  weight decay:0.0000001488, lr: 0.0000082442\n",
            "[ 79th trial] val acc:  0.13 |  weight decay:0.0000017040, lr: 0.0001026413\n",
            "[ 80th trial] val acc:  0.11 |  weight decay:0.0000038097, lr: 0.0000044517\n",
            "[ 81th trial] val acc:  0.08 |  weight decay:0.0000004203, lr: 0.0004722968\n",
            "[ 82th trial] val acc:   0.1 |  weight decay:0.0000150762, lr: 0.0001010169\n",
            "[ 83th trial] val acc:  0.07 |  weight decay:0.0000000284, lr: 0.0000117929\n",
            "[ 84th trial] val acc:  0.08 |  weight decay:0.0000000109, lr: 0.0000681486\n",
            "[ 85th trial] val acc:  0.72 |  weight decay:0.0000001943, lr: 0.0051609867\n",
            "[ 86th trial] val acc:  0.55 |  weight decay:0.0000359659, lr: 0.0042240627\n",
            "[ 87th trial] val acc:  0.11 |  weight decay:0.0000002323, lr: 0.0000021666\n",
            "[ 88th trial] val acc:  0.77 |  weight decay:0.0000274646, lr: 0.0085782844\n",
            "[ 89th trial] val acc:  0.09 |  weight decay:0.0000015058, lr: 0.0000104987\n",
            "[ 90th trial] val acc:  0.13 |  weight decay:0.0000042589, lr: 0.0000174193\n",
            "[ 91th trial] val acc:  0.12 |  weight decay:0.0000034669, lr: 0.0000108035\n",
            "[ 92th trial] val acc:  0.09 |  weight decay:0.0000007058, lr: 0.0000041256\n",
            "[ 93th trial] val acc:  0.15 |  weight decay:0.0000088026, lr: 0.0010884534\n",
            "[ 94th trial] val acc:   0.1 |  weight decay:0.0000003263, lr: 0.0005689206\n",
            "[ 95th trial] val acc:  0.23 |  weight decay:0.0000102455, lr: 0.0010020232\n",
            "[ 96th trial] val acc:  0.66 |  weight decay:0.0000011766, lr: 0.0071488818\n",
            "[ 97th trial] val acc:  0.69 |  weight decay:0.0000000212, lr: 0.0063629972\n",
            "[ 98th trial] val acc:  0.11 |  weight decay:0.0000006944, lr: 0.0000955829\n",
            "[ 99th trial] val acc:  0.14 |  weight decay:0.0000089068, lr: 0.0002282187\n"
          ]
        }
      ],
      "source": [
        "# 6.5.3.2 가중치 감소계수와 학습률을 무작위로 100쌍을 조합하여 학습을 진행하고\n",
        "#   검증 정확도가 좋은 20개 조합 (Top 20)의 wd와 lr을 관찰한다.\n",
        "\n",
        "# 은닉층 6개층 100개 노드\n",
        "# 은닉층 활성화 함수 : ReLU\n",
        "# 가중치 초기설정 : He\n",
        "\n",
        "# 배치 사이즈 : 100\n",
        "# 최대 반복 회수 : 50 epoch\n",
        "# 훈련방식 : SGD\n",
        "\n",
        "# 훈련데이터 크기 : 500\n",
        "\n",
        "# 실험 : 하이퍼파라미터 가중치감소계수와 학습률 탐색\n",
        "# weight_decay = 10 ^-8 ~ 10^-4 ( 0.000_000_01 ~ 0.0001)\n",
        "# lr = 10^-6 ~ 10^-2 (0.000_001 ~ 0.01 )\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.util import shuffle_dataset\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# MNIST 데이터 적재\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 고속화를 위해서 훈련데이터를 제거\n",
        "x_train = x_train[:500]\n",
        "t_train = t_train[:500]\n",
        "\n",
        "# 검증데이터 분리\n",
        "validation_rate = 0.20\n",
        "validation_num = int(x_train.shape[0] * validation_rate)\n",
        "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
        "x_val = x_train[:validation_num]\n",
        "t_val = t_train[:validation_num]\n",
        "x_train = x_train[validation_num:]\n",
        "t_train = t_train[validation_num:]\n",
        "\n",
        "# 훈련 함수 정의\n",
        "def __train(lr, weight_decay, epocs=50):\n",
        "    # 신경망 생성\n",
        "    network = MultiLayerNet(input_size=784,\n",
        "                            hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                            output_size=10,\n",
        "                            weight_decay_lambda=weight_decay)  #### 탐색대상\n",
        "\n",
        "    # 트레이너 생성\n",
        "    trainer = Trainer(network,             # 신경망\n",
        "                      x_train, t_train,    # 훈련 데이터\n",
        "                      x_val, t_val,        # 검증 데이터\n",
        "                      epochs=epocs,        # 기본 에폭 50\n",
        "                      mini_batch_size=100, # 배치 크기 100\n",
        "                      optimizer='sgd',\n",
        "                      optimizer_param={'lr': lr},  ##### 탐색대상\n",
        "                      verbose=False)\n",
        "    # 훈련 수행\n",
        "    trainer.train()\n",
        "\n",
        "    # 훈련 결과 훈련정확도, 검증정확도 반환\n",
        "    return trainer.test_acc_list, trainer.train_acc_list\n",
        "\n",
        "\n",
        "# 하이퍼파라미터의 랜덤 탐색 ======================================\n",
        "optimization_trial = 100\n",
        "\n",
        "#  하이퍼파라미터 조합 별 학습성능, 검증/훈련데이터별 저장 변수 초기화\n",
        "results_val = {}\n",
        "results_train = {}\n",
        "\n",
        "# 100번 동안 다양한 조합 탐색\n",
        "for i in range(optimization_trial):\n",
        "\n",
        "    # 탐색할 하이퍼파라미터를 지정된 범위에서 무작위로 설정 ===============\n",
        "\n",
        "    # weight_decay = 10 ^-8 ~ 10^-4 ( 0.000_000_01 ~ 0.0001)\n",
        "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
        "\n",
        "    # lr = 10^-6 ~ 10^-2 (0.000_001 ~ 0.01 )\n",
        "    lr = 10 ** np.random.uniform(-6, -2)\n",
        "\n",
        "    # =================================================================\n",
        "\n",
        "    # 훈련 수행\n",
        "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
        "\n",
        "    # 훈련 수행결과 최종 검증정확도와 lr, weight decay  출력\n",
        "    print(f\"[{i:3}th trial] val acc:{val_acc_list[-1]:6} |\",\n",
        "          f\" weight decay:{weight_decay:.10f}, lr: {lr:.10f}\")\n",
        "\n",
        "    # 이번 loop에서 수행한 lr값 및 weight_decay 값을 키로 하는\n",
        "    # results_val, results_train 사전에 각각의 accuracy list 를 저장한다.\n",
        "    key = f\"weight decay:{weight_decay:.10f}, lr:{lr:.10f}\"\n",
        "    results_val[key] = val_acc_list\n",
        "    results_train[key] = train_acc_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.5.3.3   #6.5.3.2 에서 시도한 100개의 조합 중에서 검증정확도 성능이 가장 좋은\n",
        "#           20개를 발췌하여 학습과정을 그래프로 표현한다. (훈련 데이터/검증데이터)\n",
        "#           weight_decay = 10 ^-8 ~ 10^-4 ( 0.000_000_01 ~ 0.0001)\n",
        "#           lr = 10^-6 ~ 10^-2 (0.000_001 ~ 0.01 )\n",
        "# 그래프 생성 ========================================================\n",
        "\n",
        "# 그래프 초기화 / 결과 출력 제목\n",
        "plt.figure(figsize = (12,10))\n",
        "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
        "\n",
        "# 그래프 그릴 갯수\n",
        "graph_draw_num = 20\n",
        "col_num = 5\n",
        "row_num = int(np.ceil(graph_draw_num / col_num))\n",
        "i = 0\n",
        "\n",
        "# results_val의 값에 저장된 val_acc_list[-1] (=각 리스트 (각 조합의 시도)의 마지막값)의\n",
        "# 내림차순으로 정렬하고 해당 시도의 하이퍼파라미터의 조합을 보여준다.\n",
        "for key, val_acc_list in sorted(results_val.items(),\n",
        "                                key=lambda x:x[1][-1], reverse=True):\n",
        "\n",
        "    print(f\"Best-{i+1:2} (val acc: {val_acc_list[-1]:.2f})| \" + key)\n",
        "\n",
        "    plt.subplot(row_num, col_num, i+1)\n",
        "    plt.title(\"Best-\" + str(i+1))\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    if i % 5: plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    x = np.arange(len(val_acc_list))\n",
        "    plt.plot(x, val_acc_list)\n",
        "    plt.plot(x, results_train[key], \"--\")\n",
        "    i += 1\n",
        "\n",
        "    if i >= graph_draw_num:    # 지정된 상위 개수 출력 후 종료\n",
        "        break\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UqAswXzvKQj2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "83f0a0d6-7040-4abd-ce17-9971df5cab66"
      },
      "id": "UqAswXzvKQj2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========== Hyper-Parameter Optimization Result ===========\n",
            "Best- 1 (val acc: 0.77)| weight decay:0.0000274646, lr:0.0085782844\n",
            "Best- 2 (val acc: 0.76)| weight decay:0.0000002179, lr:0.0082609590\n",
            "Best- 3 (val acc: 0.75)| weight decay:0.0000012689, lr:0.0080082831\n",
            "Best- 4 (val acc: 0.72)| weight decay:0.0000013212, lr:0.0050659800\n",
            "Best- 5 (val acc: 0.72)| weight decay:0.0000001943, lr:0.0051609867\n",
            "Best- 6 (val acc: 0.69)| weight decay:0.0000000212, lr:0.0063629972\n",
            "Best- 7 (val acc: 0.66)| weight decay:0.0000011766, lr:0.0071488818\n",
            "Best- 8 (val acc: 0.58)| weight decay:0.0000036704, lr:0.0051290722\n",
            "Best- 9 (val acc: 0.56)| weight decay:0.0000008746, lr:0.0029970969\n",
            "Best-10 (val acc: 0.55)| weight decay:0.0000192384, lr:0.0040826933\n",
            "Best-11 (val acc: 0.55)| weight decay:0.0000359659, lr:0.0042240627\n",
            "Best-12 (val acc: 0.54)| weight decay:0.0000065644, lr:0.0037225505\n",
            "Best-13 (val acc: 0.53)| weight decay:0.0000001634, lr:0.0050733508\n",
            "Best-14 (val acc: 0.53)| weight decay:0.0000002756, lr:0.0023386852\n",
            "Best-15 (val acc: 0.41)| weight decay:0.0000076089, lr:0.0019349308\n",
            "Best-16 (val acc: 0.40)| weight decay:0.0000001215, lr:0.0029235685\n",
            "Best-17 (val acc: 0.38)| weight decay:0.0000410896, lr:0.0019512147\n",
            "Best-18 (val acc: 0.37)| weight decay:0.0000002718, lr:0.0023570356\n",
            "Best-19 (val acc: 0.36)| weight decay:0.0000000915, lr:0.0023430973\n",
            "Best-20 (val acc: 0.35)| weight decay:0.0000193841, lr:0.0010666690\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x720 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "78zpFIX4-iAF"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}